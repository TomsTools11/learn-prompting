export interface Technique {
  id: string;
  name: string;
  description: string;
  example: string;
}

export interface Lesson {
  id: string;
  title: string;
  content: string;
  duration: string;
  keyPoints: string[];
  handsOnExercise?: string;
}

export interface Module {
  id: string;
  title: string;
  description: string;
  duration: string;
  lessons: Lesson[];
}

export const promptEngineeringTechniques: Technique[] = [
  {
    id: "zero-shot",
    name: "Zero-shot Prompting",
    description: "Ask the model to perform a task with no prior examples.",
    example: "You are a senior cybersecurity analyst. Analyze this network log and identify any potential security threats. For each threat, specify: severity level (Critical/High/Medium/Low), attack vector, and recommended mitigation strategy. Present findings in a structured incident report format."
  },
  {
    id: "few-shot",
    name: "Few-shot Prompting",
    description: "Provide a few examples to guide the model's output.",
    example: "Extract key entities from customer feedback:\n\nInput: \"The new iPhone camera is amazing but battery life is disappointing.\"\nOutput: {product: \"iPhone\", positive: [\"camera\"], negative: [\"battery life\"]}\n\nInput: \"Tesla Model 3 has incredible acceleration, though the interior feels cheap.\"\nOutput: {product: \"Tesla Model 3\", positive: [\"acceleration\"], negative: [\"interior quality\"]}\n\nNow extract from: \"The MacBook Pro M3 runs cool and quiet, but the price is hard to justify.\""
  },
  {
    id: "chain-of-thought",
    name: "Chain of Thought (CoT)",
    description: "Encourage step-by-step reasoning.",
    example: "A company's revenue grew from $2.5M to $4.8M over 3 years, while operating costs increased from $1.8M to $3.1M. Should they expand to a new market requiring $800K investment? Think step by step:\n\n1. Calculate revenue growth rate\n2. Calculate cost growth rate and profit margins\n3. Assess sustainability of current trajectory\n4. Evaluate risk vs. opportunity of expansion\n5. Provide recommendation with justification"
  },
  {
    id: "meta-prompting",
    name: "Meta Prompting",
    description: "Ask the model to generate or refine its own prompts before answering.",
    example: "Before answering the user's question about implementing microservices architecture, first:\n\n1. Generate 3 clarifying questions to understand their context (team size, current architecture, scale requirements)\n2. Create a structured prompt template that will guide a comprehensive analysis\n3. Rate your prompt template on clarity (1-10) and refine if below 8\n4. Then use your refined prompt to provide the answer"
  },
  {
    id: "self-consistency",
    name: "Self-consistency",
    description: "Request multiple independent answers and select the most coherent.",
    example: "Generate three independent solutions to this algorithm problem using different approaches (dynamic programming, greedy algorithm, divide-and-conquer). For each solution:\n- Provide the pseudocode\n- Analyze time/space complexity\n- List edge cases\n\nThen compare all three, identify the most efficient approach, and explain why it's optimal."
  },
  {
    id: "rag",
    name: "Retrieval Augmented Generation (RAG)",
    description: "Combine external information retrieval with generative AI.",
    example: "Based on the latest Q4 2024 earnings reports from Apple, Microsoft, and Google (retrieved from SEC filings), analyze the AI investment trends across these tech giants. For each company:\n- Identify AI-related revenue streams\n- Compare YoY AI infrastructure spending\n- Extract future AI strategy statements from CEO remarks\n\nCite specific page numbers and filing sections for all claims."
  },
  {
    id: "react",
    name: "ReAct (Reasoning and Acting)",
    description: "Combine reasoning and acting prompts.",
    example: "Research the current state of quantum computing applications in cryptography:\n\nThought: I need to find recent developments in post-quantum cryptography\nAction: Search academic papers from 2024-2025 on 'post-quantum cryptography standards'\nObservation: [results show NIST standardization of new algorithms]\n\nThought: I should understand the timeline for implementation\nAction: Search for 'NIST post-quantum migration timeline enterprise'\nObservation: [results indicate 2025-2030 transition period]\n\nProvide a comprehensive analysis with actionable recommendations for enterprise security teams."
  },
  {
    id: "tree-of-thoughts",
    name: "Tree of Thoughts (ToT)",
    description: "Explore multiple reasoning paths and evaluate the best solution.",
    example: "Design a scalable notification system for 10M users. Explore three architectural approaches:\n\nPath 1: Push-based (WebSocket) ‚Üí Evaluate: latency, server cost, complexity\nPath 2: Pull-based (Polling) ‚Üí Evaluate: freshness, client battery, server load  \nPath 3: Hybrid (Push + Queue) ‚Üí Evaluate: reliability, cost, implementation time\n\nFor each path, identify pros/cons and potential failure modes. Select the optimal approach based on: real-time requirements, budget constraints, and team expertise."
  },
  {
    id: "structured-output",
    name: "Structured Output",
    description: "Define precise output format using JSON schemas or templates.",
    example: "Analyze this product review and return a JSON object matching this schema:\n\n{\n  \"sentiment\": \"positive\" | \"negative\" | \"neutral\",\n  \"rating_prediction\": 1-5,\n  \"key_aspects\": [\n    {\"feature\": string, \"sentiment\": string, \"quote\": string}\n  ],\n  \"purchase_intent\": \"high\" | \"medium\" | \"low\",\n  \"competitor_mentions\": [string],\n  \"actionable_feedback\": string\n}\n\nReview: \"This laptop beats my old Dell in every way - the M3 chip is blazingly fast for video editing. Battery lasts all day unlike my colleague's HP. Only wish it had more ports, but USB-C hubs solve that. Definitely buying another for my team.\""
  },
  {
    id: "self-refinement",
    name: "Self-Refinement",
    description: "Model critiques and improves its own outputs iteratively.",
    example: "Write a Python function to detect SQL injection attempts.\n\nAfter writing, perform self-review:\n1. Rate your solution's security coverage (1-10)\n2. Identify any edge cases you missed\n3. Check for performance bottlenecks\n4. Suggest improvements to your own code\n5. Rewrite the function incorporating your critiques\n6. Explain what changed and why it's better\n\nRepeat the review process until you rate it 9/10 or higher."
  }
];

export const courseModules: Module[] = [
  {
    id: "module-1",
    title: "Foundations of Large Language Models and Prompt Engineering",
    description: "Learn the fundamentals of LLMs, how they work, and why prompt engineering is essential for getting the best results.",
    duration: "12-16 hours",
    lessons: [
      {
        id: "lesson-1-1",
        title: "Introduction to Large Language Models",
        duration: "2 hours",
        content: "# What is a Large Language Model?\n\nLarge Language Models (LLMs) are sophisticated AI systems trained on vast amounts of text data. They work as advanced prediction engines that understand and generate human-like text.\n\n## Key Concepts\n\n### LLMs as Prediction Engines\n- Process text as tokens (sub-word units)\n- Calculate probability distributions for next token predictions\n- Generate responses by sampling from these distributions\n\n### Training and Emergent Abilities\n- Trained on massive text corpora from the internet, books, and other sources\n- Develop unexpected capabilities like reasoning and problem-solving\n- Can perform tasks they weren't explicitly trained for\n\n## Brief History\n\n**Pre-2000s:** Simple N-gram models with limited capabilities\n\n**Mid-2000s:** Neural networks and deep learning emerged\n\n**Early 2010s:** LSTM networks improved sequential processing\n\n**Late 2010s:** Transformer revolution with attention mechanisms\n\n**2020s:** GPT era bringing powerful, accessible models",
        keyPoints: [
          "LLMs predict text based on probability distributions",
          "Tokens are the basic unit of processing",
          "Models are trained on massive datasets",
          "Transformers revolutionized language modeling",
          "Modern LLMs have emergent capabilities"
        ]
      },
      {
        id: "lesson-1-2",
        title: "The Transformer Architecture: A Deep Dive",
        duration: "3 hours",
        content: "# The Transformer Architecture\n\nTransformers are the revolutionary neural network architecture that powers modern LLMs. Understanding their mechanics helps you write better prompts.\n\n## Self-Attention Mechanism\n\n### The Core Innovation\n\nSelf-attention allows the model to weigh the importance of different words in relation to each other, regardless of their distance in the text.\n\n**Three Key Components:**\n- **Query (Q):** What am I looking for?\n- **Key (K):** What do I contain?\n- **Value (V):** What information do I provide?\n\n### How It Works\n\n1. Each word creates Q, K, V vectors\n2. Calculate attention scores: Q √ó K^T\n3. Apply softmax to get attention weights\n4. Weighted sum of values produces output\n\n## Multi-Head Attention\n\n**Why Multiple Heads?**\n- Different heads learn different relationships\n- One head might focus on syntax, another on semantics\n- Parallel processing of diverse patterns\n\n**Example:** In \"The bank by the river flooded\"\n- Head 1: 'bank' attends to 'river' (location context)\n- Head 2: 'bank' attends to 'flooded' (disaster context)\n\n## Positional Encodings\n\n**The Problem:** Self-attention has no inherent sense of word order\n\n**The Solution:** Add positional information to embeddings\n- Sine and cosine functions of different frequencies\n- Allows model to understand sequence relationships\n- Enables processing of arbitrary-length sequences\n\n## Layer Architecture\n\n**Encoder-Decoder vs. Decoder-Only:**\n- Original Transformers: Both encoder and decoder\n- GPT Models: Decoder-only (autoregressive)\n- BERT Models: Encoder-only (bidirectional)\n\n**Each Layer Contains:**\n1. Multi-head self-attention\n2. Layer normalization\n3. Feed-forward neural network\n4. Residual connections",
        keyPoints: [
          "Self-attention weighs word relationships dynamically",
          "Query-Key-Value mechanism enables parallel processing",
          "Multi-head attention captures diverse patterns",
          "Positional encodings preserve sequence information",
          "Residual connections enable training of deep networks"
        ],
        handsOnExercise: "Visualize attention patterns in a simple sentence using online tools like BertViz to see which words attend to each other."
      },
      {
        id: "lesson-1-3",
        title: "LLM Training and Fine-Tuning Methodologies",
        duration: "3 hours",
        content: "# Training Large Language Models\n\nUnderstanding how LLMs are trained helps you leverage their capabilities effectively through prompting.\n\n## Pre-Training: The Foundation\n\n### Objective\nLearn general language understanding from massive unlabeled text corpora (trillions of tokens).\n\n### Training Process\n1. **Data Collection:** Crawl internet, books, code repositories\n2. **Tokenization:** Convert text to model-readable format\n3. **Next-Token Prediction:** Learn P(word_n | word_1...word_n-1)\n4. **Optimization:** Adjust billions of parameters via gradient descent\n\n### Compute Requirements\n- Training GPT-3: ~$4.6M in compute costs\n- Thousands of GPUs for weeks/months\n- Distributed training across data centers\n\n## Instruction Fine-Tuning (IFT)\n\n### Purpose\nAdapt pre-trained models to follow human instructions and perform specific tasks.\n\n### The Process\n1. **Dataset Creation:** Collect instruction-response pairs\n   - \"Summarize this article\" ‚Üí [summary]\n   - \"Translate to French\" ‚Üí [translation]\n2. **Supervised Learning:** Train model on these examples\n3. **Result:** Model learns to follow instructions\n\n### Key Datasets\n- FLAN: 1,800+ tasks across 60+ languages\n- Alpaca: 52K instruction-following examples\n- Dolly: High-quality human-generated pairs\n\n## Reinforcement Learning from Human Feedback (RLHF)\n\n### The Alignment Problem\nPre-trained models may generate harmful, biased, or unhelpful content.\n\n### RLHF Pipeline\n\n**Step 1: Collect Comparison Data**\n- Show humans multiple model outputs\n- Rank responses by quality/helpfulness\n- Build preference dataset\n\n**Step 2: Train Reward Model**\n- Learn to predict human preferences\n- Outputs scalar reward for any response\n\n**Step 3: Optimize with PPO**\n- Use Proximal Policy Optimization\n- Model learns to maximize predicted reward\n- Maintains diversity, avoids mode collapse\n\n### Constitutional AI (Alternative)\n- Models critique their own outputs\n- Self-improve based on principles\n- Reduces need for human feedback\n\n## Impact on Prompting\n\n**Understanding training helps you:**\n- Know what models are good/bad at\n- Recognize training data influences\n- Craft prompts that align with training objectives\n- Anticipate potential biases or limitations",
        keyPoints: [
          "Pre-training creates general language understanding",
          "Instruction fine-tuning teaches models to follow directions",
          "RLHF aligns models with human preferences",
          "Training methodology shapes model capabilities",
          "Understanding training helps craft better prompts"
        ],
        handsOnExercise: "Compare responses from base models vs. instruction-tuned models for the same prompt to see the impact of fine-tuning."
      },
      {
        id: "lesson-1-4",
        title: "A Comparative Guide to Modern LLMs",
        duration: "2 hours",
        content: "# Modern LLM Landscape (2025)\n\nChoosing the right model for your use case is crucial for effective prompt engineering.\n\n## Major Model Families\n\n### GPT Series (OpenAI)\n\n**GPT-4 Turbo & GPT-4o**\n- **Context:** 128K tokens\n- **Strengths:** Reasoning, coding, multimodal understanding\n- **Best For:** Complex analysis, creative writing, code generation\n- **Limitations:** Cost, speed trade-offs\n\n**GPT-3.5 Turbo**\n- **Context:** 16K tokens\n- **Strengths:** Fast, cost-effective, good general performance\n- **Best For:** High-volume applications, simple tasks\n\n### Claude (Anthropic)\n\n**Claude 3.5 Sonnet**\n- **Context:** 200K tokens\n- **Strengths:** Long document analysis, nuanced reasoning, safety\n- **Best For:** Research, document analysis, ethical applications\n- **Key Feature:** Constitutional AI for enhanced safety\n\n**Claude 3 Opus**\n- **Context:** 200K tokens\n- **Strengths:** Highest capability in Claude family\n- **Best For:** Complex tasks requiring deep reasoning\n\n### Llama (Meta)\n\n**Llama 3.1 (405B)**\n- **Context:** 128K tokens\n- **Strengths:** Open source, customizable, strong multilingual\n- **Best For:** On-premise deployment, customization needs\n- **Advantage:** No API costs for self-hosting\n\n### Gemini (Google)\n\n**Gemini 1.5 Pro**\n- **Context:** 1M tokens (industry-leading)\n- **Strengths:** Massive context, multimodal, fast\n- **Best For:** Full document processing, video analysis\n- **Innovation:** Native multimodal architecture\n\n## Specialized Models\n\n### Code-Focused\n- **GitHub Copilot:** Code completion and generation\n- **CodeLlama:** Open-source code specialist\n- **Replit Ghostwriter:** Context-aware coding assistant\n\n### Domain-Specific\n- **Med-PaLM:** Medical knowledge and diagnosis\n- **BloombergGPT:** Financial analysis\n- **Galactica:** Scientific research\n\n## Selection Criteria\n\n### By Use Case\n\n**Creative Writing:** GPT-4, Claude 3 Opus\n- Rich vocabulary, narrative coherence\n\n**Code Generation:** GPT-4, CodeLlama\n- Syntax accuracy, debugging capabilities\n\n**Data Analysis:** Claude 3.5 Sonnet, Gemini 1.5 Pro\n- Long context for full datasets\n\n**Customer Service:** GPT-3.5 Turbo, Llama 3.1\n- Cost-effective, fast responses\n\n### By Constraints\n\n**Budget-Limited:** Llama (self-hosted), GPT-3.5\n**Privacy-Critical:** Llama (on-premise)\n**Long Documents:** Gemini 1.5 Pro, Claude\n**Speed-Critical:** GPT-3.5 Turbo, smaller Llama variants\n\n## Performance Benchmarks\n\n**MMLU (Multitask Understanding):**\n- GPT-4: 86.4%\n- Claude 3 Opus: 86.8%\n- Gemini 1.5 Pro: 85.9%\n- Llama 3.1 405B: 85.2%\n\n**HumanEval (Code Generation):**\n- GPT-4: 67%\n- Claude 3.5 Sonnet: 92%\n- Llama 3.1 405B: 61%\n\n## Cost Comparison (per 1M tokens)\n\n- GPT-4 Turbo: $10 input / $30 output\n- GPT-3.5 Turbo: $0.50 input / $1.50 output\n- Claude 3.5 Sonnet: $3 input / $15 output\n- Gemini 1.5 Pro: $3.50 input / $10.50 output\n- Llama 3.1: Self-hosted (infrastructure costs)",
        keyPoints: [
          "Different models excel at different tasks",
          "Context window size varies from 16K to 1M tokens",
          "Cost-performance trade-offs are significant",
          "Open-source options offer customization and privacy",
          "Benchmark scores help but real-world testing is crucial"
        ],
        handsOnExercise: "Test the same prompt across 3 different models and compare outputs, speed, and cost for your specific use case."
      },
      {
        id: "lesson-1-5",
        title: "What is Prompt Engineering?",
        duration: "2 hours",
        content: "# Understanding Prompt Engineering\n\nPrompt engineering is the art and science of crafting effective instructions to guide AI models toward desired outputs.\n\n## Core Elements of a Prompt\n\n1. **Instructions:** Clear directions about what you want\n2. **Questions:** Specific queries that need answers\n3. **Input Data:** Information the model should process\n4. **Examples:** Demonstrations of desired outputs\n\n## Why It Matters\n\n**Cost-Effective:** Improve model performance without retraining\n\n**Accessible:** Anyone can become an AI developer\n\n**Immediate Results:** No need for expensive fine-tuning\n\n## Different from Traditional Programming\n\nTraditional programming is explicit and deterministic. Prompt engineering is suggestive and probabilistic - you guide the AI rather than command it exactly.",
        keyPoints: [
          "Prompt engineering is communication with AI",
          "Prompts contain instructions, questions, data, and examples",
          "More cost-effective than model fine-tuning",
          "Anyone can learn prompt engineering",
          "Different from traditional programming"
        ]
      },
      {
        id: "lesson-1-6",
        title: "Advanced Tokenization and Sampling Strategies",
        duration: "2 hours",
        content: "# Tokenization and Sampling Deep Dive\n\nMastering tokens and sampling parameters gives you fine control over model behavior.\n\n## Tokenization Strategies\n\n### Byte-Pair Encoding (BPE)\n**Used by:** GPT models, many others\n\n**How it works:**\n1. Start with character-level tokens\n2. Iteratively merge most frequent pairs\n3. Build vocabulary of subwords\n\n**Example:** \"unhappiness\"\n- Initial: [u, n, h, a, p, p, i, n, e, s, s]\n- After merging: [un, happi, ness]\n\n**Advantages:**\n- Handles rare words effectively\n- Language-agnostic approach\n- Balances vocabulary size and coverage\n\n### WordPiece\n**Used by:** BERT, some Google models\n\n**Difference from BPE:**\n- Chooses merges based on likelihood maximization\n- Often prefixes subwords with ##\n- Example: \"unhappiness\" ‚Üí [un, ##happi, ##ness]\n\n### SentencePiece\n**Used by:** Llama, T5\n\n**Key Feature:**\n- Treats spaces as tokens\n- Works directly on raw text\n- Better multilingual support\n\n## Token Count Impact on Prompts\n\n**Why Token Counts Matter:**\n- Cost (billed per token)\n- Context limits (hard boundaries)\n- Latency (more tokens = slower)\n\n**Optimization Strategies:**\n1. Be concise but not cryptic\n2. Use technical terms (often fewer tokens)\n3. Avoid repetition\n4. Compress examples when possible\n\n**Example:**\n- Verbose (20 tokens): \"I would like you to please analyze this data and provide insights\"\n- Optimized (11 tokens): \"Analyze this data and provide insights\"\n\n## Sampling Parameters\n\n### Temperature (0.0 - 2.0)\n\n**Controls randomness and creativity**\n\n**Temperature = 0.0:** Deterministic (always picks highest probability token)\n- Use for: Math, code, factual Q&A\n- Output: Consistent, safe, predictable\n\n**Temperature = 0.7:** Balanced creativity\n- Use for: General content, conversation\n- Output: Natural variation, mostly coherent\n\n**Temperature = 1.5+:** High creativity\n- Use for: Creative writing, brainstorming\n- Output: Diverse, sometimes unexpected\n\n### Top-p (Nucleus Sampling)\n\n**Selects from smallest set of tokens whose cumulative probability exceeds p**\n\n**Top-p = 0.1:** Very focused\n- Only considers top 10% probability mass\n- Conservative, safe outputs\n\n**Top-p = 0.9:** Balanced (recommended default)\n- Allows variety while excluding unlikely tokens\n- Good for most applications\n\n**Top-p = 1.0:** All tokens considered\n- Maximum diversity\n- Risk of incoherent outputs\n\n### Top-k Sampling\n\n**Limits selection to k most likely tokens**\n\n**Top-k = 1:** Same as temperature 0\n- Deterministic selection\n\n**Top-k = 40:** Moderate variety\n- Considers 40 most likely next tokens\n- Prevents very unlikely words\n\n**Top-k = 100+:** High diversity\n- Wide selection pool\n\n### Frequency & Presence Penalties\n\n**Frequency Penalty:** Reduces probability of repeated tokens\n- Range: 0.0 to 2.0\n- Use: Prevent repetitive outputs\n\n**Presence Penalty:** Encourages new topics\n- Range: 0.0 to 2.0\n- Use: Increase topic diversity\n\n## Practical Guidelines\n\n**For Factual Tasks:**\n- Temperature: 0.0-0.3\n- Top-p: 0.1-0.5\n- Penalties: Low (0.0-0.3)\n\n**For Creative Tasks:**\n- Temperature: 0.8-1.2\n- Top-p: 0.9-0.95\n- Penalties: Medium (0.5-1.0)\n\n**For Balanced General Use:**\n- Temperature: 0.7\n- Top-p: 0.9\n- Penalties: 0.0\n\n## Interactive Token Visualization\n\n**Try these tools:**\n- OpenAI Tokenizer: Count tokens for different models\n- Hugging Face Tokenizer: Compare tokenization strategies\n- GPT-3 Playground: Experiment with sampling parameters\n\n**Exercise:** Take a paragraph and see how different models tokenize it. Notice:\n- Technical terms may be 1 or many tokens\n- Spaces and punctuation handling\n- Code vs. natural language differences",
        keyPoints: [
          "BPE, WordPiece, and SentencePiece have different strengths",
          "Token efficiency reduces costs and fits more context",
          "Temperature controls creativity vs. consistency",
          "Top-p and top-k limit token selection pools",
          "Penalties reduce repetition and encourage diversity"
        ],
        handsOnExercise: "Experiment with temperature settings: Generate creative story at 1.2, then factual summary at 0.2. Compare coherence and variety."
      }
    ]
  },
  {
    id: "module-2",
    title: "Basic Prompting Techniques",
    description: "Master the fundamental techniques including zero-shot, few-shot learning, and instruction design best practices.",
    duration: "14-18 hours",
    lessons: [
      {
        id: "lesson-2-1",
        title: "Zero-Shot and Few-Shot Learning",
        duration: "3 hours",
        content: "# Zero-Shot and Few-Shot Learning\n\n## Zero-Shot Prompting\n\nAsk the model to perform a task without any examples.\n\n**When to Use:**\n- Simple, well-defined tasks\n- When the model already understands the task type\n- To save tokens and reduce prompt length\n\n**Best Practice:** Be extremely specific and clear about what you want.\n\n## Few-Shot Learning\n\nProvide 2-5 examples to demonstrate the desired output pattern.\n\n**When to Use:**\n- Complex or unusual tasks\n- Specific formatting requirements\n- Consistent style or tone needs\n\n**Key Strategy:** Quality over quantity - 2-3 excellent examples beat 10 mediocre ones.",
        keyPoints: [
          "Zero-shot: no examples, just clear instructions",
          "Few-shot: 2-5 examples to demonstrate pattern",
          "Choose examples wisely for best results",
          "Quality of examples matters more than quantity",
          "Different tasks need different approaches"
        ],
        handsOnExercise: "Create both zero-shot and few-shot prompts for the same task. Compare the results."
      },
      {
        id: "lesson-2-2",
        title: "Instruction Design Best Practices",
        duration: "3 hours",
        content: "# Writing Effective Instructions\n\n## Core Principles\n\n### Be Specific and Precise\nBad: Write about dogs\nGood: Write a 200-word paragraph about Golden Retrievers' temperament for a pet adoption website\n\n### Avoid Negative Instructions\nBad: Don't be too technical\nGood: Explain in simple terms a 12-year-old would understand\n\n### Define Context and Audience\nAlways include:\n- Who will read this?\n- What's the purpose?\n- What's the desired tone?\n\n## Common Pitfalls\n\n1. **Ambiguous Language:** Be clear and specific\n2. **Missing Context:** Provide all relevant information\n3. **Overwhelming Complexity:** Break into focused prompts",
        keyPoints: [
          "Specificity is crucial for good results",
          "Avoid negative instructions - state what you want",
          "Always define audience and context",
          "Common pitfalls: ambiguity, missing context, complexity"
        ],
        handsOnExercise: "Transform a vague prompt into a highly specific instruction. Test both and compare outputs."
      },
      {
        id: "lesson-2-3",
        title: "Mastering Prompt Structure and Formatting",
        duration: "4 hours",
        content: "# Prompt Structure and Formatting\n\nWell-structured prompts dramatically improve model performance and output quality.\n\n## Using Delimiters Effectively\n\n### Why Delimiters Matter\n\nDelimiters clearly separate instructions from content, preventing confusion and injection attacks.\n\n**Common Delimiter Patterns:**\n\n**Triple Quotes:**\n```\nSummarize the following text:\n\"\"\"\n[User's text here]\n\"\"\"\n```\n\n**Triple Hashtags:**\n```\nAnalyze this customer review:\n###\n[Review content]\n###\n```\n\n**XML Tags:**\n```\n<instruction>Translate to Spanish</instruction>\n<text>\n[English text]\n</text>\n```\n\n**Markdown Code Blocks:**\n```\nDebug this code:\n```python\n[Code here]\n```\n```\n\n### Benefits\n- **Security:** Prevents prompt injection\n- **Clarity:** Model knows where content begins/ends\n- **Structure:** Enables complex multi-part prompts\n\n## Structured Output Formats\n\n### JSON Output\n\n**Prompt Pattern:**\n```\nExtract entities from this text and return JSON:\n\nText: \"Apple Inc. CEO Tim Cook announced iPhone 15 in Cupertino.\"\n\nReturn format:\n{\n  \"company\": string,\n  \"person\": string,\n  \"product\": string,\n  \"location\": string\n}\n```\n\n**Why JSON?**\n- Easy to parse programmatically\n- Language-agnostic\n- Supports nested structures\n- Type-safe with schemas\n\n### XML Output\n\n**When to Use:**\n- Hierarchical data\n- Document-style content\n- Legacy system integration\n\n**Example:**\n```\n<article>\n  <title>...</title>\n  <summary>...</summary>\n  <keyPoints>\n    <point>...</point>\n  </keyPoints>\n</article>\n```\n\n### Markdown Tables\n\n**Perfect For:**\n- Comparative data\n- Structured lists\n- Reports\n\n**Prompt:**\n```\nCompare these products in a markdown table:\n| Feature | Product A | Product B |\n|---------|-----------|-----------|  \n```\n\n## System, User, and Assistant Roles\n\n### Role-Based Prompting\n\n**System Message:**\n- Sets overall behavior and context\n- Persistent across conversation\n- Defines constraints and style\n\n```\nSystem: You are a Python expert who writes clean, documented code.\nUser: Write a function to sort a list.\n```\n\n**User Message:**\n- The actual request or query\n- Changes with each interaction\n- Contains the task or question\n\n**Assistant Message:**\n- Previous model responses\n- Provides conversation history\n- Enables context continuity\n\n### Multi-Turn Conversations\n\n**Structure:**\n```\nSystem: Financial analyst with 15 years experience\nUser: Analyze AAPL stock performance\nAssistant: [Previous analysis]\nUser: Now compare with MSFT\n```\n\n## Advanced Formatting Techniques\n\n### Numbered Instructions\n\n**For Sequential Tasks:**\n```\n1. Extract all email addresses\n2. Validate each address format\n3. Group by domain\n4. Return as JSON sorted by count\n```\n\n### Section Headers\n\n**For Complex Prompts:**\n```\n## TASK\nTranslate technical documentation\n\n## CONTEXT\nSoftware engineering audience, B2B\n\n## CONSTRAINTS\n- Preserve code examples\n- Maintain technical accuracy\n- Target language: Spanish\n\n## INPUT\n[Document here]\n```\n\n### Template Variables\n\n**Reusable Prompts:**\n```\nAnalyze {TOPIC} from perspective of {ROLE}.\n\nConsider:\n- {ASPECT_1}\n- {ASPECT_2}\n- {ASPECT_3}\n\nProvide {OUTPUT_LENGTH} response in {FORMAT}.\n```\n\n## Parsing Structured Output\n\n### JavaScript/TypeScript\n```javascript\nconst response = await callLLM(prompt);\nconst data = JSON.parse(response);\n// Type-safe access\n```\n\n### Python\n```python\nimport json\nresponse = llm.generate(prompt)\ndata = json.loads(response)\n```\n\n### Error Handling\n\n**Robust Parsing:**\n1. Validate format before parsing\n2. Use try-catch blocks\n3. Provide fallback for malformed output\n4. Log parsing errors for prompt improvement\n\n## Best Practices\n\n1. **Be Consistent:** Use same delimiters throughout\n2. **Be Explicit:** Specify exact output format\n3. **Be Defensive:** Anticipate parsing errors\n4. **Be Clear:** Visual structure aids model understanding\n5. **Be Tested:** Validate format with multiple examples",
        keyPoints: [
          "Delimiters prevent injection and clarify structure",
          "JSON/XML enable programmatic output parsing",
          "System/User/Assistant roles organize conversations",
          "Template variables create reusable prompts",
          "Structured output requires robust error handling"
        ],
        handsOnExercise: "Create a prompt with XML delimiters that returns JSON output. Test parsing in your preferred language."
      },
      {
        id: "lesson-2-4",
        title: "Advanced Few-Shot Learning and Prompt Templating",
        duration: "4 hours",
        content: "# Advanced Few-Shot Learning\n\nMaster the art of example selection and template creation for maximum effectiveness.\n\n## Example Selection Strategies\n\n### Diversity Over Similarity\n\n**Bad Approach:** All similar examples\n```\nExample 1: \"Great product!\" ‚Üí Positive\nExample 2: \"Love it!\" ‚Üí Positive  \nExample 3: \"Amazing!\" ‚Üí Positive\n```\n\n**Good Approach:** Diverse examples\n```\nExample 1: \"Great product!\" ‚Üí Positive\nExample 2: \"Terrible quality\" ‚Üí Negative\nExample 3: \"It's okay, not special\" ‚Üí Neutral\n```\n\n### Edge Case Coverage\n\n**Include Corner Cases:**\n- Ambiguous inputs\n- Unusual formatting\n- Mixed sentiments\n- Domain-specific jargon\n\n**Example for Sentiment Analysis:**\n```\n\"This camera is great, but the lens cap broke\" ‚Üí Mixed\n\"Not bad for the price\" ‚Üí Conditional Positive\n\"Meh üòê\" ‚Üí Neutral\n\"10/10 would NOT recommend\" ‚Üí Negative (sarcasm)\n```\n\n### Balanced Representation\n\n**For Classification:**\n- Equal examples per class\n- Prevents bias toward frequent labels\n- Improves minority class performance\n\n**Example:**\n```\n2 Positive examples\n2 Negative examples\n2 Neutral examples\n= Balanced 6-shot learning\n```\n\n## Prompt Templating\n\n### Dynamic Prompt Templates\n\n**Template Structure:**\n```python\ntemplate = \"\"\"\nRole: {role}\nTask: {task}\n\nExamples:\n{examples}\n\nNow analyze:\n{input}\n\nOutput format: {format}\n\"\"\"\n\n# Usage\nprompt = template.format(\n  role=\"Senior Data Analyst\",\n  task=\"Extract key metrics\",\n  examples=example_string,\n  input=user_data,\n  format=\"JSON\"\n)\n```\n\n### Template Libraries\n\n**Build Reusable Templates:**\n\n```python\nTEMPLATES = {\n  'sentiment': \"Classify sentiment: {text}\\nSentiment:\",\n  'summarize': \"Summarize in {words} words:\\n{text}\",\n  'extract': \"Extract {entity_type} from:\\n{text}\",\n  'translate': \"Translate to {lang}:\\n{text}\"\n}\n\ndef get_prompt(template_name, **kwargs):\n  return TEMPLATES[template_name].format(**kwargs)\n```\n\n### Conditional Templates\n\n**Task-Specific Logic:**\n```python\ndef build_prompt(task_type, data):\n  if task_type == 'short':\n    return f\"Briefly: {data}\"\n  elif task_type == 'detailed':\n    examples = load_examples(task_type)\n    return f\"{examples}\\n\\nNow analyze: {data}\"\n```\n\n## Optimizing Prompt Length\n\n### Token Budget Management\n\n**Strategy 1: Example Compression**\n```\n# Verbose (150 tokens)\nInput: \"The customer service was absolutely terrible\"\nOutput: \"Sentiment: Negative, Confidence: High, Aspect: Service\"\n\n# Compressed (80 tokens)\nIn: \"terrible customer service\"\nOut: \"NEG|service|0.95\"\n```\n\n**Strategy 2: Selective Examples**\n- Start with 5 examples\n- Measure performance\n- Remove least informative example\n- Repeat until optimal\n\n**Strategy 3: Example Rotation**\n- Maintain example bank\n- Select most relevant for each query\n- Use semantic similarity for selection\n\n### Cost-Performance Analysis\n\n**Calculate ROI:**\n```\nCost = (input_tokens + output_tokens) √ó price_per_token\nPerformance = accuracy_metric\nROI = Performance / Cost\n```\n\n**Optimization Process:**\n1. Baseline: Zero-shot (cheapest)\n2. Add examples incrementally\n3. Plot performance vs. cost\n4. Find optimal point\n\n## Advanced Few-Shot Patterns\n\n### Chain-of-Thought Few-Shot\n\n**Show Reasoning in Examples:**\n```\nQ: John has 15 apples, gives 4 to Mary. How many left?\nThought: Start with 15, subtract 4\nA: 11\n\nQ: Sarah has 23 candies, eats 7, buys 12 more. Total?\nThought: 23 - 7 = 16, then 16 + 12 = 28\nA: 28\n\nQ: [New problem]\n```\n\n### Contrastive Examples\n\n**Show Good vs. Bad:**\n```\n‚ùå Bad: \"Write code\"\n‚úÖ Good: \"Write a Python function that validates email addresses using regex\"\n\n‚ùå Bad: \"Summarize article\"\n‚úÖ Good: \"Summarize this medical research article in 3 bullet points for healthcare professionals\"\n\nNow improve: [User's vague prompt]\n```\n\n### Meta-Learning Examples\n\n**Teach Pattern Recognition:**\n```\nThese prompts get good results:\n1. Specific task + context + format\n2. Role definition + examples + constraints\n3. Step-by-step instructions + validation criteria\n\nAnalyze why this prompt works: [Prompt]\n```\n\n## A/B Testing Prompts\n\n### Experimental Design\n\n**Version A:**\n```\nClassify sentiment:\nReview: {text}\nSentiment:\n```\n\n**Version B:**\n```\nYou are a sentiment expert.\n\nExamples:\n[3 examples]\n\nReview: {text}\nSentiment (Positive/Negative/Neutral):\n```\n\n### Metrics to Track\n- Accuracy / F1 Score\n- Latency (response time)\n- Cost per request\n- User satisfaction (if applicable)\n\n### Statistical Significance\n```python\nfrom scipy import stats\n\n# Compare two prompt versions\nversion_a_scores = [0.85, 0.87, 0.86, ...]\nversion_b_scores = [0.90, 0.91, 0.89, ...]\n\nt_stat, p_value = stats.ttest_ind(version_a_scores, version_b_scores)\n\nif p_value < 0.05:\n  print(\"Version B is significantly better\")\n```\n\n## Prompt Version Control\n\n### Git-Based Workflow\n\n```bash\n# Track prompt changes\ngit add prompts/sentiment_v2.txt\ngit commit -m \"feat: add contrastive examples to sentiment prompt\"\ngit tag v2.1.0\n```\n\n### Changelog Format\n```markdown\n## v2.1.0 - 2025-01-15\n### Added\n- 3 contrastive examples for edge cases\n- Explicit output format specification\n\n### Changed\n- Simplified role description\n- Reduced from 6 to 4 examples\n\n### Performance\n- Accuracy: 0.82 ‚Üí 0.89 (+8.5%)\n- Cost: $0.015 ‚Üí $0.012 (-20%)\n```\n\n### Rollback Strategy\n```python\nPROMPT_VERSIONS = {\n  'v1.0': \"...\",\n  'v2.0': \"...\",\n  'v2.1': \"...\"\n}\n\n# Easy rollback if v2.1 underperforms\ncurrent_prompt = PROMPT_VERSIONS['v2.0']\n```",
        keyPoints: [
          "Diverse examples outperform similar ones",
          "Include edge cases and corner scenarios",
          "Template systems enable reusability and consistency",
          "Token optimization balances cost and performance",
          "A/B testing validates prompt improvements"
        ],
        handsOnExercise: "Create a prompt template library with 5 reusable templates. A/B test two few-shot example sets and measure accuracy difference."
      },
      {
        id: "lesson-2-5",
        title: "Cross-Model Prompting Best Practices",
        duration: "3 hours",
        content: "# Cross-Model Prompting\n\nDifferent models require different prompting approaches. Learn to adapt effectively.\n\n## Model-Specific Characteristics\n\n### GPT Models (OpenAI)\n\n**Strengths:**\n- Follows detailed instructions well\n- Strong at creative tasks\n- Good with structured output\n\n**Prompting Tips:**\n- Be explicit about format requirements\n- Use system messages for persistent context\n- Temperature 0.7-1.0 for creative tasks\n\n**Example:**\n```\nSystem: You are a technical writer.\n\nUser: Write API documentation for:\n[function signature]\n\nFormat:\n## Description\n## Parameters\n## Returns\n## Example\n```\n\n### Claude (Anthropic)\n\n**Strengths:**\n- Excellent at nuanced analysis\n- Strong safety guardrails\n- Verbose by default\n\n**Prompting Tips:**\n- Ask for conciseness if needed: \"Be brief\"\n- Leverages XML tags naturally\n- Excels at long-form content\n\n**Example:**\n```\n<instruction>Analyze this contract</instruction>\n<document>[contract]</document>\n<format>Bullet points only</format>\n<focus>Risk factors</focus>\n```\n\n### Llama Models (Meta)\n\n**Strengths:**\n- Good multilingual capabilities\n- Fast inference\n- Customizable\n\n**Prompting Tips:**\n- More instruction-following if fine-tuned\n- May need more explicit examples\n- Test different prompt formats\n\n**Example:**\n```\n### Instruction\nTranslate to Spanish\n\n### Input\n[English text]\n\n### Response\n```\n\n### Gemini (Google)\n\n**Strengths:**\n- Massive context window (1M tokens)\n- Strong multimodal capabilities\n- Fast processing\n\n**Prompting Tips:**\n- Leverage full documents without chunking\n- Native image + text understanding\n- Good at factual tasks\n\n**Example:**\n```\nAnalyze this entire research paper:\n[paste full 50-page PDF]\n\nProvide:\n1. Executive summary\n2. Methodology critique\n3. Key findings\n```\n\n## Adapting Prompts Across Models\n\n### Universal Prompt Pattern\n\n**Core Structure:**\n```\n1. Role/Context\n2. Task description\n3. Input data\n4. Output format\n5. Constraints\n```\n\n**Works across all models with minor adjustments.**\n\n### Model-Specific Adjustments\n\n**GPT Optimization:**\n```python\ndef adapt_for_gpt(base_prompt):\n  return f\"\"\"\nSystem: {base_prompt['role']}\n\nUser: {base_prompt['task']}\n\nInput:\n{base_prompt['data']}\n\nFormat: {base_prompt['format']}\n\"\"\"\n```\n\n**Claude Optimization:**\n```python\ndef adapt_for_claude(base_prompt):\n  return f\"\"\"\n<role>{base_prompt['role']}</role>\n<task>{base_prompt['task']}</task>\n<input>{base_prompt['data']}</input>\n<format>{base_prompt['format']}</format>\n\"\"\"\n```\n\n**Llama Optimization:**\n```python\ndef adapt_for_llama(base_prompt):\n  return f\"\"\"\n### Role\n{base_prompt['role']}\n\n### Task  \n{base_prompt['task']}\n\n### Input\n{base_prompt['data']}\n\n### Format\n{base_prompt['format']}\n\n### Response\n\"\"\"\n```\n\n## Handling Model Sensitivities\n\n### Content Filtering\n\n**GPT:** Moderate filtering\n- Refuses harmful/illegal requests\n- Sometimes overly cautious\n\n**Claude:** Strong safety focus\n- Extensive refusal training\n- May decline edge cases\n\n**Workaround for legitimate uses:**\n```\nContext: This is for educational cybersecurity training.\nTask: Explain how SQL injection works.\nPurpose: Help developers write secure code.\n```\n\n### Instruction Following\n\n**GPT-4:** Excellent\n- Follows complex multi-step instructions\n- Rarely deviates from format\n\n**GPT-3.5:** Good\n- May miss subtle instructions\n- Occasionally skips steps\n\n**Claude:** Excellent but verbose\n- Follows instructions precisely\n- May add extra explanation\n\n**Llama:** Variable\n- Depends on fine-tuning\n- Test instruction adherence\n\n### Hallucination Tendencies\n\n**All models hallucinate, but differently:**\n\n**GPT:** Confident hallucinations\n- States false facts assertively\n- Mitigation: Ask for sources\n\n**Claude:** Admits uncertainty more\n- Says \"I don't know\" when unsure\n- Mitigation: Still verify facts\n\n**Mitigation Strategy:**\n```\nIf you're unsure, say \"I don't have enough information\" rather than guessing.\n\nFor factual claims, cite sources or note uncertainty.\n```\n\n## Prompt Versioning Strategy\n\n### Multi-Model Prompt Registry\n\n**Structure:**\n```python\nPROMPTS = {\n  'summarize': {\n    'base': \"Summarize: {text}\",\n    'gpt': \"Provide a concise summary: {text}\",\n    'claude': \"<task>Summarize briefly</task><text>{text}</text>\",\n    'llama': \"### Summarize\\n{text}\\n### Summary\"\n  }\n}\n\ndef get_prompt(task, model, **kwargs):\n  template = PROMPTS[task].get(model) or PROMPTS[task]['base']\n  return template.format(**kwargs)\n```\n\n### Fallback Chain\n\n**If primary model fails:**\n```python\nMODEL_PRIORITY = ['gpt-4', 'claude', 'gpt-3.5', 'llama']\n\nfor model in MODEL_PRIORITY:\n  try:\n    prompt = get_prompt(task, model, data=data)\n    result = call_model(model, prompt)\n    if validate(result):\n      return result\n  except Exception as e:\n    log_error(model, e)\n    continue\n```\n\n## Testing Across Models\n\n### Benchmark Suite\n\n```python\nTEST_CASES = [\n  {\"input\": \"...\", \"expected\": \"...\"},\n  # ... more cases\n]\n\nresults = {}\nfor model in ['gpt-4', 'claude', 'llama']:\n  scores = []\n  for case in TEST_CASES:\n    prompt = adapt_prompt(case['input'], model)\n    output = call_model(model, prompt)\n    score = evaluate(output, case['expected'])\n    scores.append(score)\n  results[model] = np.mean(scores)\n```\n\n### Model Selection Logic\n\n**Decision Framework:**\n```python\ndef select_model(task_type, context_size, budget):\n  if context_size > 100000:\n    return 'gemini'  # Largest context\n  elif task_type == 'creative':\n    return 'gpt-4'   # Best creative\n  elif task_type == 'analysis':\n    return 'claude'  # Best analytical\n  elif budget == 'low':\n    return 'llama'   # Self-hosted\n  else:\n    return 'gpt-3.5' # Balanced default\n```",
        keyPoints: [
          "Each model family has unique strengths and prompting styles",
          "GPT uses system messages, Claude prefers XML, Llama uses markdown",
          "Adapt base prompts to model-specific formats",
          "Build prompt registries for multi-model deployment",
          "Test across models to find optimal choice per task"
        ],
        handsOnExercise: "Take one prompt and adapt it for GPT, Claude, and Llama. Test all three versions and compare quality, cost, and speed."
      }
    ]
  },
  {
    id: "module-3",
    title: "Intermediate Prompting Strategies",
    description: "Explore chain-of-thought prompting, self-consistency, role-playing, and safety considerations.",
    duration: "16-20 hours",
    lessons: [
      {
        id: "lesson-3-1",
        title: "Advanced Chain-of-Thought and Self-Consistency",
        duration: "5 hours",
        content: "# Advanced Chain-of-Thought Prompting\n\nMaster sophisticated reasoning techniques for complex problem-solving.\n\n## Chain-of-Thought Fundamentals\n\n**The Magic Phrase:** \"Let's think step by step\"\n\nThis simple addition can improve accuracy by 40%+ on reasoning tasks.\n\n### When CoT Works Best\n- Math and logic problems\n- Multi-step planning\n- Causal reasoning\n- Complex analysis\n\n### Basic CoT Pattern\n```\nProblem: A store had 20 apples. They sold 12 in the morning and received a shipment of 15 in the afternoon. How many do they have now?\n\nLet's think step by step:\n1. Started with: 20 apples\n2. Sold in morning: 20 - 12 = 8 apples remaining\n3. Received shipment: 8 + 15 = 23 apples\n\nAnswer: 23 apples\n```\n\n## Self-Consistency Implementation\n\n### The Technique\nGenerate multiple independent reasoning paths, then select the most common answer through majority voting.\n\n### Why It Works\n- Reduces impact of random errors\n- Increases confidence in correct answers\n- Reveals ambiguity in problems\n\n### Implementation in Code\n\n```python\ndef self_consistency_prompt(problem, num_samples=5):\n  prompt = f\"{problem}\\n\\nLet's think step by step:\"\n  \n  answers = []\n  for i in range(num_samples):\n    response = llm.generate(prompt, temperature=0.7)\n    answer = extract_final_answer(response)\n    answers.append(answer)\n  \n  # Majority voting\n  from collections import Counter\n  vote_counts = Counter(answers)\n  final_answer = vote_counts.most_common(1)[0][0]\n  confidence = vote_counts[final_answer] / num_samples\n  \n  return final_answer, confidence\n```\n\n### Example Results\n```\nSample 1: 23 apples ‚úì\nSample 2: 23 apples ‚úì\nSample 3: 27 apples ‚úó\nSample 4: 23 apples ‚úì\nSample 5: 23 apples ‚úì\n\nFinal Answer: 23 apples (80% confidence)\n```\n\n## Prompt Chaining\n\n### Sequential Problem Decomposition\n\nBreak complex tasks into a series of simpler prompts where each output feeds the next input.\n\n**Example: Research Paper Summary**\n\n**Chain Step 1: Extract Key Information**\n```\nFrom this research paper, extract:\n1. Main hypothesis\n2. Methodology\n3. Key findings\n4. Limitations\n\n[Paper text]\n```\n\n**Chain Step 2: Analyze Quality**\n```\nBased on this extracted information:\n{output_from_step_1}\n\nEvaluate:\n- Methodology rigor (1-10)\n- Sample size adequacy\n- Potential biases\n```\n\n**Chain Step 3: Generate Summary**\n```\nUsing this analysis:\n{output_from_step_2}\n\nWrite a 200-word executive summary for non-experts.\n```\n\n### Benefits of Chaining\n- Each step is simpler and more focused\n- Easier to debug and optimize\n- Can use different models for different steps\n- Intermediate outputs provide transparency\n\n### Implementation Pattern\n\n```python\nclass PromptChain:\n  def __init__(self, steps):\n    self.steps = steps  # List of prompt templates\n  \n  def execute(self, initial_input):\n    context = {\"input\": initial_input}\n    \n    for step in self.steps:\n      prompt = step['template'].format(**context)\n      output = llm.generate(prompt)\n      context[step['output_key']] = output\n    \n    return context\n\n# Usage\nchain = PromptChain([\n  {\"template\": \"Summarize: {input}\", \"output_key\": \"summary\"},\n  {\"template\": \"Key points from: {summary}\", \"output_key\": \"points\"},\n  {\"template\": \"Action items from: {points}\", \"output_key\": \"actions\"}\n])\n\nresult = chain.execute(long_document)\nprint(result['actions'])\n```\n\n## Decomposition Strategies\n\n### Least-to-Most Prompting\n\nSolve simpler sub-problems first, then use those solutions for harder problems.\n\n**Example: Complex Math Problem**\n```\nOriginal: \"Calculate the compound interest on $10,000 at 5% annual rate for 3 years, compounded quarterly.\"\n\nStep 1 (Simpler): \"What is the quarterly interest rate if annual rate is 5%?\"\nAnswer: 1.25%\n\nStep 2 (Build on Step 1): \"How many compounding periods in 3 years if compounded quarterly?\"\nAnswer: 12 periods\n\nStep 3 (Final): \"Using r=1.25% and n=12, calculate: 10000 √ó (1 + 0.0125)^12\"\nAnswer: $11,607.55\n```\n\n### Problem Reduction\n\nTransform a complex problem into a known, solvable pattern.\n\n```\nComplex: \"How would introducing a 4-day work week affect employee productivity in tech companies?\"\n\nReduce to known patterns:\n1. \"What factors affect employee productivity?\" (Known research)\n2. \"How does work-life balance impact productivity?\" (Known research)\n3. \"What are case studies of 4-day weeks?\" (Known data)\n4. Synthesize answers ‚Üí Answer original question\n```\n\n## Advanced CoT Patterns\n\n### Analogical Reasoning CoT\n\n```\nProblem: How should a startup allocate a $500K seed round?\n\nLet's use analogies:\n\nAnalogy 1: Like building a house\n- Foundation (infrastructure): 30%\n- Frame (core product): 40%  \n- Finishing (marketing): 20%\n- Reserve (contingency): 10%\n\nAnalogy 2: Like a chess game\n- Opening (setup): 25%\n- Mid-game (growth): 50%\n- End-game (scale): 15%\n- Reserve: 10%\n\nSynthesis:\n- Infrastructure/Setup: ~30%\n- Core Product/Growth: ~45%\n- Marketing/Scale: ~15%\n- Reserve: ~10%\n```\n\n### Counterfactual CoT\n\n```\nDecision: Should we expand to European markets now?\n\nLet's consider what happens if we do AND if we don't:\n\nScenario A: We expand\n- Upside: ‚Ç¨2M potential revenue, brand recognition\n- Risks: ‚Ç¨500K costs, team distraction, regulatory complexity\n- Likelihood: 60% success rate\n\nScenario B: We don't expand  \n- Upside: Focus on US market, strengthen core\n- Risks: Competitors take EU market, missed opportunity\n- Likelihood: 70% US success rate\n\nExpected value:\nA: 0.6 √ó ‚Ç¨2M - ‚Ç¨500K = ‚Ç¨700K\nB: 0.7 √ó ‚Ç¨1M = ‚Ç¨700K\n\nConclusion: Similar EV, but B has less risk ‚Üí Wait 6 months\n```\n\n## Debugging CoT Prompts\n\n### Common Failure Modes\n\n**1. Skipping Steps**\nBad: \"The answer is 23\"\nGood: \"Starting with 20, minus 12 equals 8, plus 15 equals 23\"\n\n**2. Incorrect Logic**\n- Ask model to verify each step\n- Use self-critique: \"Check your work\"\n\n**3. Ambiguous Reasoning**\n- Make steps more explicit\n- Number each reasoning step\n\n### Testing Framework\n\n```python\ndef test_cot_quality(problem, response):\n  checks = {\n    'has_steps': '1.' in response or 'Step' in response,\n    'shows_math': any(op in response for op in ['+', '-', '√ó', '√∑']),\n    'has_conclusion': 'therefore' in response.lower() or 'answer:' in response.lower(),\n    'step_count': len([s for s in response.split('\\n') if s.strip()])\n  }\n  \n  quality_score = sum(checks.values()) / len(checks)\n  return quality_score, checks\n```\n\n## Performance Metrics\n\n**CoT Improvements (GPT-4):**\n- Math word problems: +18% accuracy\n- Logic puzzles: +25% accuracy\n- Multi-step planning: +31% accuracy\n- Code debugging: +22% success rate\n\n**Self-Consistency Improvements:**\n- With 5 samples: +12% over single CoT\n- With 10 samples: +15% over single CoT\n- Diminishing returns beyond 10 samples",
        keyPoints: [
          "Self-consistency uses majority voting across multiple reasoning paths",
          "Prompt chaining breaks complex tasks into sequential steps",
          "Least-to-most prompting solves simple sub-problems first",
          "Problem decomposition transforms complexity into known patterns",
          "CoT can improve accuracy by 20-40% on reasoning tasks"
        ],
        handsOnExercise: "Implement self-consistency with 5 samples for a logic puzzle. Compare single-shot accuracy vs. majority-voted answer."
      },
      {
        id: "lesson-3-2",
        title: "Tree of Thoughts: Exploring Multiple Reasoning Paths",
        duration: "4 hours",
        content: "# Tree of Thoughts (ToT) Framework\n\nSystematically explore and evaluate multiple reasoning strategies before converging on the best solution.\n\n## Core Concept\n\nUnlike linear Chain-of-Thought, ToT creates a tree structure where the model explores multiple branches and evaluates which path leads to the best solution.\n\n### Linear CoT vs. Tree of Thoughts\n\n**Chain of Thought (Linear):**\n```\nProblem ‚Üí Step 1 ‚Üí Step 2 ‚Üí Step 3 ‚Üí Answer\n```\n\n**Tree of Thoughts (Branching):**\n```\nProblem ‚Üí ‚î¨‚Üí Approach A ‚Üí ‚î¨‚Üí A1 ‚Üí Evaluate\n          ‚îÇ               ‚îî‚Üí A2 ‚Üí Evaluate\n          ‚îú‚Üí Approach B ‚Üí ‚Üí B1 ‚Üí Evaluate\n          ‚îî‚Üí Approach C ‚Üí ‚Üí C1 ‚Üí Evaluate\n                               ‚Üì\n                         Select Best\n```\n\n## ToT Implementation\n\n### Step 1: Generate Multiple Approaches\n\n```\nProblem: Design a caching strategy for a social media feed with 10M users.\n\nGenerate 3 fundamentally different architectural approaches:\n\nApproach 1: [Think about first approach]\nApproach 2: [Think about second approach]\nApproach 3: [Think about third approach]\n```\n\n**Model Output:**\n```\nApproach 1: CDN-based edge caching\n- Cache full feeds at CDN nodes\n- Pros: Low latency, scales globally\n- Cons: Stale data, high CDN costs\n\nApproach 2: In-memory cache (Redis) with write-through\n- Cache user feeds in Redis clusters\n- Pros: Real-time updates, cost-effective\n- Cons: Memory constraints, cache invalidation complexity\n\nApproach 3: Materialized views with lazy loading\n- Pre-compute feeds, load on demand\n- Pros: Balanced cost/performance\n- Cons: Computation overhead, partial staleness\n```\n\n### Step 2: Explore Each Branch\n\n```\nFor Approach 2 (In-memory cache), explore implementation details:\n\nBranch 2.1: Cluster sharding strategy\nBranch 2.2: Eviction policy\nBranch 2.3: Failover mechanism\n\nEvaluate each branch for:\n- Scalability\n- Reliability  \n- Cost\n```\n\n### Step 3: Evaluate and Prune\n\n```\nEvaluation Criteria:\n1. Meets 99.9% uptime SLA? (Yes/No)\n2. Scales to 10M users? (Yes/No)\n3. Monthly cost under $50K? (Yes/No)\n4. Implementation time under 2 months? (Yes/No)\n\nApproach 1 (CDN): No (cost > $50K) ‚ùå\nApproach 2 (Redis): Yes to all ‚úì\nApproach 3 (Materialized): No (time > 2 months) ‚ùå\n\nWinner: Approach 2\n```\n\n### Step 4: Refine Winner\n\n```\nNow that we've selected Approach 2 (Redis caching), refine the design:\n\n1. Sharding: Hash-based user ID sharding across 20 Redis nodes\n2. Eviction: LRU with 1-hour TTL\n3. Failover: Read replicas with automatic promotion\n4. Monitoring: Cache hit rate > 90% threshold\n\nFinal Architecture: [Detailed design]\n```\n\n## ToT Prompt Template\n\n```python\nTOT_TEMPLATE = \"\"\"\n# Problem\n{problem}\n\n# Step 1: Generate Approaches\nPropose {n} fundamentally different approaches to solve this.\nFor each approach, briefly describe:\n- Core strategy\n- Main advantages\n- Main drawbacks\n\n# Step 2: Detailed Exploration\nFor each approach, explore:\n{exploration_aspects}\n\n# Step 3: Evaluation\nEvaluate each approach against:\n{evaluation_criteria}\n\nScore each approach (1-10) on each criterion.\n\n# Step 4: Selection\nSelect the highest-scoring approach and explain why.\n\n# Step 5: Refinement\nProvide a detailed implementation plan for the selected approach.\n\"\"\"\n\ndef tree_of_thoughts(problem, n_approaches=3, criteria=[]):\n  prompt = TOT_TEMPLATE.format(\n    problem=problem,\n    n=n_approaches,\n    exploration_aspects=\"\\n\".join(f\"- {a}\" for a in aspects),\n    evaluation_criteria=\"\\n\".join(f\"- {c}\" for c in criteria)\n  )\n  return llm.generate(prompt)\n```\n\n## Use Cases Where ToT Excels\n\n### 1. Strategic Planning\n**When:** Multiple valid strategies exist, trade-offs unclear\n\n**Example:**\n```\nProblem: Enter Asian market with limited budget\n\nPath A: Partner with local distributor\nPath B: Direct online sales\nPath C: Franchise model\n\n[Explore, evaluate, select]\n```\n\n### 2. Creative Problem Solving\n**When:** No obvious solution, creativity needed\n\n**Example:**\n```\nProblem: Reduce customer churn by 30%\n\nPath A: Gamification loyalty program\nPath B: Predictive intervention\nPath C: Community building features\n\n[Explore, evaluate, select]\n```\n\n### 3. Technical Architecture\n**When:** System design with many architectural choices\n\n**Example:**\n```\nProblem: Real-time analytics dashboard for IoT sensors\n\nPath A: Stream processing (Kafka + Spark)\nPath B: Time-series DB (InfluxDB)\nPath C: Serverless (Lambda + DynamoDB)\n\n[Explore, evaluate, select]\n```\n\n### 4. Research Synthesis\n**When:** Multiple theories or perspectives need integration\n\n**Example:**\n```\nProblem: Explain declining birth rates in developed nations\n\nPath A: Economic factors (cost of living, wages)\nPath B: Cultural shifts (career focus, individualism)\nPath C: Policy impacts (parental leave, childcare)\n\n[Explore, evaluate, synthesize]\n```\n\n## Advanced ToT Techniques\n\n### Beam Search ToT\n\nKeep top-k branches at each level instead of exploring all.\n\n```python\ndef beam_search_tot(problem, beam_width=3, depth=3):\n  candidates = [(problem, 0)]  # (state, score)\n  \n  for level in range(depth):\n    next_candidates = []\n    \n    for state, score in candidates:\n      # Generate new branches\n      branches = generate_branches(state)\n      \n      # Evaluate each branch\n      for branch in branches:\n        branch_score = evaluate(branch)\n        next_candidates.append((branch, score + branch_score))\n    \n    # Keep only top-k\n    candidates = sorted(next_candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n  \n  return candidates[0]  # Return best path\n```\n\n### Self-Evaluation ToT\n\nModel evaluates its own branches without external criteria.\n\n```\nFor each approach, rate your own confidence (1-10) and explain:\n- What could go wrong?\n- What assumptions are you making?\n- How robust is this to edge cases?\n\nThis self-critique often reveals hidden flaws.\n```\n\n### Hybrid ToT + CoT\n\nCombine tree exploration with chain reasoning within each branch.\n\n```\nStep 1: ToT to select approach\nStep 2: CoT to implement selected approach step-by-step\nStep 3: ToT to handle encountered obstacles\n```\n\n## Comparing Reasoning Strategies\n\n**When to use each:**\n\n**Standard Prompting:**\n- Simple, well-defined tasks\n- Single correct answer\n- Speed priority\n\n**Chain-of-Thought:**\n- Multi-step reasoning\n- Math and logic\n- Verification needed\n\n**Tree of Thoughts:**\n- Multiple valid approaches\n- Trade-off analysis required\n- Strategic decisions\n- Creative problem-solving\n\n**Self-Consistency:**\n- High-stakes decisions\n- Noisy reasoning paths\n- Need confidence estimation\n\n## Performance Metrics\n\n**ToT vs CoT (GPT-4):**\n- Creative tasks: ToT +35% quality\n- Strategic planning: ToT +28% success\n- Game playing: ToT +41% win rate\n- Standard math: CoT faster, similar accuracy\n\n**Trade-offs:**\n- ToT: 3-5x more tokens/cost than CoT\n- ToT: 2-4x longer latency than CoT\n- ToT: Significantly better on open-ended problems",
        keyPoints: [
          "ToT explores multiple reasoning branches in parallel",
          "Systematic evaluation selects optimal solution path",
          "Excels at strategic planning and creative problem-solving",
          "Beam search limits exploration to top-k branches",
          "3-5x cost increase but 30-40% quality improvement on complex tasks"
        ],
        handsOnExercise: "Use ToT to design a solution for a complex system design problem. Generate 3 approaches, evaluate against 4 criteria, and refine the winner."
      },
      {
        id: "lesson-3-3",
        title: "Role-Playing and Persona Design",
        duration: "3 hours",
        content: "# Role-Playing and Persona Design\n\nGuide AI behavior by assigning specific roles and personas.\n\n## Creating Effective Personas\n\n### Include These Elements\n\n1. **Background/Expertise:** \"20 years experience in...\"\n2. **Communication Style:** Formal vs. casual, technical vs. accessible\n3. **Approach/Methodology:** \"Always start by...\"\n4. **Personality Traits:** Patient, analytical, creative\n\n## Example Personas\n\n**Teacher:** \"You are an experienced science teacher who excels at making complex topics accessible.\"\n\n**Consultant:** \"You are a senior business strategy consultant with 15 years of experience.\"\n\n**Creative Writer:** \"You are a creative storyteller inspired by authors like Neil Gaiman.\"",
        keyPoints: [
          "Personas guide AI behavior and style",
          "Include expertise, style, approach, and personality",
          "Professional roles need different personas than creative ones",
          "Context management maintains conversation continuity"
        ],
        handsOnExercise: "Create three different personas and ask each the same question. Compare response styles."
      },
      {
        id: "lesson-3-4",
        title: "Mastering Context and Creativity",
        duration: "4 hours",
        content: "# Context Management and Creativity Control\n\nLearn to manage long conversations and fine-tune model creativity for optimal outputs.\n\n## Advanced Context Management\n\n### The Context Window Challenge\n\n**Problem:** Models have finite memory (context windows)\n- GPT-4: 128K tokens (~96,000 words)\n- Claude 3.5: 200K tokens (~150,000 words)\n- Gemini 1.5 Pro: 1M tokens (~750,000 words)\n\n**Once exceeded:** Model \"forgets\" oldest information\n\n### Context Compression Strategies\n\n#### 1. Progressive Summarization\n\nAs conversation grows, periodically summarize and compress.\n\n```python\ndef manage_context(messages, max_tokens=100000):\n  current_tokens = count_tokens(messages)\n  \n  if current_tokens > max_tokens:\n    # Summarize older messages\n    old_messages = messages[:len(messages)//2]\n    summary = summarize(old_messages)\n    \n    # Keep summary + recent messages\n    messages = [\n      {\"role\": \"system\", \"content\": f\"Previous conversation summary: {summary}\"},\n      *messages[len(messages)//2:]\n    ]\n  \n  return messages\n```\n\n**Example:**\n```\nOriginal (10,000 tokens):\nUser: Tell me about renewable energy\nAssistant: [Long explanation about solar, wind, hydro...]\nUser: What about costs?\nAssistant: [Detailed cost breakdown...]\n...[20 more exchanges]\n\nCompressed (2,000 tokens):\nSummary: Discussion covered renewable energy types (solar, wind, hydro), cost comparisons, and implementation challenges. User interested in residential solar.\n\n[Keep last 5 exchanges]\n```\n\n#### 2. Sliding Window with Overlap\n\nProcess long documents in chunks with overlapping sections.\n\n```python\ndef sliding_window_analysis(document, window_size=3000, overlap=500):\n  chunks = []\n  start = 0\n  \n  while start < len(document):\n    end = min(start + window_size, len(document))\n    chunk = document[start:end]\n    \n    analysis = llm.analyze(chunk)\n    chunks.append(analysis)\n    \n    start += window_size - overlap  # Overlap prevents missing info\n  \n  # Synthesize chunk analyses\n  final = llm.synthesize(chunks)\n  return final\n```\n\n#### 3. Hierarchical Context\n\nOrganize context in layers of abstraction.\n\n```\nLevel 1 (Always included): Core system prompt, critical facts\nLevel 2 (Recent): Last 10 messages\nLevel 3 (Relevant): Retrieved based on current topic\nLevel 4 (Archive): Summarized historical context\n\nTotal tokens: L1(500) + L2(5000) + L3(3000) + L4(1500) = 10,000\n```\n\n#### 4. Semantic Retrieval\n\nFor very long conversations, retrieve only relevant context.\n\n```python\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef retrieve_relevant_context(query, message_history, top_k=5):\n  # Embed all messages\n  message_embeddings = [embed(msg) for msg in message_history]\n  query_embedding = embed(query)\n  \n  # Compute similarity\n  similarities = cosine_similarity([query_embedding], message_embeddings)[0]\n  \n  # Get top-k most relevant\n  top_indices = np.argsort(similarities)[-top_k:]\n  relevant_messages = [message_history[i] for i in top_indices]\n  \n  return relevant_messages\n```\n\n### Handling Ambiguity and Uncertainty\n\n#### Disambiguation Prompts\n\n**When input is ambiguous:**\n```\nUser: \"Book a table for 4\"\n\nAmbiguous context. Before proceeding, clarify:\n1. Which restaurant? (if multiple in context)\n2. What date and time?\n3. Any dietary restrictions or preferences?\n\nRespond with clarifying questions, then make reservation.\n```\n\n#### Confidence-Aware Responses\n\n```\nIf you're certain (>90% confidence):\n- State directly: \"The answer is X\"\n\nIf moderately certain (60-90%):\n- Qualify: \"Based on the information provided, it's likely X\"\n\nIf uncertain (<60%):\n- Admit: \"I don't have enough information to answer confidently. Here's what I know: [partial info]. To answer fully, I'd need: [requirements]\"\n```\n\n## Controlling Creativity with Temperature\n\n### Temperature Scale (0.0 - 2.0)\n\n**Temperature = 0.0:** Deterministic\n```\nPrompt: \"Complete: The capital of France is\"\nOutput: \"Paris\" (always)\nUse for: Facts, code, math\n```\n\n**Temperature = 0.3:** Mostly consistent\n```\nPrompt: \"Write a product description for a water bottle\"\nOutput: Professional, safe, consistent tone\nUse for: Business content, technical writing\n```\n\n**Temperature = 0.7:** Balanced (default)\n```\nPrompt: \"Write a short story opening\"\nOutput: Natural variation, coherent\nUse for: General content, conversation\n```\n\n**Temperature = 1.0:** Creative\n```\nPrompt: \"Brainstorm unusual product ideas\"\nOutput: Diverse, unexpected ideas\nUse for: Brainstorming, creative writing\n```\n\n**Temperature = 1.5+:** Highly creative (risky)\n```\nPrompt: \"Create a surreal poem\"\nOutput: Very unusual, sometimes incoherent\nUse for: Experimental art, idea generation\n```\n\n### Temperature by Task Type\n\n**Factual Tasks (T=0.0-0.2):**\n- Answering questions\n- Data extraction\n- Code generation\n- Mathematical problems\n\n**Professional Content (T=0.3-0.5):**\n- Business writing\n- Documentation\n- Reports and summaries\n- Email drafting\n\n**Creative Tasks (T=0.7-1.2):**\n- Story writing\n- Poetry\n- Brainstorming\n- Art descriptions\n\n**Experimental (T=1.3-2.0):**\n- Abstract art\n- Surreal content\n- Idea mutation\n- Style experimentation\n\n### Advanced Sampling Parameters\n\n#### Dynamic Temperature\n\nAdjust temperature based on context.\n\n```python\ndef adaptive_temperature(task_type, user_preference):\n  base_temp = {\n    'factual': 0.1,\n    'professional': 0.4,\n    'creative': 0.9,\n    'experimental': 1.3\n  }[task_type]\n  \n  # Adjust based on user feedback\n  if user_preference == 'more_creative':\n    return min(base_temp + 0.3, 2.0)\n  elif user_preference == 'more_focused':\n    return max(base_temp - 0.3, 0.0)\n  \n  return base_temp\n```\n\n#### Top-p (Nucleus Sampling)\n\nLimit token selection to cumulative probability p.\n\n**Top-p = 0.1:** Very focused (top 10% likely tokens)\n- Safest outputs\n- Predictable\n- Less creative\n\n**Top-p = 0.9:** Balanced (default)\n- Good variety\n- Filters unlikely tokens\n- Coherent outputs\n\n**Top-p = 1.0:** All tokens considered\n- Maximum diversity\n- Risk of incoherence\n\n#### Combining Temperature + Top-p\n\n**Conservative (Factual):**\n```python\nsettings = {\n  'temperature': 0.2,\n  'top_p': 0.1,\n  'frequency_penalty': 0.0\n}\n```\n\n**Balanced (General):**\n```python\nsettings = {\n  'temperature': 0.7,\n  'top_p': 0.9,\n  'frequency_penalty': 0.3\n}\n```\n\n**Creative (Exploration):**\n```python\nsettings = {\n  'temperature': 1.1,\n  'top_p': 0.95,\n  'frequency_penalty': 0.7,\n  'presence_penalty': 0.5\n}\n```\n\n### Preventing Repetition\n\n#### Frequency Penalty (0.0 - 2.0)\n\nReduces probability of tokens based on how often they've appeared.\n\n```python\n# Without frequency penalty (repetitive)\nOutput: \"The product is great. Great quality. Great price. Great service.\"\n\n# With frequency_penalty=0.8\nOutput: \"The product is excellent. High quality, competitive pricing, and outstanding customer service.\"\n```\n\n#### Presence Penalty (0.0 - 2.0)\n\nEncourages new topics/tokens regardless of frequency.\n\n```python\n# Without presence penalty (stays on topic)\nOutput: \"Solar panels generate electricity. They convert sunlight to power. They reduce energy bills.\"\n\n# With presence_penalty=0.8\nOutput: \"Solar panels generate electricity. Wind turbines offer an alternative. Combining both creates resilient systems.\"\n```\n\n### Practical Creativity Control\n\n**Recipe Generation:**\n```python\n# Low creativity: Traditional recipes\ngenerate_recipe(temp=0.4, top_p=0.7)\n‚Üí \"Classic Spaghetti Carbonara\"\n\n# High creativity: Fusion cuisine\ngenerate_recipe(temp=1.1, top_p=0.95, presence_penalty=0.8)\n‚Üí \"Korean-Italian Kimchi Carbonara Fusion\"\n```\n\n**Code Refactoring:**\n```python\n# Low creativity: Safe, standard patterns\nrefactor_code(temp=0.2, top_p=0.5)\n‚Üí Standard design patterns, well-known libraries\n\n# High creativity: Novel approaches\nrefactor_code(temp=0.8, top_p=0.9)\n‚Üí Creative optimizations, unusual patterns\n```\n\n## Context-Aware Creativity\n\n### Iterative Refinement\n\n```python\ndef creative_iteration(initial_idea, iterations=3):\n  idea = initial_idea\n  \n  for i in range(iterations):\n    # Increase creativity each iteration\n    temp = 0.7 + (i * 0.2)\n    \n    prompt = f\"Build upon this idea with more creativity: {idea}\"\n    idea = llm.generate(prompt, temperature=temp)\n  \n  return idea\n\n# Example\nfinal = creative_iteration(\"A productivity app\")\n‚Üí \"A productivity app\" \n‚Üí \"An AI-powered task manager\" \n‚Üí \"A gamified AI productivity coach with AR integration\"\n```\n\n### Constraint-Based Creativity\n\nMore constraints can increase quality creativity.\n\n```\nGenerate a story with these constraints:\n- Must be exactly 100 words\n- Include these words: quantum, library, midnight\n- Twist ending\n- Noir style\n\n[Temperature=1.0 for creativity within rigid structure]\n```",
        keyPoints: [
          "Progressive summarization manages long conversation context",
          "Sliding windows with overlap prevent information loss",
          "Temperature controls randomness: 0=deterministic, 2=chaotic",
          "Top-p limits token selection to probable candidates",
          "Frequency/presence penalties prevent repetition and encourage diversity"
        ],
        handsOnExercise: "Generate the same content at temperatures 0.2, 0.7, and 1.3. Compare coherence, creativity, and usefulness for different use cases."
      }
    ]
  },
  {
    id: "module-4",
    title: "Advanced Prompting Techniques",
    description: "Master advanced strategies including ReAct framework, tree of thoughts, and automatic optimization.",
    duration: "16-20 hours",
    lessons: [
      {
        id: "lesson-4-1",
        title: "Automatic Prompt Engineering and Optimization",
        duration: "4 hours",
        content: "# Automatic Prompt Engineering (APE)\n\nLet AI optimize prompts automatically instead of manual trial-and-error.\n\n## The Problem with Manual Optimization\n\n**Traditional Approach:**\n1. Write prompt\n2. Test on examples\n3. Manually tweak wording\n4. Repeat 10-50 times\n5. Hope for best results\n\n**Limitations:**\n- Time-consuming\n- Requires expertise\n- Local optima (miss better solutions)\n- Inconsistent results\n\n## Automatic Prompt Engineering\n\n### Core Idea\nUse LLMs to generate and optimize prompts for other LLMs.\n\n**Meta-Prompt for APE:**\n```\nI need a prompt that accomplishes this task:\n{task_description}\n\nGenerate 10 diverse prompt variations that could accomplish this task effectively. For each prompt:\n1. Use different phrasings and approaches\n2. Vary instruction style (direct, conversational, structured)\n3. Include different examples if applicable\n\nTask: Classify customer feedback as positive, negative, or neutral\n\nExamples:\n{example_inputs_outputs}\n```\n\n### APE Implementation\n\n```python\nclass AutoPromptEngineer:\n    def __init__(self, task, examples, eval_metric):\n        self.task = task\n        self.examples = examples\n        self.eval_metric = eval_metric\n    \n    def generate_candidates(self, n=10):\n        \"\"\"Generate n prompt candidates\"\"\"\n        meta_prompt = f\"\"\"\n        Create {n} different prompts for this task: {self.task}\n        \n        Examples of input/output:\n        {self.examples}\n        \n        Requirements:\n        - Clear instructions\n        - Appropriate format specification\n        - Diverse approaches\n        \"\"\"\n        \n        response = llm.generate(meta_prompt)\n        prompts = self.extract_prompts(response)\n        return prompts\n    \n    def evaluate_prompt(self, prompt, test_set):\n        \"\"\"Test prompt on evaluation dataset\"\"\"\n        correct = 0\n        for example in test_set:\n            result = llm.generate(f\"{prompt}\\n\\nInput: {example.input}\")\n            if self.eval_metric(result, example.expected_output):\n                correct += 1\n        return correct / len(test_set)\n    \n    def optimize(self, iterations=3):\n        \"\"\"Iteratively improve prompts\"\"\"\n        best_prompt = None\n        best_score = 0\n        \n        for i in range(iterations):\n            # Generate candidates\n            candidates = self.generate_candidates()\n            \n            # Evaluate each\n            for prompt in candidates:\n                score = self.evaluate_prompt(prompt, self.examples)\n                if score > best_score:\n                    best_score = score\n                    best_prompt = prompt\n            \n            # Use best prompt to generate improved versions\n            if i < iterations - 1:\n                self.task = f\"Improve this prompt: {best_prompt}\"\n        \n        return best_prompt, best_score\n```\n\n### Example: Sentiment Analysis Optimization\n\n**Iteration 1 Candidates:**\n```\nPrompt A: \"Classify the sentiment of this text as positive, negative, or neutral.\"\nScore: 72%\n\nPrompt B: \"Analyze the emotional tone and categorize as: positive (happy, satisfied), negative (angry, disappointed), or neutral (factual, balanced).\"\nScore: 81%\n\nPrompt C: \"Rate sentiment:\nText: {text}\nSentiment (Positive/Negative/Neutral):\"\nScore: 68%\n```\n\n**Winner: Prompt B (81%)**\n\n**Iteration 2: Refine Prompt B**\n```\nPrompt B1: \"Analyze emotional tone. Consider context, sarcasm, and implicit meaning. Categorize as: positive (happy, satisfied, optimistic), negative (angry, disappointed, frustrated), or neutral (factual, balanced, informational).\"\nScore: 87%\n```\n\n## Advanced APE Techniques\n\n### 1. Instruction Induction\n\nGiven input-output pairs, induce the instruction.\n\n```python\ndef induce_instruction(examples):\n    prompt = \"\"\"\n    Given these input-output pairs, what instruction would produce these results?\n    \n    Examples:\n    Input: \"2, 4, 6\" ‚Üí Output: \"8\"\n    Input: \"1, 3, 5\" ‚Üí Output: \"7\"\n    Input: \"10, 12, 14\" ‚Üí Output: \"16\"\n    \n    Induced instruction:\n    \"\"\"\n    \n    instruction = llm.generate(prompt)\n    return instruction\n    # Result: \"Find the next number in the arithmetic sequence\"\n```\n\n### 2. Gradient-Based Optimization\n\nUse optimization algorithms to improve prompts.\n\n```python\ndef gradient_descent_prompts(initial_prompt, learning_rate=0.1):\n    \"\"\"\n    Conceptual: Treat prompt words as parameters to optimize\n    \"\"\"\n    current_prompt = initial_prompt\n    \n    for iteration in range(50):\n        # Generate variations by substituting words\n        variations = generate_variations(current_prompt)\n        \n        # Evaluate each variation\n        scores = [evaluate(v) for v in variations]\n        \n        # Move toward better performing variations\n        best_idx = np.argmax(scores)\n        current_prompt = variations[best_idx]\n        \n        if scores[best_idx] > 0.95:  # Threshold\n            break\n    \n    return current_prompt\n```\n\n### 3. Prompt Compression\n\nReduce token count while maintaining performance.\n\n```python\ndef compress_prompt(verbose_prompt, target_reduction=0.5):\n    \"\"\"\n    Remove redundant words while preserving meaning\n    \"\"\"\n    compression_prompt = f\"\"\"\n    Compress this prompt to {int(len(verbose_prompt.split()) * target_reduction)} words while maintaining all critical information:\n    \n    Original:\n    {verbose_prompt}\n    \n    Compressed version:\n    \"\"\"\n    \n    compressed = llm.generate(compression_prompt)\n    \n    # Verify performance hasn't degraded\n    original_score = evaluate(verbose_prompt)\n    compressed_score = evaluate(compressed)\n    \n    if compressed_score >= original_score * 0.95:  # Allow 5% degradation\n        return compressed\n    else:\n        return verbose_prompt\n```\n\n**Example:**\n```\nOriginal (45 tokens):\n\"Please carefully analyze the following customer review text and determine whether the overall sentiment expressed is positive, negative, or neutral. Consider the context, tone, and specific words used.\"\n\nCompressed (18 tokens):\n\"Classify this review's sentiment (positive/negative/neutral) based on tone and context.\"\n\nPerformance: Original 84% ‚Üí Compressed 83% ‚úì\nTokens saved: 60%\n```\n\n## Prompt Ensembling\n\nCombine multiple prompts for better results.\n\n### Voting Ensemble\n\n```python\ndef ensemble_prompts(prompts, input_text):\n    \"\"\"\n    Run multiple prompts and vote on result\n    \"\"\"\n    results = []\n    \n    for prompt in prompts:\n        result = llm.generate(f\"{prompt}\\n{input_text}\")\n        results.append(result)\n    \n    # Majority vote\n    from collections import Counter\n    vote_counts = Counter(results)\n    final_result = vote_counts.most_common(1)[0][0]\n    confidence = vote_counts[final_result] / len(prompts)\n    \n    return final_result, confidence\n\n# Example\nprompts = [\n    \"Classify sentiment: {text}\",\n    \"Emotional tone analysis: {text}\\nSentiment:\",\n    \"Rate as positive/negative/neutral: {text}\"\n]\n\nresult, conf = ensemble_prompts(prompts, customer_review)\nprint(f\"Result: {result} (confidence: {conf})\")\n# Result: Positive (confidence: 0.67)\n```\n\n### Weighted Ensemble\n\n```python\ndef weighted_ensemble(prompts, weights, input_text):\n    \"\"\"\n    Weight prompts by historical performance\n    \"\"\"\n    results = {}\n    \n    for prompt, weight in zip(prompts, weights):\n        result = llm.generate(f\"{prompt}\\n{input_text}\")\n        results[result] = results.get(result, 0) + weight\n    \n    # Return highest weighted result\n    return max(results.items(), key=lambda x: x[1])[0]\n\n# Weights based on validation performance\nweights = [0.75, 0.89, 0.82]  # Prompt 2 performs best\nresult = weighted_ensemble(prompts, weights, input_text)\n```\n\n## Benchmarking and Evaluation\n\n### Creating Evaluation Datasets\n\n```python\ndef create_eval_dataset(domain, size=100):\n    \"\"\"\n    Generate diverse test cases\n    \"\"\"\n    dataset = []\n    \n    # Include edge cases\n    edge_cases = [\n        \"Sarcastic text: 'Oh great, another bug!'\",\n        \"Mixed sentiment: 'Good product, terrible service'\",\n        \"Neutral: 'The item shipped on Tuesday'\"\n    ]\n    \n    # Generate varied examples\n    for i in range(size - len(edge_cases)):\n        example = generate_example(domain)\n        dataset.append(example)\n    \n    dataset.extend(edge_cases)\n    return dataset\n```\n\n### A/B Testing Framework\n\n```python\nclass PromptABTest:\n    def __init__(self, prompt_a, prompt_b, test_set):\n        self.prompt_a = prompt_a\n        self.prompt_b = prompt_b\n        self.test_set = test_set\n    \n    def run_test(self):\n        results_a = [self.run_prompt(self.prompt_a, ex) for ex in self.test_set]\n        results_b = [self.run_prompt(self.prompt_b, ex) for ex in self.test_set]\n        \n        score_a = self.calculate_metrics(results_a)\n        score_b = self.calculate_metrics(results_b)\n        \n        # Statistical significance test\n        from scipy import stats\n        t_stat, p_value = stats.ttest_ind(score_a, score_b)\n        \n        winner = 'A' if np.mean(score_a) > np.mean(score_b) else 'B'\n        significant = p_value < 0.05\n        \n        return {\n            'winner': winner,\n            'significant': significant,\n            'p_value': p_value,\n            'score_a': np.mean(score_a),\n            'score_b': np.mean(score_b)\n        }\n```\n\n## Production Best Practices\n\n**1. Version Control Prompts**\n```python\nPROMPT_REGISTRY = {\n    'sentiment_v1': {\"prompt\": \"...\", \"accuracy\": 0.72, \"date\": \"2025-01-01\"},\n    'sentiment_v2': {\"prompt\": \"...\", \"accuracy\": 0.84, \"date\": \"2025-01-15\"},\n    'sentiment_v3': {\"prompt\": \"...\", \"accuracy\": 0.87, \"date\": \"2025-02-01\"}\n}\n```\n\n**2. Gradual Rollout**\n```python\ndef gradual_rollout(new_prompt, old_prompt, traffic_percentage=10):\n    import random\n    \n    if random.random() < traffic_percentage / 100:\n        return new_prompt\n    else:\n        return old_prompt\n```\n\n**3. Performance Monitoring**\n```python\ndef monitor_prompt_performance(prompt_id):\n    metrics = {\n        'accuracy': calculate_accuracy(),\n        'latency': measure_latency(),\n        'cost': calculate_cost(),\n        'user_satisfaction': get_feedback_score()\n    }\n    \n    log_metrics(prompt_id, metrics)\n    \n    if metrics['accuracy'] < 0.8:  # Threshold\n        alert_team(f\"Prompt {prompt_id} accuracy degraded\")\n        rollback_to_previous_version()\n```",
        keyPoints: [
          "APE uses AI to generate and optimize prompts automatically",
          "Instruction induction derives prompts from input-output pairs",
          "Prompt ensembling combines multiple prompts via voting",
          "A/B testing validates improvements with statistical significance",
          "Production systems need versioning, gradual rollout, and monitoring"
        ],
        handsOnExercise: "Implement APE to optimize a classification task. Generate 5 prompt candidates, evaluate each, and select the best performer."
      },
      {
        id: "lesson-4-2",
        title: "Meta-Prompting and Self-Correction",
        duration: "4 hours",
        content: "# Meta-Prompting and Self-Correction\n\nTeach models to improve their own prompts and outputs through reflection and iteration.\n\n## Meta-Prompting Fundamentals\n\n### What is Meta-Prompting?\n\nUsing prompts about prompts - asking the model to generate, analyze, or improve prompts.\n\n**Example:**\n```\nUser: I need help writing better prompts for code review.\n\nModel (Meta-Prompt):\n\"Before I help, let me understand your needs:\n1. What programming languages?\n2. What aspects to review? (security, performance, style)\n3. What's your team's experience level?\n4. Preferred output format?\n\nBased on your answers, I'll craft an optimized prompt.\"\n```\n\n### Self-Generated Prompts\n\n```\nTask: Summarize research papers\n\nMeta-Prompt:\n\"First, generate a prompt template that would be ideal for summarizing research papers. Consider:\n- Key sections to extract (abstract, methods, findings, limitations)\n- Target audience (researchers vs. general public)\n- Length constraints\n- Citation requirements\n\nThen use your generated prompt on this paper: [paper content]\"\n```\n\n**Model Output:**\n```\nGenerated Prompt:\n\"Extract from this research paper:\n1. Core research question\n2. Methodology (2-3 sentences)\n3. Key findings (bullet points)\n4. Limitations\n5. Practical implications\n\nTarget: Academic audience\nLength: 200-250 words\nCitations: Include year and first author\"\n\n[Then applies this prompt to the paper]\n```\n\n## Self-Correction Techniques\n\n### Basic Self-Critique\n\n```\nStep 1: Generate initial answer\nQuestion: What's the capital of Australia?\nInitial Answer: Sydney\n\nStep 2: Self-critique\n\"Wait, let me verify this answer:\n- Sydney is Australia's largest city\n- But the capital is where the government is located\n- The capital is actually Canberra\n\nCorrected Answer: Canberra\"\n```\n\n### Structured Self-Reflection\n\n```python\nSELF_REFLECTION_TEMPLATE = \"\"\"\nTask: {task}\n\nStep 1: Initial Response\n{initial_response}\n\nStep 2: Self-Evaluation\nRate your response (1-10) on:\n- Accuracy: {score}/10\n- Completeness: {score}/10\n- Clarity: {score}/10\n\nStep 3: Identify Issues\nWhat could be wrong or missing?\n1. {issue_1}\n2. {issue_2}\n\nStep 4: Improved Response\n{improved_response}\n\nStep 5: Verify Improvement\nHow is the new response better?\n{explanation}\n\"\"\"\n```\n\n**Example Application:**\n```\nTask: Explain quantum entanglement\n\nInitial Response: \"Quantum entanglement is when particles are connected.\"\n\nSelf-Evaluation:\n- Accuracy: 5/10 (too vague)\n- Completeness: 3/10 (missing key concepts)\n- Clarity: 6/10 (simple but imprecise)\n\nIdentified Issues:\n1. Doesn't explain \"connected\" mechanism\n2. Missing non-locality concept\n3. No practical examples\n\nImproved Response: \"Quantum entanglement occurs when particles become correlated such that measuring one instantly affects the other, regardless of distance. This 'spooky action at a distance' means if you measure one particle's spin as 'up', its entangled partner will be 'down', even light-years away. Used in quantum computing and cryptography.\"\n\nVerification: Added mechanism (correlated states), non-locality (distance independent), and applications.\n```\n\n### Iterative Refinement Loop\n\n```python\ndef iterative_self_improvement(task, max_iterations=3):\n    response = llm.generate(task)\n    \n    for i in range(max_iterations):\n        critique_prompt = f\"\"\"\n        Review this response to: {task}\n        \n        Response: {response}\n        \n        Critique:\n        1. What's good?\n        2. What's missing or wrong?\n        3. How to improve?\n        \n        Provide improved version.\n        \"\"\"\n        \n        critique_and_improvement = llm.generate(critique_prompt)\n        new_response = extract_improved_version(critique_and_improvement)\n        \n        # Check if improvement occurred\n        if quality_score(new_response) > quality_score(response):\n            response = new_response\n        else:\n            break  # No more improvements\n    \n    return response\n```\n\n## Advanced Self-Correction Patterns\n\n### Socratic Self-Questioning\n\nModel questions its own assumptions.\n\n```\nClaim: \"Remote work always increases productivity.\"\n\nSocratic Questions:\n1. Is this true for ALL workers? (No - some need structure)\n2. What evidence supports this? (Some studies show mixed results)\n3. What factors might change this? (Job type, home environment, management)\n4. Am I overgeneralizing? (Yes - \"always\" is too strong)\n\nRefined Claim: \"Remote work can increase productivity for knowledge workers in roles with clear deliverables, when supported by proper tools and management practices.\"\n```\n\n### Multi-Perspective Critique\n\n```\nProposal: \"Implement 4-day work week\"\n\nPerspective 1 (Employee): \n- Pro: Better work-life balance\n- Con: Compressed schedules might be stressful\n\nPerspective 2 (Manager):\n- Pro: May improve retention\n- Con: Coordination challenges\n\nPerspective 3 (Customer):\n- Pro: Happier employees = better service\n- Con: Reduced availability\n\nPerspective 4 (Financial):\n- Pro: Lower overhead (1 less day)\n- Con: May need more staff\n\nSynthesis: \"4-day week viable IF: 1) Staggered schedules maintain coverage, 2) Clear productivity metrics, 3) 3-month pilot program to measure impact.\"\n```\n\n### Chain-of-Verification (CoVe)\n\n```\nQuestion: \"Name 5 countries in South America\"\n\nInitial Answer:\n1. Brazil\n2. Argentina  \n3. Mexico\n4. Colombia\n5. Chile\n\nVerification Questions:\n1. Is Mexico in South America? (Check geography)\n2. Did I list only South American countries? (No - Mexico is North/Central America)\n\nVerification Results:\n- Mexico is NOT in South America ‚úó\n\nCorrected Answer:\n1. Brazil\n2. Argentina\n3. Peru (replaces Mexico)\n4. Colombia\n5. Chile\n```\n\n### Implementation:\n\n```python\ndef chain_of_verification(question, initial_answer):\n    # Generate verification questions\n    verification_prompt = f\"\"\"\n    Given this Q&A, generate verification questions:\n    \n    Q: {question}\n    A: {initial_answer}\n    \n    What questions would verify each fact?\n    \"\"\"\n    \n    verification_qs = llm.generate(verification_prompt)\n    \n    # Answer verification questions\n    verifications = []\n    for vq in verification_qs:\n        answer = llm.generate(vq)\n        verifications.append(answer)\n    \n    # Generate final corrected answer\n    correction_prompt = f\"\"\"\n    Original: {initial_answer}\n    Verification results: {verifications}\n    \n    Provide corrected answer based on verification.\n    \"\"\"\n    \n    return llm.generate(correction_prompt)\n```\n\n## Confidence Calibration\n\n### Confidence-Aware Responses\n\n```python\ndef confidence_aware_generation(question):\n    response_prompt = f\"\"\"\n    {question}\n    \n    Provide:\n    1. Your answer\n    2. Confidence level (0-100%)\n    3. What would increase confidence\n    4. Alternative possibilities if uncertain\n    \"\"\"\n    \n    response = llm.generate(response_prompt)\n    return parse_confidence_response(response)\n\n# Example output:\n{\n    \"answer\": \"The meeting is likely on Tuesday\",\n    \"confidence\": 70,\n    \"confidence_factors\": \"Based on usual schedule pattern, but no explicit confirmation\",\n    \"alternatives\": [\"Could be Wednesday if schedule shifted\", \"Check calendar for confirmation\"]\n}\n```\n\n### Uncertainty Quantification\n\n```\nQuestion: \"Will this startup succeed?\"\n\nResponse with Uncertainty:\n\"I cannot predict success with certainty. Based on available information:\n\nStrong indicators (70% confidence):\n- Experienced team\n- Clear market need\n- Early traction\n\nRisk factors (concerns):\n- Competitive market\n- Limited runway\n- Unproven business model\n\nConclusion: Moderate success probability (50-60%) contingent on:\n1. Raising Series A within 6 months\n2. Achieving product-market fit\n3. Maintaining team cohesion\n\nWhat I don't know (would improve confidence):\n- Detailed financials\n- Customer retention data\n- Competitive response strategies\"\n```\n\n## Practical Applications\n\n### Code Review Self-Correction\n\n```python\ndef self_correcting_code_review(code):\n    # Initial review\n    review_1 = llm.generate(f\"Review this code:\\n{code}\")\n    \n    # Self-critique of review\n    meta_review = llm.generate(f\"\"\"\n    You provided this code review:\n    {review_1}\n    \n    Critique your own review:\n    1. Did you check security issues?\n    2. Did you consider edge cases?\n    3. Did you suggest improvements, not just find problems?\n    4. Were recommendations specific and actionable?\n    \n    Provide enhanced review addressing any gaps.\n    \"\"\")\n    \n    return meta_review\n```\n\n### Factual Claim Verification\n\n```\nClaim: \"Python is faster than Java\"\n\nSelf-Verification:\n1. Is this always true? (No - context dependent)\n2. What metrics? (Execution speed, development speed, etc.)\n3. What version of each language? (Matters significantly)\n\nCorrected Claim: \"Python often enables faster development time than Java due to simpler syntax and extensive libraries, but Java typically executes faster for compute-intensive tasks. Performance depends on specific use case, optimization, and implementation quality.\"\n```\n\n### Research Synthesis\n\n```\nTask: Summarize AI safety research\n\nInitial Summary: [Generated summary]\n\nSelf-Critique Questions:\n1. Did I cover all major schools of thought?\n2. Is this balanced or biased toward one perspective?\n3. Are my sources current (within 2 years)?\n4. Did I explain technical terms for general audience?\n5. Are conclusions supported by cited evidence?\n\nRevised Summary: [Improved based on self-critique]\n```\n\n## Meta-Learning: Learning to Learn\n\n### Prompt Pattern Recognition\n\n```\nAnalyze these successful prompts:\n\nPrompt 1: \"Act as X, analyze Y, provide Z format\"\nPrompt 2: \"You are X expert. Given Y, output Z structure\"  \nPrompt 3: \"Role: X. Task: Analyze Y. Format: Z\"\n\nPattern Recognition:\n- All define a role/persona (X)\n- All specify input to analyze (Y)\n- All request structured output (Z)\n\nGeneral Template:\n\"Adopt role {R}, process input {I}, deliver output {O} in format {F}\"\n```\n\n### Adaptive Prompting\n\n```python\ndef adaptive_prompt(task, previous_attempts):\n    \"\"\"\n    Learn from failures to improve prompts\n    \"\"\"\n    if not previous_attempts:\n        return basic_prompt(task)\n    \n    analysis_prompt = f\"\"\"\n    These prompts failed:\n    {previous_attempts}\n    \n    Analyze why they failed and generate improved version.\n    \n    Consider:\n    - Too vague?\n    - Missing examples?\n    - Wrong format?\n    - Ambiguous instructions?\n    \"\"\"\n    \n    improved_prompt = llm.generate(analysis_prompt)\n    return improved_prompt\n```",
        keyPoints: [
          "Meta-prompting asks models to generate or improve prompts",
          "Self-correction uses iterative critique and refinement",
          "Chain-of-verification validates facts with targeted questions",
          "Confidence calibration quantifies uncertainty in responses",
          "Socratic questioning challenges assumptions systematically"
        ],
        handsOnExercise: "Implement a self-correction loop: generate answer, critique it, improve it. Repeat 3 times and compare initial vs. final quality."
      },
      {
        id: "lesson-4-3",
        title: "Building Autonomous AI Agents with ReAct",
        duration: "5 hours",
        content: "# Building AI Agents with ReAct\n\nCreate autonomous agents that reason, act, and learn from their environment.\n\n## ReAct Framework Deep Dive\n\n### The Thought-Action-Observation Loop\n\n**Thought:** Internal reasoning about what to do next\n**Action:** Execute a tool or operation\n**Observation:** Receive and interpret results\n\n**Example: Research Agent**\n```\nGoal: Find the current CEO of Apple and their previous role\n\nThought 1: I need to find current Apple CEO\nAction 1: Search[\"Apple CEO 2025\"]\nObservation 1: Tim Cook is the CEO of Apple\n\nThought 2: Now I need Tim Cook's previous role before CEO\nAction 2: Search[\"Tim Cook previous role before CEO\"]\nObservation 2: He was Apple's COO (Chief Operating Officer)\n\nThought 3: I have both pieces of information\nAction 3: Finish[\"Tim Cook is Apple's CEO. He was previously COO.\"]\n```\n\n### Agent Architecture\n\n```python\nclass ReActAgent:\n    def __init__(self, tools, max_iterations=10):\n        self.tools = tools  # Dict of available functions\n        self.max_iterations = max_iterations\n        self.history = []\n    \n    def run(self, goal):\n        context = f\"Goal: {goal}\\n\\n\"\n        \n        for i in range(self.max_iterations):\n            # Generate thought and action\n            prompt = f\"\"\"\n            {context}\n            Available tools: {list(self.tools.keys())}\n            \n            What do you think and what action should you take?\n            \n            Thought: [your reasoning]\n            Action: [ToolName[input]] or Finish[final answer]\n            \"\"\"\n            \n            response = llm.generate(prompt)\n            thought, action = self.parse_response(response)\n            \n            # Execute action\n            if action.startswith(\"Finish\"):\n                return self.extract_answer(action)\n            \n            tool_name, tool_input = self.parse_action(action)\n            observation = self.tools[tool_name](tool_input)\n            \n            # Update context\n            context += f\"Thought {i+1}: {thought}\\n\"\n            context += f\"Action {i+1}: {action}\\n\"\n            context += f\"Observation {i+1}: {observation}\\n\\n\"\n            \n            self.history.append({\n                'thought': thought,\n                'action': action,\n                'observation': observation\n            })\n        \n        return \"Max iterations reached without conclusion\"\n    \n    def parse_response(self, response):\n        # Extract thought and action from model response\n        thought = extract_between(response, \"Thought:\", \"Action:\")\n        action = extract_after(response, \"Action:\")\n        return thought.strip(), action.strip()\n    \n    def parse_action(self, action_str):\n        # Parse \"ToolName[input]\" format\n        tool_name = action_str.split('[')[0]\n        tool_input = action_str.split('[')[1].rstrip(']')\n        return tool_name, tool_input\n```\n\n## Building Agent Tools\n\n### Search Tool\n\n```python\ndef web_search(query):\n    \"\"\"\n    Search the web and return relevant snippets\n    \"\"\"\n    # Using a search API\n    results = search_api.search(query, num_results=3)\n    \n    snippets = []\n    for result in results:\n        snippets.append(f\"{result.title}: {result.snippet}\")\n    \n    return \"\\n\".join(snippets)\n\n# Register tool\ntools = {\"Search\": web_search}\n```\n\n### Calculator Tool\n\n```python\ndef calculate(expression):\n    \"\"\"\n    Safely evaluate mathematical expressions\n    \"\"\"\n    try:\n        # Safe eval with limited scope\n        allowed_names = {\"abs\": abs, \"round\": round, \"pow\": pow}\n        result = eval(expression, {\"__builtins__\": {}}, allowed_names)\n        return f\"Result: {result}\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\ntools[\"Calculate\"] = calculate\n```\n\n### Database Query Tool\n\n```python\ndef query_database(sql):\n    \"\"\"\n    Execute SQL query and return results\n    \"\"\"\n    # Sanitize and execute query\n    if is_safe_query(sql):\n        results = db.execute(sql)\n        return format_results(results)\n    else:\n        return \"Unsafe query rejected\"\n\ntools[\"QueryDB\"] = query_database\n```\n\n### API Call Tool\n\n```python\ndef api_call(endpoint, method=\"GET\", data=None):\n    \"\"\"\n    Make HTTP API requests\n    \"\"\"\n    import requests\n    \n    try:\n        if method == \"GET\":\n            response = requests.get(endpoint)\n        elif method == \"POST\":\n            response = requests.post(endpoint, json=data)\n        \n        return response.json()\n    except Exception as e:\n        return f\"API Error: {str(e)}\"\n\ntools[\"APICall\"] = api_call\n```\n\n## Advanced Agent Patterns\n\n### Multi-Step Research Agent\n\n```python\nresearch_agent = ReActAgent({\n    \"Search\": web_search,\n    \"Summarize\": summarize_text,\n    \"Extract\": extract_entities,\n    \"Verify\": fact_check\n})\n\ngoal = \"\"\"\nResearch the impact of AI on job markets:\n1. Find recent studies\n2. Extract key statistics  \n3. Summarize findings\n4. Verify claims with sources\n\"\"\"\n\nresult = research_agent.run(goal)\n```\n\n**Execution Trace:**\n```\nThought 1: Need recent studies on AI and jobs\nAction 1: Search[\"AI impact on job market 2024 2025 studies\"]\nObservation 1: [Search results with papers]\n\nThought 2: Extract key stats from these studies\nAction 2: Extract[entities=statistics, source=\"search results\"]\nObservation 2: \"30% automation risk, 12% jobs created, ...\" \n\nThought 3: Summarize findings\nAction 3: Summarize[\"Key points: automation risk, job creation, ...\"]\nObservation 3: [Summary text]\n\nThought 4: Verify main claims\nAction 4: Verify[\"30% automation risk statistic\"]\nObservation 4: \"Confirmed by MIT study 2024\"\n\nThought 5: Have complete, verified research\nAction 5: Finish[final_report]\n```\n\n### Data Analysis Agent\n\n```python\nanalysis_agent = ReActAgent({\n    \"QueryDB\": query_database,\n    \"Calculate\": calculate,\n    \"Visualize\": create_chart,\n    \"Interpret\": interpret_results\n})\n\ngoal = \"Analyze sales trends and identify top products\"\n\nresult = analysis_agent.run(goal)\n```\n\n### Customer Support Agent\n\n```python\nsupport_agent = ReActAgent({\n    \"SearchKB\": search_knowledge_base,\n    \"CheckStatus\": check_order_status,\n    \"CreateTicket\": create_support_ticket,\n    \"SendEmail\": send_notification\n})\n\ngoal = \"Help customer with order #12345 shipping delay\"\n\nresult = support_agent.run(goal)\n```\n\n## Error Handling and Recovery\n\n### Tool Failure Handling\n\n```python\nclass RobustReActAgent(ReActAgent):\n    def run(self, goal):\n        context = f\"Goal: {goal}\\n\\n\"\n        \n        for i in range(self.max_iterations):\n            # ... standard loop ...\n            \n            try:\n                observation = self.tools[tool_name](tool_input)\n            except Exception as e:\n                observation = f\"Tool error: {str(e)}\"\n                \n                # Ask agent to recover\n                recovery_prompt = f\"\"\"\n                Your action failed: {e}\n                \n                Think about:\n                1. Can you try a different tool?\n                2. Can you modify the input?\n                3. Can you break down the task differently?\n                \n                Next thought and action:\n                \"\"\"\n                \n                response = llm.generate(recovery_prompt)\n                thought, action = self.parse_response(response)\n                continue\n            \n            # ... rest of loop ...\n```\n\n### Infinite Loop Prevention\n\n```python\ndef detect_loop(history, window=3):\n    \"\"\"\n    Detect if agent is stuck in a loop\n    \"\"\"\n    if len(history) < window * 2:\n        return False\n    \n    recent = history[-window:]\n    previous = history[-window*2:-window]\n    \n    # Check if recent actions repeat previous ones\n    recent_actions = [h['action'] for h in recent]\n    previous_actions = [h['action'] for h in previous]\n    \n    if recent_actions == previous_actions:\n        return True  # Loop detected\n    \n    return False\n\n# In agent loop:\nif detect_loop(self.history):\n    prompt += \"\\n\\nWARNING: You seem stuck. Try a completely different approach.\"\n```\n\n## Advanced ReAct Techniques\n\n### Hierarchical Planning\n\nBreak complex goals into sub-goals.\n\n```python\ndef hierarchical_react(goal):\n    # First, plan high-level steps\n    plan_prompt = f\"\"\"\n    Goal: {goal}\n    \n    Break this into 3-5 high-level steps.\n    \n    Steps:\n    1.\n    2.\n    ...\n    \"\"\"\n    \n    plan = llm.generate(plan_prompt)\n    steps = parse_steps(plan)\n    \n    results = []\n    for step in steps:\n        # Execute each step with ReAct\n        agent = ReActAgent(tools)\n        result = agent.run(step)\n        results.append(result)\n    \n    # Synthesize results\n    return synthesize(results)\n```\n\n### Reflection and Learning\n\n```python\nclass LearningReActAgent(ReActAgent):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.successful_strategies = []\n    \n    def run(self, goal):\n        result = super().run(goal)\n        \n        # Reflect on performance\n        reflection_prompt = f\"\"\"\n        You just completed: {goal}\n        \n        Your history:\n        {self.history}\n        \n        Reflection:\n        1. What worked well?\n        2. What was inefficient?\n        3. What would you do differently?\n        \n        Key lesson:\n        \"\"\"\n        \n        lesson = llm.generate(reflection_prompt)\n        self.successful_strategies.append({\n            'goal_type': classify_goal(goal),\n            'lesson': lesson\n        })\n        \n        return result\n    \n    def run_with_memory(self, goal):\n        # Use past lessons\n        relevant_lessons = self.find_relevant_lessons(goal)\n        \n        enhanced_goal = f\"\"\"\n        Goal: {goal}\n        \n        Lessons from similar past tasks:\n        {relevant_lessons}\n        \n        Apply these lessons while working.\n        \"\"\"\n        \n        return self.run(enhanced_goal)\n```\n\n## Production Deployment\n\n### Safety Constraints\n\n```python\nclass SafeReActAgent(ReActAgent):\n    def __init__(self, tools, max_iterations=10, budget=100):\n        super().__init__(tools, max_iterations)\n        self.budget = budget  # Maximum tool calls\n        self.calls_made = 0\n        self.unsafe_patterns = [\n            r\"delete.*database\",\n            r\"drop.*table\",\n            r\"rm -rf\"\n        ]\n    \n    def is_safe_action(self, action):\n        import re\n        for pattern in self.unsafe_patterns:\n            if re.search(pattern, action, re.IGNORECASE):\n                return False\n        return True\n    \n    def run(self, goal):\n        # ... in the loop ...\n        \n        if not self.is_safe_action(action):\n            observation = \"Unsafe action blocked\"\n            continue\n        \n        if self.calls_made >= self.budget:\n            return \"Budget exceeded\"\n        \n        self.calls_made += 1\n        # ... execute action ...\n```\n\n### Monitoring and Logging\n\n```python\nimport logging\n\nclass MonitoredReActAgent(ReActAgent):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.logger = logging.getLogger(__name__)\n    \n    def run(self, goal):\n        self.logger.info(f\"Agent started: {goal}\")\n        \n        try:\n            result = super().run(goal)\n            self.logger.info(f\"Agent succeeded: {result}\")\n            \n            # Log metrics\n            metrics = {\n                'iterations': len(self.history),\n                'tools_used': set(h['action'].split('[')[0] for h in self.history),\n                'success': True\n            }\n            log_metrics(metrics)\n            \n            return result\n        \n        except Exception as e:\n            self.logger.error(f\"Agent failed: {e}\")\n            raise\n```",
        keyPoints: [
          "ReAct agents use Thought-Action-Observation loops for autonomy",
          "Tools extend agent capabilities (search, calculate, API calls)",
          "Error handling and loop detection ensure robustness",
          "Hierarchical planning breaks complex goals into sub-tasks",
          "Safety constraints and monitoring are critical for production"
        ],
        handsOnExercise: "Build a ReAct agent with 3 tools: web search, calculator, and summarizer. Have it research a topic and create a summary with verified statistics."
      },
      {
        id: "lesson-4-4",
        title: "Multi-Agent Systems and Coordination",
        duration: "4 hours",
        content: "# Multi-Agent Systems\n\nCoordinate multiple AI agents to solve complex problems through collaboration.\n\n## Why Multi-Agent Systems?\n\n### Limitations of Single Agents\n- Cognitive overload on complex tasks\n- Jack-of-all-trades, master of none\n- Single point of failure\n- Limited perspectives\n\n### Benefits of Multiple Agents\n- **Specialization:** Each agent excels at specific tasks\n- **Parallel Processing:** Work on subtasks simultaneously\n- **Robustness:** Failure of one agent doesn't halt system\n- **Diverse Perspectives:** Different approaches to problems\n\n## Multi-Agent Architectures\n\n### 1. Pipeline Architecture\n\nAgents work sequentially, each adding value.\n\n```python\nclass PipelineMultiAgent:\n    def __init__(self, agents):\n        self.agents = agents  # Ordered list of agents\n    \n    def run(self, input_data):\n        result = input_data\n        \n        for agent in self.agents:\n            result = agent.process(result)\n        \n        return result\n\n# Example: Content Creation Pipeline\npipeline = PipelineMultiAgent([\n    ResearchAgent(),      # Gathers information\n    OutlineAgent(),       # Creates structure\n    WriterAgent(),        # Writes content\n    EditorAgent(),        # Polishes output\n    SEOAgent()            # Optimizes for search\n])\n\narticle = pipeline.run(\"Topic: Quantum Computing Applications\")\n```\n\n**Execution Flow:**\n```\nInput: \"Quantum Computing Applications\"\n  ‚Üì\nResearchAgent ‚Üí [Research data]\n  ‚Üì\nOutlineAgent ‚Üí [Structured outline]\n  ‚Üì\nWriterAgent ‚Üí [Draft article]\n  ‚Üì\nEditorAgent ‚Üí [Polished article]\n  ‚Üì\nSEOAgent ‚Üí [SEO-optimized article]\n  ‚Üì\nOutput: Final article\n```\n\n### 2. Hub-and-Spoke Architecture\n\nCentral coordinator distributes tasks to specialized agents.\n\n```python\nclass HubSpokeMultiAgent:\n    def __init__(self, coordinator, specialists):\n        self.coordinator = coordinator\n        self.specialists = specialists  # Dict of specialist agents\n    \n    def run(self, task):\n        # Coordinator analyzes task\n        subtasks = self.coordinator.decompose(task)\n        \n        # Distribute to specialists\n        results = {}\n        for subtask in subtasks:\n            agent_type = subtask['type']\n            specialist = self.specialists[agent_type]\n            results[subtask['id']] = specialist.execute(subtask)\n        \n        # Coordinator synthesizes results\n        final_result = self.coordinator.synthesize(results)\n        return final_result\n\n# Example: Business Analysis System\nsystem = HubSpokeMultiAgent(\n    coordinator=CoordinatorAgent(),\n    specialists={\n        'financial': FinancialAnalyst(),\n        'market': MarketAnalyst(),\n        'competitor': CompetitorAnalyst(),\n        'technical': TechnicalAnalyst()\n    }\n)\n\nanalysis = system.run(\"Analyze acquisition target: Company X\")\n```\n\n**Execution Flow:**\n```\n                 Coordinator\n                      |\n        +-------------+-------------+\n        |             |             |\n   Financial      Market      Competitor\n   Analyst       Analyst       Analyst\n        |             |             |\n        +-------------+-------------+\n                      |\n                 Synthesize\n                   Results\n```\n\n### 3. Autonomous Collaboration\n\nAgents communicate peer-to-peer without central control.\n\n```python\nclass AutonomousMultiAgent:\n    def __init__(self, agents):\n        self.agents = agents\n        self.message_queue = []\n    \n    def run(self, goal):\n        # Broadcast goal to all agents\n        for agent in self.agents:\n            agent.set_goal(goal)\n        \n        # Agents work autonomously and communicate\n        max_rounds = 10\n        for round in range(max_rounds):\n            for agent in self.agents:\n                # Agent decides what to do\n                action = agent.decide_action(self.message_queue)\n                \n                if action['type'] == 'task':\n                    result = agent.execute(action)\n                    self.broadcast_message(agent.id, result)\n                \n                elif action['type'] == 'query':\n                    self.broadcast_message(agent.id, action['query'])\n                \n                elif action['type'] == 'answer':\n                    self.broadcast_message(agent.id, action['answer'])\n                \n                elif action['type'] == 'complete':\n                    return action['result']\n        \n        return \"Collaboration incomplete\"\n    \n    def broadcast_message(self, sender_id, message):\n        self.message_queue.append({\n            'sender': sender_id,\n            'message': message,\n            'timestamp': time.time()\n        })\n```\n\n**Example Conversation:**\n```\nGoal: \"Design a mobile app for recipe sharing\"\n\nPM Agent: \"Let's define requirements. UX Agent, what user flows do we need?\"\nUX Agent: \"Core flows: browse recipes, create recipe, share with friends.\"\nPM Agent: \"Backend Agent, what's the data model?\"\nBackend Agent: \"Users, Recipes (with ingredients, steps), Comments, Likes.\"\nUX Agent: \"I'll create wireframes based on these flows.\"\nBackend Agent: \"I'll design API endpoints for these entities.\"\nFrontend Agent: \"I'll implement the UI using the wireframes and API.\"\n... [collaboration continues] ...\nPM Agent: \"All components ready. Here's the complete design doc.\"\n```\n\n### 4. Debate Architecture\n\nAgents argue different positions to reach best solution.\n\n```python\nclass DebateMultiAgent:\n    def __init__(self, debaters, judge):\n        self.debaters = debaters\n        self.judge = judge\n    \n    def run(self, question):\n        positions = []\n        \n        # Each debater takes a position\n        for debater in self.debaters:\n            position = debater.argue(question)\n            positions.append(position)\n        \n        # Debate rounds\n        for round in range(3):\n            rebuttals = []\n            for i, debater in enumerate(self.debaters):\n                # Debater responds to others\n                others = [p for j, p in enumerate(positions) if j != i]\n                rebuttal = debater.rebut(others)\n                rebuttals.append(rebuttal)\n            \n            positions = rebuttals\n        \n        # Judge decides\n        decision = self.judge.decide(positions)\n        return decision\n\n# Example: Technical Decision\ndebate = DebateMultiAgent(\n    debaters=[\n        Agent(role=\"Microservices Advocate\"),\n        Agent(role=\"Monolith Advocate\"),\n        Agent(role=\"Serverless Advocate\")\n    ],\n    judge=Agent(role=\"CTO\")\n)\n\ndecision = debate.run(\"Best architecture for our new platform?\")\n```\n\n**Debate Flow:**\n```\nRound 1:\nMicroservices: \"Scalability and independent deployment\"\nMonolith: \"Simplicity and easier debugging\"\nServerless: \"No infrastructure management, auto-scaling\"\n\nRound 2:\nMicroservices: \"Monoliths become unmaintainable at scale\"\nMonolith: \"Microservices add complexity and latency\"\nServerless: \"Both have vendor lock-in issues\"\n\nRound 3:\nMicroservices: \"Tooling has matured, complexity manageable\"\nMonolith: \"Start simple, extract services when needed\"\nServerless: \"Best of both: modular functions, managed infra\"\n\nJudge Decision: \"Start with modular monolith, extract critical services, use serverless for event-driven components. Balanced approach based on current team size and requirements.\"\n```\n\n## Agent Communication Protocols\n\n### Message Passing\n\n```python\nclass Message:\n    def __init__(self, sender, receiver, content, msg_type):\n        self.sender = sender\n        self.receiver = receiver  # Can be broadcast: \"all\"\n        self.content = content\n        self.msg_type = msg_type  # query, response, task, result\n        self.timestamp = time.time()\n\nclass MessageBus:\n    def __init__(self):\n        self.messages = []\n        self.subscribers = {}\n    \n    def publish(self, message):\n        self.messages.append(message)\n        \n        # Deliver to subscribers\n        if message.receiver == \"all\":\n            for agent_id in self.subscribers:\n                self.subscribers[agent_id].receive(message)\n        elif message.receiver in self.subscribers:\n            self.subscribers[message.receiver].receive(message)\n    \n    def subscribe(self, agent_id, agent):\n        self.subscribers[agent_id] = agent\n```\n\n### Shared Context\n\n```python\nclass SharedContext:\n    def __init__(self):\n        self.facts = {}  # Shared knowledge\n        self.goals = []  # Active goals\n        self.results = {}  # Completed work\n    \n    def add_fact(self, key, value, source):\n        self.facts[key] = {\n            'value': value,\n            'source': source,\n            'timestamp': time.time()\n        }\n    \n    def get_facts(self, filter_fn=None):\n        if filter_fn:\n            return {k: v for k, v in self.facts.items() if filter_fn(k, v)}\n        return self.facts\n    \n    def add_result(self, task_id, result, agent_id):\n        self.results[task_id] = {\n            'result': result,\n            'agent': agent_id,\n            'timestamp': time.time()\n        }\n```\n\n## Coordination Strategies\n\n### Task Allocation\n\n```python\ndef allocate_tasks(tasks, agents):\n    \"\"\"\n    Assign tasks to agents based on capabilities and load\n    \"\"\"\n    allocation = {}\n    \n    # Score each agent for each task\n    for task in tasks:\n        scores = []\n        for agent in agents:\n            capability_score = agent.capability_match(task)\n            load_score = 1.0 - (agent.current_load / agent.max_load)\n            total_score = capability_score * 0.7 + load_score * 0.3\n            scores.append((agent, total_score))\n        \n        # Assign to best agent\n        best_agent = max(scores, key=lambda x: x[1])[0]\n        allocation[task.id] = best_agent\n        best_agent.current_load += task.estimated_cost\n    \n    return allocation\n```\n\n### Consensus Building\n\n```python\ndef reach_consensus(agents, proposal, threshold=0.7):\n    \"\"\"\n    Agents vote on a proposal until consensus\n    \"\"\"\n    max_rounds = 5\n    \n    for round in range(max_rounds):\n        votes = []\n        \n        for agent in agents:\n            vote = agent.evaluate(proposal)\n            votes.append(vote)\n            \n            if vote['support'] < 0.5:\n                # Agent provides reason for opposition\n                feedback = agent.explain_opposition(proposal)\n                proposal = modify_proposal(proposal, feedback)\n        \n        # Check if consensus reached\n        support_ratio = sum(v['support'] for v in votes) / len(votes)\n        \n        if support_ratio >= threshold:\n            return {'consensus': True, 'proposal': proposal, 'support': support_ratio}\n    \n    return {'consensus': False, 'proposal': proposal, 'support': support_ratio}\n```\n\n## Real-World Multi-Agent Systems\n\n### 1. Software Development Team\n\n```python\ndev_team = {\n    'pm': ProductManagerAgent(),\n    'designer': DesignerAgent(),\n    'frontend': FrontendDevAgent(),\n    'backend': BackendDevAgent(),\n    'qa': QAAgent(),\n    'devops': DevOpsAgent()\n}\n\nworkflow = [\n    ('pm', 'Create requirements'),\n    ('designer', 'Design UI/UX'),\n    (['frontend', 'backend'], 'Implement features'),  # Parallel\n    ('qa', 'Test implementation'),\n    ('devops', 'Deploy to production')\n]\n\nresult = execute_workflow(dev_team, workflow, \"Build user authentication\")\n```\n\n### 2. Investment Research Team\n\n```python\ninvestment_team = HubSpokeMultiAgent(\n    coordinator=ChiefAnalystAgent(),\n    specialists={\n        'fundamental': FundamentalAnalyst(),\n        'technical': TechnicalAnalyst(),\n        'sentiment': SentimentAnalyst(),\n        'macro': MacroEconomist(),\n        'risk': RiskAnalyst()\n    }\n)\n\nreport = investment_team.run(\"Should we invest in Tesla?\")\n```\n\n### 3. Content Creation Studio\n\n```python\ncontent_studio = PipelineMultiAgent([\n    TrendResearcher(),    # Find trending topics\n    TopicGenerator(),     # Generate content ideas\n    Outliner(),           # Create structure\n    Writer(),             # Write content\n    Editor(),             # Edit and refine\n    FactChecker(),        # Verify claims\n    SEOOptimizer(),       # Optimize for search\n    Scheduler()           # Plan publication\n])\n\narticles = content_studio.run(niche=\"AI Technology\")\n```\n\n## Error Handling in Multi-Agent Systems\n\n### Agent Failure Recovery\n\n```python\nclass ResilientMultiAgent:\n    def __init__(self, agents):\n        self.agents = agents\n        self.backup_agents = {}  # Backup for critical agents\n    \n    def execute_with_fallback(self, agent_id, task):\n        agent = self.agents[agent_id]\n        \n        try:\n            return agent.execute(task)\n        except Exception as e:\n            logging.error(f\"Agent {agent_id} failed: {e}\")\n            \n            # Try backup agent\n            if agent_id in self.backup_agents:\n                backup = self.backup_agents[agent_id]\n                return backup.execute(task)\n            \n            # Redistribute task\n            return self.redistribute_task(task)\n```\n\n### Deadlock Prevention\n\n```python\ndef detect_deadlock(agents, timeout=30):\n    \"\"\"\n    Detect if agents are waiting on each other\n    \"\"\"\n    wait_graph = build_wait_graph(agents)\n    \n    if has_cycle(wait_graph):\n        # Deadlock detected\n        logging.warning(\"Deadlock detected\")\n        \n        # Break cycle by resetting lowest priority agent\n        cycle_agents = find_cycle(wait_graph)\n        lowest_priority = min(cycle_agents, key=lambda a: a.priority)\n        lowest_priority.reset()\n        \n        return True\n    \n    return False\n```\n\n## Performance Optimization\n\n### Parallel Execution\n\n```python\nimport asyncio\n\nclass ParallelMultiAgent:\n    async def run_parallel(self, tasks):\n        # Execute independent tasks concurrently\n        coroutines = []\n        \n        for task in tasks:\n            agent = self.select_agent(task)\n            coroutines.append(agent.execute_async(task))\n        \n        results = await asyncio.gather(*coroutines)\n        return results\n```\n\n### Load Balancing\n\n```python\ndef balance_load(tasks, agents):\n    # Sort tasks by estimated cost\n    sorted_tasks = sorted(tasks, key=lambda t: t.cost, reverse=True)\n    \n    # Assign using greedy algorithm\n    agent_loads = {a: 0 for a in agents}\n    allocation = {}\n    \n    for task in sorted_tasks:\n        # Find least loaded capable agent\n        capable = [a for a in agents if a.can_handle(task)]\n        min_load_agent = min(capable, key=lambda a: agent_loads[a])\n        \n        allocation[task] = min_load_agent\n        agent_loads[min_load_agent] += task.cost\n    \n    return allocation\n```",
        keyPoints: [
          "Multi-agent systems distribute work across specialized agents",
          "Pipeline, hub-spoke, and autonomous architectures serve different needs",
          "Message passing and shared context enable agent coordination",
          "Task allocation considers agent capabilities and current load",
          "Resilience requires fallback agents and deadlock prevention"
        ],
        handsOnExercise: "Design a multi-agent system for a specific domain (e.g., customer support, research, content creation). Define 4-5 specialized agents and their coordination strategy."
      }
    ]
  },
  {
    id: "module-5",
    title: "Specialized Applications",
    description: "Apply prompt engineering to code generation, RAG, multimodal AI, and business use cases.",
    duration: "16-20 hours",
    lessons: [
      {
        id: "lesson-5-1",
        title: "Advanced Retrieval-Augmented Generation (RAG)",
        duration: "5 hours",
        content: "# Advanced RAG Systems\n\nBuild production-grade retrieval-augmented generation systems that ground AI responses in real data.\n\n## RAG Fundamentals\n\n### Why RAG?\n\n**Problems RAG Solves:**\n- Knowledge cutoff dates (models trained on old data)\n- Hallucination on facts (making up information)\n- Domain-specific knowledge needs\n- Real-time data requirements\n- Proprietary/private data access\n\n### Basic RAG Architecture\n\n```\nUser Query\n    ‚Üì\n[1] Query Embedding\n    ‚Üì\n[2] Vector Search in Knowledge Base\n    ‚Üì\n[3] Retrieve Top-K Documents\n    ‚Üì\n[4] Construct Prompt with Context\n    ‚Üì\n[5] LLM Generates Answer\n    ‚Üì\nResponse with Citations\n```\n\n## Building a RAG System\n\n### Step 1: Document Ingestion and Chunking\n\n```python\nclass DocumentProcessor:\n    def __init__(self, chunk_size=500, chunk_overlap=50):\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n    \n    def chunk_document(self, document, chunk_by='tokens'):\n        \"\"\"\n        Split document into overlapping chunks\n        \"\"\"\n        if chunk_by == 'tokens':\n            return self.chunk_by_tokens(document)\n        elif chunk_by == 'semantic':\n            return self.chunk_semantically(document)\n    \n    def chunk_by_tokens(self, text):\n        tokens = tokenize(text)\n        chunks = []\n        \n        start = 0\n        while start < len(tokens):\n            end = start + self.chunk_size\n            chunk_tokens = tokens[start:end]\n            chunks.append(detokenize(chunk_tokens))\n            start += self.chunk_size - self.chunk_overlap\n        \n        return chunks\n    \n    def chunk_semantically(self, text):\n        \"\"\"\n        Chunk by semantic boundaries (paragraphs, sections)\n        \"\"\"\n        paragraphs = text.split('\\n\\n')\n        chunks = []\n        current_chunk = []\n        current_length = 0\n        \n        for para in paragraphs:\n            para_length = len(tokenize(para))\n            \n            if current_length + para_length > self.chunk_size:\n                if current_chunk:\n                    chunks.append('\\n\\n'.join(current_chunk))\n                current_chunk = [para]\n                current_length = para_length\n            else:\n                current_chunk.append(para)\n                current_length += para_length\n        \n        if current_chunk:\n            chunks.append('\\n\\n'.join(current_chunk))\n        \n        return chunks\n```\n\n**Chunking Best Practices:**\n- **Size**: 300-800 tokens per chunk (balances context and precision)\n- **Overlap**: 10-20% overlap prevents information loss at boundaries\n- **Semantic boundaries**: Chunk at paragraph/section breaks when possible\n- **Metadata**: Store source, page number, date with each chunk\n\n### Step 2: Vector Embeddings\n\n```python\nimport openai\nimport numpy as np\n\nclass EmbeddingService:\n    def __init__(self, model='text-embedding-3-large'):\n        self.model = model\n    \n    def embed_text(self, text):\n        response = openai.embeddings.create(\n            model=self.model,\n            input=text\n        )\n        return response.data[0].embedding\n    \n    def embed_batch(self, texts, batch_size=100):\n        embeddings = []\n        for i in range(0, len(texts), batch_size):\n            batch = texts[i:i+batch_size]\n            response = openai.embeddings.create(\n                model=self.model,\n                input=batch\n            )\n            embeddings.extend([d.embedding for d in response.data])\n        return embeddings\n```\n\n**Embedding Model Comparison:**\n- **OpenAI text-embedding-3-large**: 3072 dims, high quality, $0.13/1M tokens\n- **OpenAI text-embedding-3-small**: 1536 dims, fast, $0.02/1M tokens\n- **Cohere embed-english-v3.0**: 1024 dims, multilingual support\n- **Open source (BAAI/bge-large)**: Free, self-hosted, 1024 dims\n\n### Step 3: Vector Database\n\n```python\nfrom pinecone import Pinecone, ServerlessSpec\n\nclass VectorStore:\n    def __init__(self, index_name, dimension=1536):\n        self.pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))\n        \n        # Create index if doesn't exist\n        if index_name not in self.pc.list_indexes().names():\n            self.pc.create_index(\n                name=index_name,\n                dimension=dimension,\n                metric='cosine',\n                spec=ServerlessSpec(cloud='aws', region='us-east-1')\n            )\n        \n        self.index = self.pc.Index(index_name)\n    \n    def upsert_documents(self, chunks, embeddings, metadata_list):\n        vectors = []\n        for i, (chunk, embedding, metadata) in enumerate(zip(chunks, embeddings, metadata_list)):\n            vectors.append({\n                'id': f'doc_{i}',\n                'values': embedding,\n                'metadata': {\n                    'text': chunk,\n                    **metadata\n                }\n            })\n        \n        # Batch upsert\n        batch_size = 100\n        for i in range(0, len(vectors), batch_size):\n            self.index.upsert(vectors=vectors[i:i+batch_size])\n    \n    def search(self, query_embedding, top_k=5, filter=None):\n        results = self.index.query(\n            vector=query_embedding,\n            top_k=top_k,\n            filter=filter,\n            include_metadata=True\n        )\n        return results.matches\n```\n\n**Vector Database Options:**\n- **Pinecone**: Managed, scales automatically, $0.096/1M vectors/month\n- **Weaviate**: Open source, hybrid search, self-hosted or cloud\n- **Qdrant**: High performance, filters, good for production\n- **ChromaDB**: Simple, embedded, great for prototyping\n\n### Step 4: Retrieval\n\n```python\nclass RAGRetriever:\n    def __init__(self, vector_store, embedding_service):\n        self.vector_store = vector_store\n        self.embedding_service = embedding_service\n    \n    def retrieve(self, query, top_k=5, rerank=True):\n        # Embed query\n        query_embedding = self.embedding_service.embed_text(query)\n        \n        # Retrieve candidates (get more for reranking)\n        k = top_k * 3 if rerank else top_k\n        results = self.vector_store.search(query_embedding, top_k=k)\n        \n        # Extract documents\n        documents = [{\n            'text': r.metadata['text'],\n            'score': r.score,\n            'metadata': r.metadata\n        } for r in results]\n        \n        # Optional: Rerank for better relevance\n        if rerank:\n            documents = self.rerank(query, documents, top_k)\n        \n        return documents\n    \n    def rerank(self, query, documents, top_k):\n        \"\"\"\n        Use cross-encoder for better relevance scoring\n        \"\"\"\n        from sentence_transformers import CrossEncoder\n        \n        model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n        pairs = [[query, doc['text']] for doc in documents]\n        scores = model.predict(pairs)\n        \n        # Sort by reranked scores\n        for doc, score in zip(documents, scores):\n            doc['rerank_score'] = float(score)\n        \n        documents.sort(key=lambda x: x['rerank_score'], reverse=True)\n        return documents[:top_k]\n```\n\n### Step 5: Generation with Context\n\n```python\ndef generate_rag_response(query, retrieved_docs, model='gpt-4'):\n    # Format context from retrieved documents\n    context = \"\\n\\n\".join([\n        f\"[Document {i+1}] (Score: {doc['score']:.3f})\\n{doc['text']}\"\n        for i, doc in enumerate(retrieved_docs)\n    ])\n    \n    prompt = f\"\"\"\nYou are a helpful assistant. Answer the user's question based ONLY on the provided context. If the context doesn't contain enough information, say so.\n\nContext:\n{context}\n\nQuestion: {query}\n\nInstructions:\n1. Base your answer strictly on the context above\n2. Cite source documents using [Document X] notation\n3. If information is insufficient, clearly state that\n4. Do not make up or infer information not in the context\n\nAnswer:\n\"\"\"\n    \n    response = openai.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0.1  # Low temperature for factual responses\n    )\n    \n    return response.choices[0].message.content\n```\n\n## Advanced RAG Techniques\n\n### 1. Hybrid Search (Dense + Sparse)\n\nCombine vector similarity with keyword search.\n\n```python\ndef hybrid_search(query, vector_store, bm25_index, alpha=0.5):\n    # Dense retrieval (semantic)\n    query_embedding = embed(query)\n    dense_results = vector_store.search(query_embedding, top_k=20)\n    \n    # Sparse retrieval (keyword)\n    sparse_results = bm25_index.search(query, top_k=20)\n    \n    # Combine scores (weighted)\n    combined = {}\n    for doc in dense_results:\n        combined[doc.id] = alpha * doc.score\n    \n    for doc in sparse_results:\n        combined[doc.id] = combined.get(doc.id, 0) + (1 - alpha) * doc.score\n    \n    # Sort and return top-k\n    sorted_docs = sorted(combined.items(), key=lambda x: x[1], reverse=True)\n    return sorted_docs[:5]\n```\n\n### 2. Query Expansion\n\nGenerate multiple query variations for better recall.\n\n```python\ndef expand_query(original_query):\n    expansion_prompt = f\"\"\"\n    Generate 3 alternative phrasings of this search query that maintain the same intent:\n    \n    Original: {original_query}\n    \n    Alternatives (one per line):\n    \"\"\"\n    \n    response = llm.generate(expansion_prompt)\n    alternatives = [q.strip() for q in response.split('\\n') if q.strip()]\n    \n    # Search with all variations\n    all_results = []\n    for query in [original_query] + alternatives:\n        results = retrieve(query, top_k=3)\n        all_results.extend(results)\n    \n    # Deduplicate and return top results\n    return deduplicate(all_results)[:5]\n```\n\n### 3. Self-Query: Extract Filters from Query\n\n```python\ndef self_query(user_query):\n    \"\"\"\n    Extract structured filters from natural language query\n    \"\"\"\n    extraction_prompt = f\"\"\"\n    Extract search parameters from this query:\n    \n    Query: \"{user_query}\"\n    \n    Extract (JSON):\n    - query: semantic search terms\n    - filters: dict of metadata filters (date, category, author, etc.)\n    \n    Example output:\n    {{\n      \"query\": \"machine learning algorithms\",\n      \"filters\": {{\"date_after\": \"2023-01-01\", \"category\": \"research\"}}\n    }}\n    \"\"\"\n    \n    response = llm.generate(extraction_prompt)\n    params = json.loads(response)\n    \n    # Use extracted filters\n    results = vector_store.search(\n        embed(params['query']),\n        filter=params['filters']\n    )\n    return results\n```\n\n### 4. Parent-Child Chunking\n\nRetrieve small chunks but provide larger context to LLM.\n\n```python\nclass HierarchicalRAG:\n    def __init__(self):\n        self.child_store = VectorStore('children', dimension=1536)\n        self.parent_store = {}  # Document ID -> full text\n    \n    def index_document(self, document):\n        # Split into large parent chunks (2000 tokens)\n        parents = chunk_document(document, size=2000)\n        \n        for parent_id, parent in enumerate(parents):\n            # Store parent\n            self.parent_store[f'parent_{parent_id}'] = parent\n            \n            # Split parent into small children (400 tokens)\n            children = chunk_document(parent, size=400)\n            \n            # Index children with parent reference\n            for child in children:\n                embedding = embed(child)\n                self.child_store.upsert(\n                    embedding=embedding,\n                    metadata={'text': child, 'parent_id': f'parent_{parent_id}'}\n                )\n    \n    def retrieve(self, query, top_k=3):\n        # Search small chunks for precision\n        child_results = self.child_store.search(embed(query), top_k=top_k)\n        \n        # Return full parent chunks for context\n        parent_ids = set(r.metadata['parent_id'] for r in child_results)\n        parents = [self.parent_store[pid] for pid in parent_ids]\n        \n        return parents\n```\n\n## Evaluation and Optimization\n\n### RAG Evaluation Metrics\n\n```python\ndef evaluate_rag_system(test_queries, ground_truth):\n    metrics = {\n        'retrieval_accuracy': [],\n        'answer_faithfulness': [],\n        'answer_relevance': [],\n        'latency': []\n    }\n    \n    for query, expected in zip(test_queries, ground_truth):\n        start = time.time()\n        \n        # Retrieve\n        docs = retrieve(query)\n        retrieved_ids = [d['id'] for d in docs]\n        \n        # Check if correct docs retrieved\n        relevant_docs = expected['relevant_docs']\n        retrieval_acc = len(set(retrieved_ids) & set(relevant_docs)) / len(relevant_docs)\n        metrics['retrieval_accuracy'].append(retrieval_acc)\n        \n        # Generate answer\n        answer = generate_rag_response(query, docs)\n        \n        # Evaluate answer quality\n        faithfulness = check_faithfulness(answer, docs)  # No hallucination\n        relevance = check_relevance(answer, query)  # Answers the question\n        \n        metrics['answer_faithfulness'].append(faithfulness)\n        metrics['answer_relevance'].append(relevance)\n        metrics['latency'].append(time.time() - start)\n    \n    # Aggregate\n    return {k: np.mean(v) for k, v in metrics.items()}\n```\n\n### Production Optimizations\n\n**1. Caching**\n```python\nfrom functools import lru_cache\n\n@lru_cache(maxsize=1000)\ndef cached_retrieve(query):\n    return retrieve(query)\n```\n\n**2. Async Processing**\n```python\nimport asyncio\n\nasync def async_rag(queries):\n    tasks = [retrieve_async(q) for q in queries]\n    results = await asyncio.gather(*tasks)\n    return results\n```\n\n**3. Streaming Responses**\n```python\ndef stream_rag_response(query, docs):\n    for chunk in openai.chat.completions.create(\n        model='gpt-4',\n        messages=[{\"role\": \"user\", \"content\": build_prompt(query, docs)}],\n        stream=True\n    ):\n        if chunk.choices[0].delta.content:\n            yield chunk.choices[0].delta.content\n```",
        keyPoints: [
          "RAG combines retrieval with generation to ground responses in real data",
          "Chunking strategy critically affects performance (300-800 tokens ideal)",
          "Hybrid search (dense + sparse) improves recall over pure vector search",
          "Reranking with cross-encoders boosts precision after initial retrieval",
          "Parent-child chunking retrieves precise chunks but provides broader context to LLM"
        ],
        handsOnExercise: "Build a RAG system for a specific knowledge domain. Ingest 10-20 documents, implement semantic search, and generate answers with citations."
      },
      {
        id: "lesson-5-2",
        title: "Code Generation and AI-Assisted Development",
        duration: "4 hours",
        content: "# Code Generation with LLMs\n\nMaster prompt engineering techniques for generating, reviewing, and debugging code.\n\n## Code Generation Fundamentals\n\n### Why LLMs Excel at Code\n\n1. **Large training corpus**: Billions of lines of open-source code\n2. **Structured nature**: Code follows strict syntax rules\n3. **Pattern recognition**: Common idioms and patterns\n4. **Multi-language understanding**: Transfer learning across languages\n\n### Effective Code Prompts\n\n**Bad Prompt:**\n```\n\"Write a function to sort data\"\n```\n\n**Good Prompt:**\n```\nWrite a Python function that:\n- Takes a list of dictionaries as input\n- Each dict has 'name' (str) and 'score' (int) keys\n- Sorts by score descending, then name ascending\n- Returns the sorted list\n- Include type hints and docstring\n\nExample input: [{'name': 'Alice', 'score': 85}, {'name': 'Bob', 'score': 92}]\nExample output: [{'name': 'Bob', 'score': 92}, {'name': 'Alice', 'score': 85}]\n```\n\n### Code Generation Template\n\n```\n# Context\nLanguage: [Python/JavaScript/etc.]\nFramework/Libraries: [if any]\nPurpose: [what this code will do]\n\n# Requirements\n1. [Specific requirement]\n2. [Edge case to handle]\n3. [Performance constraint]\n\n# Input/Output Examples\nInput: [example]\nOutput: [expected result]\n\n# Additional Constraints\n- Code style: [PEP 8, Airbnb, Google]\n- Error handling: [how to handle errors]\n- Documentation: [docstrings, comments]\n\nGenerate the code:\n```\n\n## Advanced Code Generation Patterns\n\n### 1. Step-by-Step Implementation\n\n```\nTask: Build a REST API endpoint for user authentication\n\nBreak this down into steps and implement each:\n\nStep 1: Define the data model (User schema)\nStep 2: Create password hashing utility\nStep 3: Implement registration endpoint\nStep 4: Implement login endpoint\nStep 5: Add JWT token generation\nStep 6: Create authentication middleware\n\nFor each step, provide:\n- Complete code\n- Explanation of key decisions\n- Testing considerations\n\nStart with Step 1:\n```\n\n### 2. Fill-in-the-Blank Pattern\n\n```python\n# Provide partial code and ask for completion\n\ndef binary_search(arr, target):\n    \"\"\"\n    Perform binary search on sorted array.\n    \n    Args:\n        arr: Sorted list of integers\n        target: Value to find\n    \n    Returns:\n        Index of target, or -1 if not found\n    \"\"\"\n    left, right = 0, len(arr) - 1\n    \n    # TODO: Complete the binary search logic\n    # Handle: middle element comparison, left/right pointer updates\n    \n    return -1\n```\n\n### 3. Test-Driven Development\n\n```\nWrite a function that passes these test cases:\n\n```python\nassert calculate_fibonacci(0) == 0\nassert calculate_fibonacci(1) == 1\nassert calculate_fibonacci(10) == 55\nassert calculate_fibonacci(20) == 6765\n\n# Should handle edge cases\nassert calculate_fibonacci(-1) raises ValueError\nassert calculate_fibonacci(1.5) raises TypeError\n```\n\nRequirements:\n- Use memoization for efficiency\n- Time complexity: O(n)\n- Space complexity: O(n)\n\nImplement calculate_fibonacci function:\n```\n\n## Code Review and Debugging\n\n### Code Review Prompt\n\n```\nReview this code for:\n1. **Correctness**: Logic errors, edge cases\n2. **Security**: Injection vulnerabilities, data validation\n3. **Performance**: Time/space complexity, bottlenecks\n4. **Maintainability**: Readability, naming, documentation\n5. **Best Practices**: Idioms, patterns for this language\n\n```python\n[CODE TO REVIEW]\n```\n\nProvide:\n- Issues found (severity: Critical/High/Medium/Low)\n- Specific line numbers\n- Suggested fixes\n- Improved version of problematic sections\n```\n\n### Debugging Assistance\n\n```\nThis code produces an error. Debug it:\n\n```python\ndef merge_sorted_arrays(arr1, arr2):\n    result = []\n    i, j = 0, 0\n    \n    while i < len(arr1) and j < len(arr2):\n        if arr1[i] < arr2[j]:\n            result.append(arr1[i])\n            i += 1\n        else:\n            result.append(arr2[j])\n            j += 1\n    \n    return result\n\narr1 = [1, 3, 5]\narr2 = [2, 4, 6, 8]\nprint(merge_sorted_arrays(arr1, arr2))\n# Expected: [1, 2, 3, 4, 5, 6, 8]\n# Actual: [1, 2, 3, 4, 5, 6]\n```\n\nError: Missing elements from second array\n\nAnalyze:\n1. Trace execution step-by-step\n2. Identify the bug\n3. Explain why it occurs\n4. Provide corrected version\n5. Add test cases to prevent regression\n```\n\n## Language-Specific Patterns\n\n### Python Code Generation\n\n```\nGenerate Python code following these conventions:\n- Type hints for all function parameters and returns\n- Docstrings in Google format\n- Use pathlib for file operations\n- Prefer f-strings for formatting\n- Exception handling with specific exception types\n- List comprehensions where appropriate\n\nTask: [specific task]\n```\n\n### JavaScript/TypeScript\n\n```\nGenerate TypeScript code with:\n- Strict type definitions (no 'any')\n- Async/await for asynchronous operations\n- JSDoc comments\n- ES6+ features (arrow functions, destructuring)\n- Error boundaries\n\nTask: [specific task]\n```\n\n### SQL Query Generation\n\n```\nGenerate SQL query for:\n- Database: PostgreSQL 14\n- Schema: [describe tables and relationships]\n- Requirement: [what data to retrieve]\n- Constraints: [filters, sorting, grouping]\n- Performance: Use appropriate indexes, avoid N+1 queries\n\nProvide:\n1. The SQL query\n2. Expected output format\n3. Explanation of key clauses\n4. Index recommendations\n```\n\n## Code Explanation\n\n### Explain Like I'm 5 (ELI5)\n\n```\nExplain this code to a beginner who just started programming:\n\n```python\n[COMPLEX CODE]\n```\n\nUse:\n- Simple analogies\n- No jargon (or explain jargon)\n- Step-by-step walkthrough\n- Visual descriptions where helpful\n```\n\n### Technical Deep Dive\n\n```\nProvide a technical analysis of this code:\n\n```python\n[CODE]\n```\n\nExplain:\n1. **Algorithm/Approach**: What method is used?\n2. **Time Complexity**: Big O analysis\n3. **Space Complexity**: Memory usage\n4. **Trade-offs**: Why this approach over alternatives?\n5. **Optimizations**: Possible improvements\n6. **Edge Cases**: What could break this?\n```\n\n## Code Transformation\n\n### Refactoring\n\n```\nRefactor this code to improve:\n- Readability: Better naming, structure\n- Maintainability: Reduce complexity, modularity\n- Performance: Optimize bottlenecks\n- Testability: Easier to unit test\n\n```python\n[LEGACY CODE]\n```\n\nProvide:\n1. Refactored version\n2. Explanation of each change\n3. Before/after comparison\n```\n\n### Language Translation\n\n```\nTranslate this [Source Language] code to [Target Language]:\n\n```[source]\n[CODE]\n```\n\nEnsure:\n- Idiomatic [Target Language] patterns\n- Equivalent functionality\n- Similar performance characteristics\n- Appropriate libraries/frameworks for [Target Language]\n- Comments explaining language-specific differences\n```\n\n## AI-Assisted Development Workflows\n\n### Pair Programming with AI\n\n**Pattern 1: AI as Navigator**\n```\nI'm implementing [feature]. Guide me through it:\n\n1. Ask me clarifying questions\n2. Suggest an approach\n3. I'll implement sections\n4. Review my code and suggest improvements\n5. Repeat\n\nStart: What should I build first?\n```\n\n**Pattern 2: AI as Driver**\n```\nYou implement [feature] while explaining your thought process:\n\n1. Explain your approach\n2. Write code in small increments\n3. I'll review and ask questions\n4. Adjust based on my feedback\n5. Continue until complete\n\nRequirements: [detailed specs]\n```\n\n### Iterative Development\n\n```\nIteration 1: Basic implementation (MVP)\n[CODE]\n\nIteration 2: Add error handling\n[IMPROVED CODE]\n\nIteration 3: Optimize performance\n[OPTIMIZED CODE]\n\nIteration 4: Add comprehensive tests\n[FINAL CODE + TESTS]\n```\n\n## Best Practices\n\n### 1. Context is King\n\n**Good:**\n```\nI'm building a microservice in Go for processing payment webhooks from Stripe.\nCurrent architecture: \n- REST API (Gin framework)\n- PostgreSQL database\n- Redis for caching\n- Deployed on Kubernetes\n\nI need a function that...\n```\n\n**Bad:**\n```\nWrite a function\n```\n\n### 2. Provide Examples\n\nAlways include input/output examples:\n```\nExample 1:\n  Input: {\"user_id\": 123, \"amount\": 50.00}\n  Output: {\"transaction_id\": \"txn_456\", \"status\": \"success\"}\n\nExample 2 (error case):\n  Input: {\"user_id\": 999, \"amount\": -10.00}\n  Output: {\"error\": \"Invalid amount\"}\n```\n\n### 3. Specify Constraints\n\n```\nConstraints:\n- Must handle 1000 requests/second\n- Response time < 100ms\n- Must be thread-safe\n- Memory usage < 100MB\n```\n\n### 4. Request Explanations\n\n```\nFor each function, provide:\n1. Code implementation\n2. Docstring/comments\n3. Time complexity analysis\n4. Why you chose this approach\n```\n\n## Common Pitfalls\n\n### Pitfall 1: Vague Requirements\n\n‚ùå \"Make it faster\"\n‚úÖ \"Optimize this function to run in O(n log n) instead of O(n¬≤)\"\n\n### Pitfall 2: Missing Context\n\n‚ùå \"Fix this bug: [code]\"\n‚úÖ \"Fix this bug: [code]\nError message: [error]\nExpected behavior: [expected]\nActual behavior: [actual]\"\n\n### Pitfall 3: No Validation\n\n‚ùå Use generated code without testing\n‚úÖ Always test generated code with edge cases\n\n### Pitfall 4: Ignoring Security\n\n‚ùå \"Generate a login endpoint\"\n‚úÖ \"Generate a secure login endpoint with:\n- Password hashing (bcrypt)\n- SQL injection prevention\n- Rate limiting\n- CSRF protection\"",
        keyPoints: [
          "Detailed prompts with examples, constraints, and context generate better code",
          "Test-driven development approach helps ensure correctness",
          "Code review prompts should specify review dimensions (security, performance, maintainability)",
          "Language-specific conventions and idioms should be explicitly requested",
          "Always validate and test generated code before using in production"
        ],
        handsOnExercise: "Use AI to implement a complete feature: write requirements, generate code, review it, write tests, and refactor based on feedback."
      },
      {
        id: "lesson-5-3",
        title: "Multimodal AI: Vision, Audio, and Beyond",
        duration: "4 hours",
        content: "# Multimodal Prompt Engineering\n\nWork with AI models that understand images, audio, video, and combinations of modalities.\n\n## Vision Models: Understanding Images\n\n### GPT-4 Vision (GPT-4V)\n\n**Basic Image Analysis:**\n```python\nimport openai\nimport base64\n\ndef analyze_image(image_path, question):\n    with open(image_path, 'rb') as image_file:\n        base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n    \n    response = openai.chat.completions.create(\n        model=\"gpt-4-vision-preview\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": question},\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}\n                    }\n                ]\n            }\n        ],\n        max_tokens=1000\n    )\n    \n    return response.choices[0].message.content\n\n# Example usage\nquestion = \"What objects are in this image? Describe their spatial relationships.\"\nresult = analyze_image(\"scene.jpg\", question)\n```\n\n### Effective Vision Prompts\n\n**1. Image Classification**\n```\nAnalyze this image and classify it into one of these categories:\n- Product Photo\n- Nature/Landscape\n- Portrait\n- Screenshot/UI\n- Document/Text\n- Diagram/Chart\n- Other\n\nProvide:\n1. Category\n2. Confidence level (1-10)\n3. Key visual elements that led to this classification\n```\n\n**2. OCR and Document Understanding**\n```\nExtract all text from this image:\n\n1. Transcribe text exactly as shown\n2. Preserve formatting (headings, lists, tables)\n3. Note any non-text elements (logos, images)\n4. Flag any text that's unclear or partially obscured\n\nOutput format:\n# Transcription\n[text here]\n\n# Document Structure\n[description]\n\n# Confidence Notes\n[areas of uncertainty]\n```\n\n**3. Visual Question Answering**\n```\nImage: [product photo]\n\nAnswer these questions:\n1. What is the product?\n2. What color(s) is it?\n3. What's its approximate condition? (new/used/damaged)\n4. Are there any visible defects or damage?\n5. What's visible in the background?\n6. Estimated size based on visual cues?\n```\n\n**4. Image Comparison**\n```\nCompare these two images:\n\nImage 1: [before]\nImage 2: [after]\n\nIdentify:\n1. What changed between the images?\n2. What stayed the same?\n3. Rate the significance of changes (1-10)\n4. Categorize changes: [added/removed/modified/moved]\n```\n\n### Advanced Vision Use Cases\n\n**UI/UX Analysis**\n```\nAnalyze this website screenshot:\n\n1. **Layout Analysis**\n   - Identify main sections (header, navigation, content, footer)\n   - Describe visual hierarchy\n   - Note use of whitespace\n\n2. **Design Evaluation**\n   - Color scheme and contrast\n   - Typography choices\n   - Brand consistency\n\n3. **UX Assessment**\n   - Navigation clarity\n   - Call-to-action visibility\n   - Mobile-friendliness indicators\n\n4. **Accessibility Concerns**\n   - Text size and readability\n   - Color contrast issues\n   - Potential navigation problems\n\n5. **Improvement Suggestions**\n   - Top 3 recommendations\n   - Prioritize by impact\n```\n\n**Medical Image Analysis** (Educational/Research Context)\n```\nDisclaimer: This is for educational purposes only. Not for clinical use.\n\nAnalyze this [X-ray/MRI/CT scan]:\n\n1. Image quality assessment\n2. Visible anatomical structures\n3. Notable observations\n4. Areas requiring expert review\n5. Comparison to normal anatomy\n\nNote: All findings require verification by licensed medical professionals.\n```\n\n**Chart and Graph Extraction**\n```\nExtract data from this chart:\n\n1. **Chart Type**: [bar/line/pie/scatter/etc.]\n2. **Title and Labels**: [extract text]\n3. **Data Points**: Extract all visible data\n4. **Trends**: Describe patterns\n5. **Annotations**: Note any callouts or highlights\n\nProvide data in JSON format:\n{\n  \"chart_type\": \"...\",\n  \"title\": \"...\",\n  \"x_label\": \"...\",\n  \"y_label\": \"...\",\n  \"data\": [\n    {\"label\": \"...\", \"value\": ...},\n    ...\n  ]\n}\n```\n\n## Audio Understanding\n\n### Whisper for Speech-to-Text\n\n```python\nimport openai\n\ndef transcribe_audio(audio_path):\n    with open(audio_path, 'rb') as audio_file:\n        transcript = openai.audio.transcriptions.create(\n            model=\"whisper-1\",\n            file=audio_file,\n            response_format=\"verbose_json\",\n            timestamp_granularities=[\"word\"]\n        )\n    \n    return transcript\n\n# Then use GPT-4 to analyze transcription\ndef analyze_transcription(transcript_text):\n    prompt = f\"\"\"\n    Analyze this speech transcription:\n    \n    {transcript_text}\n    \n    Provide:\n    1. Summary of main points\n    2. Speaker sentiment/tone\n    3. Key topics discussed\n    4. Action items mentioned\n    5. Questions asked\n    \"\"\"\n    \n    response = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    \n    return response.choices[0].message.content\n```\n\n### Audio Analysis Prompts\n\n**Meeting Transcription Analysis**\n```\nTranscript: [meeting audio transcription]\n\nProvide:\n1. **Executive Summary** (3-4 sentences)\n2. **Key Decisions Made**\n   - Decision\n   - Who decided\n   - Rationale\n3. **Action Items**\n   - Task\n   - Owner\n   - Deadline (if mentioned)\n4. **Open Questions**\n5. **Next Steps**\n6. **Attendee Participation**\n   - Who spoke most/least\n   - Engagement level\n```\n\n**Customer Call Analysis**\n```\nCall transcript: [support call]\n\nAnalyze:\n1. **Customer Sentiment**\n   - Overall: [Positive/Neutral/Negative]\n   - Evolution during call\n2. **Issue Summary**\n3. **Resolution Status**\n4. **Agent Performance**\n   - Communication clarity\n   - Problem-solving\n   - Empathy\n5. **Follow-up Needed**\n```\n\n## Multimodal Combinations\n\n### Image + Text Analysis\n\n```\nI'm providing an image and a description. Verify if they match:\n\nImage: [product photo]\n\nDescription: \"Red leather handbag with gold hardware, approximately 12 inches wide,  with an adjustable shoulder strap and zippered main compartment.\"\n\nVerification:\n1. Does image match description?\n2. What matches? What doesn't?\n3. What's in the image but not described?\n4. What's described but not visible?\n5. Confidence score: X/10\n```\n\n### Video Understanding (Frame-by-Frame)\n\n```python\ndef analyze_video(video_path, frame_interval=30):\n    \"\"\"\n    Extract frames and analyze video content\n    \"\"\"\n    import cv2\n    \n    cap = cv2.VideoCapture(video_path)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    frame_count = 0\n    analyses = []\n    \n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        if frame_count % frame_interval == 0:\n            # Save frame\n            frame_path = f\"/tmp/frame_{frame_count}.jpg\"\n            cv2.imwrite(frame_path, frame)\n            \n            # Analyze frame\n            timestamp = frame_count / fps\n            analysis = analyze_image(\n                frame_path,\n                f\"Describe what's happening at {timestamp:.1f} seconds\"\n            )\n            analyses.append({\"timestamp\": timestamp, \"description\": analysis})\n        \n        frame_count += 1\n    \n    cap.release()\n    \n    # Summarize video\n    summary_prompt = f\"\"\"\n    Based on these frame analyses, create a video summary:\n    \n    {analyses}\n    \n    Provide:\n    1. Overall video summary\n    2. Key scenes and their timestamps\n    3. Main narrative or progression\n    4. Notable objects/people/actions\n    \"\"\"\n    \n    return llm.generate(summary_prompt)\n```\n\n## Domain-Specific Applications\n\n### E-commerce: Product Cataloging\n\n```\nAnalyze this product image for e-commerce listing:\n\nProvide:\n1. **Product Category** (auto-detected)\n2. **Key Attributes**\n   - Color(s)\n   - Material (if determinable)\n   - Style/design elements\n   - Condition\n3. **Suggested Title** (SEO-optimized, 60 chars)\n4. **Product Description** (150-200 words)\n5. **Tags** (10-15 relevant keywords)\n6. **Similar Products** (describe what to search for)\n7. **Quality Assessment** (photo quality for listing)\n```\n\n### Real Estate: Property Analysis\n\n```\nAnalyze this property photo:\n\n1. **Room Type**: [bedroom/kitchen/bathroom/living room/etc.]\n2. **Style**: [modern/traditional/rustic/etc.]\n3. **Condition**: [excellent/good/needs work]\n4. **Features Visible**:\n   - Flooring type\n   - Lighting\n   - Fixtures/appliances\n   - Storage\n5. **Estimated Room Size**: [based on furniture scale]\n6. **Appeal Factors**: What makes this room attractive?\n7. **Improvement Suggestions**: What could enhance it?\n```\n\n### Manufacturing: Quality Control\n\n```\nInspect this product for defects:\n\nImage: [manufactured part]\n\nInspection checklist:\n1. **Visual Defects**\n   - Scratches\n   - Dents\n   - Discoloration\n   - Misalignment\n2. **Dimensional Check** (if reference provided)\n3. **Surface Finish Quality**\n4. **Assembly Correctness**\n5. **Pass/Fail**: [with confidence level]\n6. **Flagged Areas**: [specific locations of issues]\n```\n\n## Best Practices for Multimodal Prompts\n\n### 1. Image Quality Matters\n\n```\nBefore analyzing:\n1. Check image resolution (minimum 224x224)\n2. Ensure good lighting\n3. Verify image isn't corrupted\n4. Crop to relevant area if needed\n```\n\n### 2. Provide Context\n\n**Bad:**\n```\n\"What is this?\"\n[image]\n```\n\n**Good:**\n```\n\"This is an image from a home inspection. Identify any structural issues, water damage, or safety concerns. Focus on the basement area visible in the photo.\"\n[image]\n```\n\n### 3. Multiple Images for Comparison\n\n```\nCompare these before/after images:\n\nBefore: [image1]\nAfter: [image2]\n\nContext: Home renovation project\n\nAnalyze:\n1. What renovations were done?\n2. Quality of work (1-10)\n3. Value impact (increased/decreased)\n4. Design coherence\n```\n\n### 4. Structured Output\n\n```\nAnalyze this medical diagram and output in JSON:\n\n{\n  \"diagram_type\": \"...\",\n  \"labeled_parts\": [\n    {\"label\": \"...\", \"location\": \"...\", \"function\": \"...\"}\n  ],\n  \"relationships\": [\"...\"]\n}\n```\n\n## Limitations and Handling\n\n### 1. Handle Ambiguity\n\n```\nIf the image is unclear or ambiguous:\n1. State your uncertainty\n2. Provide your best interpretation\n3. List alternative possibilities\n4. Suggest what additional info would help\n```\n\n### 2. Privacy and Sensitive Content\n\n```\nBefore analyzing:\n- Check for personal information (faces, IDs, documents)\n- Blur/redact sensitive data\n- Follow data protection regulations (GDPR, HIPAA)\n```\n\n### 3. Model Limitations\n\n- **Text Density**: Dense text may be partially missed\n- **Small Objects**: Tiny details may not be detected\n- **Artistic Content**: Subjective interpretation\n- **Cultural Context**: May miss cultural nuances",
        keyPoints: [
          "Vision models can analyze images, extract text, compare visuals, and understand scenes",
          "Provide clear context about what you want to extract or understand from images",
          "Audio transcription + LLM analysis enables meeting summaries and call analytics",
          "Multimodal combinations (image + text) enable verification and richer analysis",
          "Always specify output format (JSON, bullet points, etc.) for structured data extraction"
        ],
        handsOnExercise: "Build a multimodal application: analyze product images to generate listings (title, description, tags) and categorize them automatically."
      },
      {
        id: "lesson-5-4",
        title: "Real-World Business Case Studies",
        duration: "3 hours",
        content: "# Business Case Studies in Prompt Engineering\n\nLearn from real-world implementations across industries.\n\n## Case Study 1: E-commerce Customer Support Automation\n\n### Challenge\nOnline retailer receiving 50,000+ support tickets/month. Average response time: 24 hours. Customer satisfaction: 72%.\n\n### Solution Architecture\n\n```python\nclass CustomerSupportAI:\n    def __init__(self):\n        self.rag_system = RAGSystem(knowledge_base='support_docs')\n        self.classifier = TicketClassifier()\n        self.sentiment_analyzer = SentimentAnalyzer()\n    \n    def handle_ticket(self, ticket):\n        # Classify urgency and category\n        classification = self.classifier.classify(ticket)\n        sentiment = self.sentiment_analyzer.analyze(ticket)\n        \n        # Route based on complexity\n        if classification['complexity'] == 'high' or sentiment == 'very_negative':\n            return self.escalate_to_human(ticket)\n        \n        # Retrieve relevant knowledge\n        context = self.rag_system.retrieve(ticket['question'])\n        \n        # Generate response\n        response = self.generate_response(ticket, context, sentiment)\n        \n        # Confidence check\n        if response['confidence'] < 0.8:\n            return self.escalate_to_human(ticket, draft_response=response)\n        \n        return response\n```\n\n### Key Prompts\n\n**Ticket Classification:**\n```\nClassify this support ticket:\n\nTicket: \"{ticket_text}\"\n\nProvide JSON:\n{\n  \"category\": \"[shipping/returns/product_issue/account/billing]\",\n  \"urgency\": \"[low/medium/high/critical]\",\n  \"complexity\": \"[simple/moderate/complex]\",\n  \"sentiment\": \"[positive/neutral/negative/very_negative]\",\n  \"requires_human\": boolean,\n  \"reason\": \"[if requires_human, explain why]\"\n}\n```\n\n**Response Generation:**\n```\nYou are a helpful customer support agent for [Company].\n\nCustomer question: \"{question}\"\nCustomer sentiment: {sentiment}\n\nKnowledge base context:\n{retrieved_docs}\n\nGuidelines:\n1. Be empathetic, especially if sentiment is negative\n2. Provide clear, actionable steps\n3. Include relevant links from knowledge base\n4. If refund/return, follow policy: {policy}\n5. End with offer to help further\n\nGenerate response:\n```\n\n### Results\n- **Response time**: 24hrs ‚Üí 5 minutes (95% reduction)\n- **Ticket volume handled by AI**: 68%\n- **Customer satisfaction**: 72% ‚Üí 87% (21% increase)\n- **Cost savings**: $450K annually\n- **Agent productivity**: +40% (focus on complex issues)\n\n### Key Lessons\n1. **Confidence thresholds** prevent bad automated responses\n2. **Sentiment analysis** triggers human escalation for upset customers\n3. **RAG** ensures responses based on current policies\n4. **Human-in-the-loop** for edge cases maintains quality\n\n## Case Study 2: Legal Contract Analysis\n\n### Challenge\nLaw firm reviewing 100+ contracts/week. Each review takes 2-4 hours. Looking to accelerate initial review.\n\n### Solution\n\n```python\ndef analyze_contract(contract_text, contract_type):\n    prompt = f\"\"\"\n    Analyze this {contract_type} contract:\n    \n    {contract_text}\n    \n    Provide structured analysis:\n    \n    ## 1. Key Terms\n    - Parties involved\n    - Effective date\n    - Term/duration\n    - Termination conditions\n    - Payment terms\n    \n    ## 2. Obligations\n    ### Party A obligations:\n    [list]\n    \n    ### Party B obligations:\n    [list]\n    \n    ## 3. Risk Factors\n    [Flag unusual or risky clauses]\n    \n    ## 4. Missing Standard Clauses\n    [Note any expected clauses that are absent]\n    \n    ## 5. Ambiguous Language\n    [Highlight vague or unclear terms]\n    \n    ## 6. Non-Standard Terms\n    [Flag anything unusual for this contract type]\n    \n    ## 7. Recommendation\n    [Red flags / Yellow flags / Green - standard]\n    \"\"\"\n    \n    return llm.generate(prompt)\n```\n\n### Multi-Step Workflow\n\n```\nStep 1: Contract Classification\n‚Üí Type: NDA / Employment / Vendor / Partnership / etc.\n\nStep 2: Clause Extraction\n‚Üí Extract all key clauses with locations\n\nStep 3: Compliance Check\n‚Üí Verify required clauses present\n\nStep 4: Risk Assessment\n‚Üí Flag high-risk terms\n\nStep 5: Comparison\n‚Üí Compare to standard template\n\nStep 6: Summary Report\n‚Üí Executive summary for lawyer review\n```\n\n### Results\n- **Initial review time**: 2-4 hours ‚Üí 15 minutes\n- **Accuracy**: 94% (lawyer verification)\n- **Throughput**: +250% more contracts reviewed\n- **Cost per review**: -85%\n- **Lawyer time**: Refocused on negotiations and strategy\n\n### Key Lessons\n1. **AI assists, doesn't replace** - lawyers review AI analysis\n2. **Domain-specific** prompts with legal terminology\n3. **Structured output** makes review efficient\n4. **Comparison to templates** catches deviations\n\n## Case Study 3: Content Marketing at Scale\n\n### Challenge\nB2B SaaS company needs 200+ blog posts, social content, and email campaigns monthly.\n\n### Solution Framework\n\n```python\nclass ContentPipeline:\n    def generate_content_cluster(self, main_topic):\n        # 1. Research & Ideation\n        ideas = self.generate_content_ideas(main_topic)\n        \n        # 2. Create Content Calendar\n        calendar = self.plan_content_schedule(ideas)\n        \n        # 3. Generate Long-form Content\n        articles = []\n        for idea in calendar['blog_posts']:\n            outline = self.create_outline(idea)\n            article = self.write_article(outline)\n            edited = self.edit_content(article)\n            articles.append(edited)\n        \n        # 4. Derive Short-form Content\n        social = self.extract_social_posts(articles)\n        \n        # 5. Create Email Campaigns\n        emails = self.create_email_series(articles)\n        \n        return {\n            'articles': articles,\n            'social': social,\n            'emails': emails\n        }\n```\n\n### Key Prompts\n\n**Content Ideation:**\n```\nGenerate 20 blog post ideas for:\n\nTopic: {main_topic}\nAudience: {target_persona}\nGoal: {business_goal}\nCompetitive keywords: {keywords}\n\nFor each idea provide:\n1. Title (compelling, SEO-optimized)\n2. One-line summary\n3. Target keyword\n4. Content angle (why unique)\n5. Estimated traffic potential (H/M/L)\n\nPrioritize ideas by:\n- Search volume\n- Competitive difficulty\n- Relevance to product\n```\n\n**Long-form Article Generation:**\n```\nWrite a comprehensive blog post:\n\nTitle: \"{title}\"\nTarget audience: \"{persona}\"\nPrimary keyword: \"{keyword}\"\nTarget length: 2000-2500 words\n\nRequirements:\n- Start with compelling hook\n- Include expert insights\n- Add 5-7 relevant examples\n- Use headers for scannability (H2, H3)\n- Include actionable takeaways\n- SEO: Natural keyword usage (1-2%)\n- Tone: Professional but conversational\n- End with strong CTA\n\nOutline:\n{generated_outline}\n\nWrite the complete article:\n```\n\n**Content Repurposing:**\n```\nFrom this blog post, create:\n\n1. **Twitter Thread** (10 tweets)\n   - Hook tweet\n   - 7-8 value tweets\n   - CTA tweet\n\n2. **LinkedIn Post** (1300 chars)\n   - Professional tone\n   - Key insights\n   - Link to full article\n\n3. **Email Newsletter** (500 words)\n   - Catchy subject line\n   - Preview of insights\n   - Link to read more\n\n4. **Instagram Carousel** (10 slides)\n   - Visual quotes\n   - Key statistics\n   - Swipe-through format\n\nBlog post:\n{article_text}\n```\n\n### Results\n- **Content output**: 50 ‚Üí 200+ pieces/month\n- **Cost per piece**: 90% reduction\n- **Organic traffic**: +180% in 6 months\n- **Lead generation**: +140%\n- **Writer time**: Shifted from writing to strategy and editing\n\n### Key Lessons\n1. **Human oversight essential** - writers edit AI drafts\n2. **Templates and style guides** ensure consistency\n3. **Repurposing multiplies** value of each piece\n4. **SEO optimization** built into prompts\n\n## Case Study 4: Financial Analysis & Reporting\n\n### Challenge\nInvestment firm analyzing 1000+ companies quarterly. Manual report creation taking 40 hours/analyst.\n\n### Solution\n\n```python\ndef generate_company_analysis(ticker, financial_data, news):\n    prompt = f\"\"\"\n    Financial Analysis Report for {ticker}\n    \n    Financial Data (Last 4 quarters):\n    {financial_data}\n    \n    Recent News & Events:\n    {news}\n    \n    Generate comprehensive analysis:\n    \n    ## Executive Summary\n    [3-4 sentences on company performance]\n    \n    ## Financial Health\n    ### Revenue Analysis\n    - Growth rate (QoQ, YoY)\n    - Revenue trends\n    - Segment performance\n    \n    ### Profitability\n    - Gross margin trends\n    - Operating margin\n    - Net income analysis\n    \n    ### Balance Sheet Strength\n    - Debt levels\n    - Cash position\n    - Liquidity ratios\n    \n    ## Key Metrics\n    - P/E ratio vs industry\n    - EPS trend\n    - Cash flow analysis\n    \n    ## Risks & Opportunities\n    ### Risks:\n    [List top 3]\n    \n    ### Opportunities:\n    [List top 3]\n    \n    ## Investment Thesis\n    [Bull case / Bear case]\n    \n    ## Recommendation\n    [Buy/Hold/Sell with rationale]\n    \"\"\"\n    \n    return llm.generate(prompt)\n```\n\n### Results\n- **Report generation**: 40 hours ‚Üí 2 hours\n- **Coverage**: 200 ‚Üí 1000+ companies\n- **Consistency**: Standardized format\n- **Analyst productivity**: +95%\n- **ROI**: $2.3M annually\n\n## Case Study 5: Healthcare: Clinical Note Summarization\n\n### Challenge  \nDoctors spending 2+ hours daily on documentation. Note review by other providers time-consuming.\n\n### Solution\n\n```\nSummarize this clinical note:\n\n{clinical_note}\n\nProvide:\n\n## Chief Complaint\n[One line]\n\n## History of Present Illness\n[2-3 sentences]\n\n## Key Findings\n- Physical exam findings\n- Lab results (if any)\n- Imaging results (if any)\n\n## Assessment\n[Diagnosis/diagnoses]\n\n## Plan\n1. [Treatment/medication]\n2. [Follow-up]\n3. [Patient education]\n\n## Red Flags\n[Any concerning findings]\n\nMedical jargon preserved. Format for EHR integration.\n```\n\n### Results\n- **Documentation time**: -40%\n- **Note clarity**: +60% (peer review)\n- **Physician satisfaction**: Significantly improved\n- **Regulatory compliance**: Maintained\n\n## Common Success Patterns\n\n### 1. Human-in-the-Loop\nAI generates, humans verify/edit\n\n### 2. Confidence Scoring\nEscalate low-confidence outputs\n\n### 3. Domain-Specific Prompts\nInclude industry terminology and context\n\n### 4. Iterative Refinement\nStart simple, improve based on feedback\n\n### 5. Structured Outputs\nJSON/XML for system integration\n\n### 6. A/B Testing\nContinuously optimize prompts\n\n## Implementation Framework\n\n```\nPhase 1: Pilot (Month 1-2)\n- Select narrow use case\n- Manual prompt testing\n- 10-20% of volume\n\nPhase 2: Refinement (Month 3-4)\n- Gather feedback\n- Optimize prompts\n- Expand to 40-50%\n\nPhase 3: Scale (Month 5-6)\n- Automate workflows\n- Monitor quality\n- 70-80% coverage\n\nPhase 4: Optimize (Ongoing)\n- A/B test prompts\n- Fine-tune models\n- Continuous improvement\n```\n\n## ROI Calculation\n\n```\nTime Saved:\n- Hours per task (before vs after)\n- Number of tasks per month\n- Employee hourly rate\n\nQuality Improvement:\n- Error reduction\n- Customer satisfaction increase\n- Revenue impact\n\nCost:\n- API costs\n- Development time\n- Maintenance\n\nROI = (Benefits - Costs) / Costs √ó 100%\n```",
        keyPoints: [
          "Human-in-the-loop maintains quality while scaling with AI",
          "Confidence thresholds and escalation prevent poor outputs",
          "Domain-specific prompts with industry terminology improve accuracy",
          "Structured outputs (JSON) enable system integration",
          "Pilot ‚Üí Refine ‚Üí Scale approach minimizes risk"
        ],
        handsOnExercise: "Design a prompt engineering solution for a specific business problem in your domain. Define the workflow, key prompts, and success metrics."
      }
    ]
  },
  {
    id: "module-6",
    title: "Production Deployment",
    description: "Deploy prompt engineering systems with security, optimization, and monitoring.",
    duration: "4-6 hours",
    lessons: [
      {
        id: "lesson-6-1",
        title: "Prompt Security",
        duration: "2 hours",
        content: "# Prompt Security and Robustness\n\n## Prompt Injection Attacks\n\n**What is it?** Malicious users attempt to override your instructions.\n\n### Defense Strategies\n\n1. **Instruction Hierarchy:** Mark core instructions as highest priority\n2. **Input Sandboxing:** Treat user input as DATA, not INSTRUCTIONS\n3. **Output Constraints:** Verify responses before returning\n\n## Input Validation\n\nCheck for:\n- Injection keywords (\"ignore previous\", \"system prompt\")\n- Length limits\n- Special characters\n\n## Sensitive Data\n\n- Mask emails, phone numbers, credit cards\n- Never expose system prompts\n- Use rate limiting to prevent abuse",
        keyPoints: [
          "Prompt injection is a real security threat",
          "Use instruction hierarchy and sandboxing",
          "Mask sensitive data before processing",
          "Implement rate limiting"
        ],
        handsOnExercise: "Try to break your own prompt with injection attacks. Then implement defenses and test again."
      },
      {
        id: "lesson-6-2",
        title: "Cost Optimization and Performance",
        duration: "4 hours",
        content: "# Cost Optimization and Performance\n\nOptimize your LLM applications for both cost and speed.\n\n## Understanding API Costs\n\n### Token-Based Pricing\n\nMost LLM APIs charge per token (input + output).\n\n**OpenAI Pricing (2025):**\n- **GPT-4 Turbo**: $10/1M input tokens, $30/1M output tokens\n- **GPT-4**: $30/1M input tokens, $60/1M output tokens\n- **GPT-3.5 Turbo**: $0.50/1M input tokens, $1.50/1M output tokens\n- **GPT-4 Mini**: $0.15/1M input tokens, $0.60/1M output tokens\n\n**Claude Pricing (2025):**\n- **Claude 3.5 Sonnet**: $3/1M input, $15/1M output\n- **Claude 3 Opus**: $15/1M input, $75/1M output\n- **Claude 3 Haiku**: $0.25/1M input, $1.25/1M output\n\n### Cost Calculation\n\n```python\ndef calculate_cost(input_tokens, output_tokens, model='gpt-4-turbo'):\n    pricing = {\n        'gpt-4-turbo': {'input': 10/1_000_000, 'output': 30/1_000_000},\n        'gpt-4': {'input': 30/1_000_000, 'output': 60/1_000_000},\n        'gpt-3.5-turbo': {'input': 0.5/1_000_000, 'output': 1.5/1_000_000},\n        'claude-3.5-sonnet': {'input': 3/1_000_000, 'output': 15/1_000_000},\n    }\n    \n    model_pricing = pricing[model]\n    input_cost = input_tokens * model_pricing['input']\n    output_cost = output_tokens * model_pricing['output']\n    \n    return input_cost + output_cost\n\n# Example: 1000 requests/day, 500 input + 300 output tokens each\ndaily_tokens = 1000 * (500 + 300)\nmonthly_cost = calculate_cost(1000 * 500, 1000 * 300, 'gpt-4-turbo') * 30\nprint(f\"Monthly cost: ${monthly_cost:.2f}\")  # $240/month\n```\n\n## Cost Optimization Strategies\n\n### 1. Model Selection\n\nChoose the right model for the task.\n\n```python\nclass ModelRouter:\n    def __init__(self):\n        self.models = {\n            'simple': 'gpt-3.5-turbo',      # $0.0015/1k tokens\n            'moderate': 'gpt-4-mini',        # $0.0075/1k tokens  \n            'complex': 'gpt-4-turbo',        # $0.040/1k tokens\n            'critical': 'gpt-4'              # $0.090/1k tokens\n        }\n    \n    def classify_task(self, task):\n        \"\"\"\n        Route based on complexity\n        \"\"\"\n        complexity_prompt = f\"\"\"\n        Classify task complexity:\n        Task: {task}\n        \n        Classification: [simple/moderate/complex/critical]\n        \n        Simple: Basic Q&A, classification, simple extraction\n        Moderate: Multi-step reasoning, code generation\n        Complex: Advanced reasoning, creative tasks\n        Critical: High-stakes decisions, complex analysis\n        \"\"\"\n        \n        # Use cheap model for classification\n        complexity = llm.generate(complexity_prompt, model='gpt-3.5-turbo')\n        return self.models[complexity.lower()]\n    \n    def execute(self, task):\n        model = self.classify_task(task)\n        return llm.generate(task, model=model)\n```\n\n**Cost Savings Example:**\n- 70% simple tasks ‚Üí GPT-3.5 Turbo\n- 20% moderate tasks ‚Üí GPT-4 Mini\n- 10% complex tasks ‚Üí GPT-4 Turbo\n\n**Before (all GPT-4):** $900/month\n**After (routed):** $180/month (80% savings)\n\n### 2. Prompt Compression\n\nReduce token count while maintaining meaning.\n\n**Before:**\n```\nYou are a helpful AI assistant designed to answer questions about our products. Your goal is to provide accurate, friendly, and detailed responses to customer inquiries. Please be professional and courteous at all times.\n\nCustomer question: What's the warranty period?\n\nPlease provide a comprehensive answer including all relevant details.\n```\n**Tokens:** 58\n\n**After:**\n```\nAnswer this product question accurately:\nQ: What's the warranty period?\n```\n**Tokens:** 12 (79% reduction)\n\n### 3. Response Length Control\n\nLimit output tokens with `max_tokens` parameter.\n\n```python\ndef optimized_generate(prompt, max_tokens=None):\n    # Estimate needed length\n    if 'short' in prompt or 'brief' in prompt:\n        max_tokens = 100\n    elif 'detailed' in prompt or 'comprehensive' in prompt:\n        max_tokens = 500\n    else:\n        max_tokens = 200\n    \n    response = openai.chat.completions.create(\n        model='gpt-4-turbo',\n        messages=[{'role': 'user', 'content': prompt}],\n        max_tokens=max_tokens  # Prevent runaway generation\n    )\n    \n    return response.choices[0].message.content\n```\n\n### 4. Caching\n\nCache frequent queries and responses.\n\n```python\nimport redis\nimport hashlib\nimport json\n\nclass ResponseCache:\n    def __init__(self):\n        self.redis = redis.Redis(host='localhost', port=6379, db=0)\n        self.ttl = 3600 * 24  # 24 hours\n    \n    def get_cache_key(self, prompt, model):\n        content = f\"{model}:{prompt}\"\n        return hashlib.md5(content.encode()).hexdigest()\n    \n    def get(self, prompt, model):\n        key = self.get_cache_key(prompt, model)\n        cached = self.redis.get(key)\n        \n        if cached:\n            return json.loads(cached)\n        return None\n    \n    def set(self, prompt, model, response):\n        key = self.get_cache_key(prompt, model)\n        self.redis.setex(key, self.ttl, json.dumps(response))\n    \n    def generate(self, prompt, model='gpt-4-turbo'):\n        # Check cache\n        cached = self.get(prompt, model)\n        if cached:\n            return cached  # No API call = $0 cost\n        \n        # Generate and cache\n        response = llm.generate(prompt, model=model)\n        self.set(prompt, model, response)\n        \n        return response\n```\n\n**Cost Savings:**\n- Cache hit rate: 40%\n- Monthly savings: 40% of API costs\n\n### 5. Batch Processing\n\nProcess multiple requests in parallel.\n\n```python\nimport asyncio\n\nasync def batch_generate(prompts, model='gpt-4-turbo'):\n    \"\"\"\n    Process multiple prompts concurrently\n    \"\"\"\n    async def generate_one(prompt):\n        response = await openai.chat.completions.create(\n            model=model,\n            messages=[{'role': 'user', 'content': prompt}]\n        )\n        return response.choices[0].message.content\n    \n    tasks = [generate_one(p) for p in prompts]\n    results = await asyncio.gather(*tasks)\n    \n    return results\n\n# Process 100 prompts in ~10 seconds instead of 100+ seconds\nprompts = [...] # 100 prompts\nresults = asyncio.run(batch_generate(prompts))\n```\n\n### 6. Prompt Preprocessing\n\nReduce redundant information.\n\n```python\ndef preprocess_document(doc, max_length=4000):\n    \"\"\"\n    Compress document before using in prompt\n    \"\"\"\n    # Extract key sentences\n    summary = extractive_summarize(doc, ratio=0.3)\n    \n    # Remove boilerplate\n    summary = remove_common_phrases(summary)\n    \n    # Truncate if still too long\n    tokens = tokenize(summary)\n    if len(tokens) > max_length:\n        summary = detokenize(tokens[:max_length])\n    \n    return summary\n```\n\n## Performance Optimization\n\n### 1. Latency Reduction\n\n**Streaming Responses**\n\n```python\ndef stream_response(prompt):\n    \"\"\"\n    Stream tokens as they're generated\n    \"\"\"\n    for chunk in openai.chat.completions.create(\n        model='gpt-4-turbo',\n        messages=[{'role': 'user', 'content': prompt}],\n        stream=True\n    ):\n        if chunk.choices[0].delta.content:\n            yield chunk.choices[0].delta.content\n\n# User sees first tokens in ~500ms instead of waiting 5+ seconds\nfor token in stream_response(\"Write a blog post...\"):\n    print(token, end='', flush=True)\n```\n\n**Benefits:**\n- Perceived latency: 5000ms ‚Üí 500ms\n- User experience: Much better\n- Actual generation time: Same\n\n### 2. Parallel Execution\n\nBreak tasks into independent subtasks.\n\n```python\nasync def parallel_analysis(document):\n    \"\"\"\n    Run multiple analyses in parallel\n    \"\"\"\n    tasks = [\n        analyze_sentiment(document),\n        extract_keywords(document),\n        identify_entities(document),\n        classify_topic(document)\n    ]\n    \n    results = await asyncio.gather(*tasks)\n    \n    return {\n        'sentiment': results[0],\n        'keywords': results[1],\n        'entities': results[2],\n        'topic': results[3]\n    }\n\n# 4 analyses in ~2 seconds instead of 8 seconds\nanalysis = asyncio.run(parallel_analysis(document))\n```\n\n### 3. Model-Specific Optimizations\n\n**Use Native JSON Mode**\n\n```python\n# Instead of parsing text output\nresponse = openai.chat.completions.create(\n    model='gpt-4-turbo',\n    messages=[{'role': 'user', 'content': 'Extract user data from: ...'}],\n    response_format={'type': 'json_object'}\n)\n\ndata = json.loads(response.choices[0].message.content)\n```\n\n**Use Function Calling**\n\n```python\ntools = [{\n    'type': 'function',\n    'function': {\n        'name': 'extract_contact_info',\n        'description': 'Extract contact information',\n        'parameters': {\n            'type': 'object',\n            'properties': {\n                'name': {'type': 'string'},\n                'email': {'type': 'string'},\n                'phone': {'type': 'string'}\n            },\n            'required': ['name']\n        }\n    }\n}]\n\nresponse = openai.chat.completions.create(\n    model='gpt-4-turbo',\n    messages=[{'role': 'user', 'content': text}],\n    tools=tools,\n    tool_choice={'type': 'function', 'function': {'name': 'extract_contact_info'}}\n)\n```\n\n## Monitoring Costs\n\n### Token Tracking\n\n```python\nclass CostTracker:\n    def __init__(self):\n        self.usage = {\n            'input_tokens': 0,\n            'output_tokens': 0,\n            'total_cost': 0.0,\n            'requests': 0\n        }\n    \n    def log_request(self, response, model='gpt-4-turbo'):\n        usage = response.usage\n        cost = calculate_cost(usage.prompt_tokens, usage.completion_tokens, model)\n        \n        self.usage['input_tokens'] += usage.prompt_tokens\n        self.usage['output_tokens'] += usage.completion_tokens\n        self.usage['total_cost'] += cost\n        self.usage['requests'] += 1\n    \n    def get_report(self):\n        avg_cost = self.usage['total_cost'] / max(self.usage['requests'], 1)\n        \n        return {\n            'total_requests': self.usage['requests'],\n            'total_tokens': self.usage['input_tokens'] + self.usage['output_tokens'],\n            'total_cost': f\"${self.usage['total_cost']:.4f}\",\n            'avg_cost_per_request': f\"${avg_cost:.4f}\",\n            'projected_monthly': f\"${self.usage['total_cost'] * 30:.2f}\"\n        }\n\ntracker = CostTracker()\nresponse = openai.chat.completions.create(...)\ntracker.log_request(response)\n\nprint(tracker.get_report())\n```\n\n### Budget Alerts\n\n```python\nclass BudgetMonitor:\n    def __init__(self, daily_budget=50.0):\n        self.daily_budget = daily_budget\n        self.daily_spend = 0.0\n    \n    def check_budget(self, estimated_cost):\n        if self.daily_spend + estimated_cost > self.daily_budget:\n            raise BudgetExceededError(\n                f\"Request would exceed daily budget of ${self.daily_budget}\"\n            )\n    \n    def track_request(self, cost):\n        self.daily_spend += cost\n        \n        if self.daily_spend >= self.daily_budget * 0.9:\n            send_alert(f\"90% of daily budget used: ${self.daily_spend:.2f}\")\n```\n\n## Best Practices Summary\n\n### Cost Optimization Checklist\n\n‚úÖ **Model Selection**\n- Use cheaper models for simple tasks\n- Route by complexity\n- Upgrade only when necessary\n\n‚úÖ **Prompt Optimization**\n- Remove unnecessary verbosity\n- Compress system prompts\n- Limit response length\n\n‚úÖ **Caching Strategy**\n- Cache frequent queries\n- Set appropriate TTL\n- Monitor hit rate\n\n‚úÖ **Batch Processing**\n- Group similar requests\n- Process in parallel\n- Use async/await\n\n‚úÖ **Monitoring**\n- Track token usage\n- Monitor costs daily\n- Set budget alerts\n\n### Performance Optimization Checklist\n\n‚úÖ **Streaming**\n- Use for user-facing applications\n- Reduce perceived latency\n\n‚úÖ **Parallel Execution**\n- Break independent tasks\n- Use asyncio/threading\n\n‚úÖ **Request Optimization**\n- Minimize input tokens\n- Use native features (JSON mode)\n- Leverage function calling\n\n## Cost-Performance Trade-offs\n\n```python\n# Scenario 1: Cost-optimized (slower, cheaper)\ncost_optimized = {\n    'model': 'gpt-3.5-turbo',\n    'caching': True,\n    'batch_size': 100,\n    'streaming': False\n}\n\n# Scenario 2: Performance-optimized (faster, expensive)\nperformance_optimized = {\n    'model': 'gpt-4-turbo',\n    'caching': True,\n    'batch_size': 10,\n    'streaming': True,\n    'parallel': True\n}\n\n# Scenario 3: Balanced\nbalanced = {\n    'model': 'gpt-4-mini',  # Good quality, reasonable cost\n    'caching': True,\n    'batch_size': 50,\n    'streaming': True,\n    'parallel': True\n}\n```\n\n## Real-World Example\n\n**Before Optimization:**\n- Model: GPT-4 for everything\n- No caching\n- Sequential processing\n- No token limits\n- **Cost:** $3,500/month\n- **Avg latency:** 4.2 seconds\n\n**After Optimization:**\n- Models: 70% GPT-3.5, 20% GPT-4 Mini, 10% GPT-4\n- Caching: 35% hit rate\n- Parallel processing\n- Max tokens: 300 (avg)\n- **Cost:** $680/month (81% reduction)\n- **Avg latency:** 1.8 seconds (57% faster)",
        keyPoints: [
          "Model selection is the biggest cost lever - use cheaper models for simpler tasks",
          "Caching can reduce costs by 30-40% for repetitive queries",
          "Streaming responses dramatically improve perceived performance",
          "Track token usage and costs daily to identify optimization opportunities",
          "Balance cost vs performance based on use case requirements"
        ],
        handsOnExercise: "Implement a cost tracking system for your application. Measure current costs, then apply 3 optimization strategies and compare results."
      },
      {
        id: "lesson-6-3",
        title: "Monitoring, Testing, and A/B Experiments",
        duration: "4 hours",
        content: "# Monitoring, Testing, and A/B Experiments\n\nEnsure quality and continuously improve your prompt-based systems.\n\n## Production Monitoring\n\n### Key Metrics to Track\n\n**1. Performance Metrics**\n```python\nclass PromptMetrics:\n    def track_request(self, request_id, prompt, response, metadata):\n        metrics = {\n            'request_id': request_id,\n            'timestamp': datetime.utcnow(),\n            \n            # Latency\n            'latency_ms': metadata['duration'] * 1000,\n            'time_to_first_token': metadata.get('ttft', None),\n            \n            # Token usage\n            'input_tokens': metadata['prompt_tokens'],\n            'output_tokens': metadata['completion_tokens'],\n            'total_tokens': metadata['total_tokens'],\n            \n            # Cost\n            'cost': calculate_cost(metadata),\n            \n            # Model\n            'model': metadata['model'],\n            \n            # Quality indicators\n            'output_length': len(response),\n            'stop_reason': metadata.get('finish_reason'),\n        }\n        \n        self.log_to_database(metrics)\n        self.update_dashboards(metrics)\n```\n\n**2. Quality Metrics**\n```python\ndef track_quality(request_id, response, ground_truth=None):\n    metrics = {\n        'request_id': request_id,\n        \n        # User feedback\n        'user_rating': None,  # Populated later\n        'user_feedback': None,\n        \n        # Automated checks\n        'contains_refusal': check_refusal(response),\n        'hallucination_score': detect_hallucination(response),\n        'toxicity_score': check_toxicity(response),\n        'output_valid': validate_format(response),\n    }\n    \n    # If ground truth available\n    if ground_truth:\n        metrics['accuracy'] = calculate_accuracy(response, ground_truth)\n        metrics['bleu_score'] = bleu(response, ground_truth)\n    \n    return metrics\n```\n\n**3. Business Metrics**\n```python\nclass BusinessMetrics:\n    def track_outcome(self, request_id, outcome):\n        \"\"\"\n        Track business impact of AI responses\n        \"\"\"\n        metrics = {\n            'request_id': request_id,\n            \n            # Conversion/success metrics\n            'task_completed': outcome.get('completed', False),\n            'user_satisfied': outcome.get('satisfied', None),\n            'follow_up_needed': outcome.get('escalated', False),\n            \n            # Efficiency\n            'time_saved': outcome.get('time_saved_minutes', 0),\n            'manual_work_avoided': outcome.get('automation_success', False),\n        }\n        \n        self.log_business_metrics(metrics)\n```\n\n### Logging and Observability\n\n**Structured Logging**\n```python\nimport logging\nimport json\n\nclass StructuredLogger:\n    def __init__(self):\n        self.logger = logging.getLogger('prompt_system')\n    \n    def log_request(self, level, event, **kwargs):\n        log_entry = {\n            'timestamp': datetime.utcnow().isoformat(),\n            'event': event,\n            'level': level,\n            **kwargs\n        }\n        \n        self.logger.log(\n            getattr(logging, level.upper()),\n            json.dumps(log_entry)\n        )\n\nlogger = StructuredLogger()\n\n# Log request\nlogger.log_request('info', 'prompt_request', \n    request_id='req_123',\n    user_id='user_456',\n    prompt='What is...?',\n    model='gpt-4-turbo'\n)\n\n# Log response\nlogger.log_request('info', 'prompt_response',\n    request_id='req_123',\n    latency_ms=1234,\n    tokens=567,\n    cost=0.012\n)\n\n# Log error\nlogger.log_request('error', 'prompt_failure',\n    request_id='req_123',\n    error='Rate limit exceeded',\n    retry_after=60\n)\n```\n\n**Distributed Tracing**\n```python\nfrom opentelemetry import trace\nfrom opentelemetry.instrumentation.requests import RequestsInstrumentor\n\ntracer = trace.get_tracer(__name__)\n\ndef generate_with_tracing(prompt):\n    with tracer.start_as_current_span(\"llm_generation\") as span:\n        span.set_attribute(\"prompt.model\", \"gpt-4-turbo\")\n        span.set_attribute(\"prompt.length\", len(prompt))\n        \n        try:\n            response = openai.chat.completions.create(\n                model='gpt-4-turbo',\n                messages=[{'role': 'user', 'content': prompt}]\n            )\n            \n            span.set_attribute(\"response.tokens\", response.usage.total_tokens)\n            span.set_attribute(\"response.cost\", calculate_cost(response))\n            \n            return response.choices[0].message.content\n        \n        except Exception as e:\n            span.set_attribute(\"error\", True)\n            span.set_attribute(\"error.message\", str(e))\n            raise\n```\n\n## A/B Testing Prompts\n\n### Experiment Framework\n\n```python\nimport random\nfrom enum import Enum\n\nclass VariantResult(Enum):\n    A = 'variant_a'\n    B = 'variant_b'\n\nclass ABTest:\n    def __init__(self, name, variant_a, variant_b, split=0.5):\n        self.name = name\n        self.variant_a = variant_a\n        self.variant_b = variant_b\n        self.split = split\n        self.results = {'a': [], 'b': []}\n    \n    def get_variant(self, user_id):\n        \"\"\"\n        Deterministic variant assignment based on user_id\n        \"\"\"\n        hash_value = hash(f\"{self.name}_{user_id}\")\n        return VariantResult.A if (hash_value % 100) < (self.split * 100) else VariantResult.B\n    \n    def run(self, user_id, input_data):\n        variant = self.get_variant(user_id)\n        \n        if variant == VariantResult.A:\n            result = self.variant_a(input_data)\n            self.log_result('a', result)\n        else:\n            result = self.variant_b(input_data)\n            self.log_result('b', result)\n        \n        return result, variant\n    \n    def log_result(self, variant, result):\n        self.results[variant].append({\n            'timestamp': datetime.utcnow(),\n            'latency': result.get('latency'),\n            'cost': result.get('cost'),\n            'quality': result.get('quality'),\n            'user_feedback': result.get('feedback')\n        })\n    \n    def get_statistics(self):\n        import numpy as np\n        \n        a_metrics = [r['quality'] for r in self.results['a'] if r.get('quality')]\n        b_metrics = [r['quality'] for r in self.results['b'] if r.get('quality')]\n        \n        return {\n            'variant_a': {\n                'n': len(a_metrics),\n                'mean_quality': np.mean(a_metrics) if a_metrics else 0,\n                'std': np.std(a_metrics) if a_metrics else 0\n            },\n            'variant_b': {\n                'n': len(b_metrics),\n                'mean_quality': np.mean(b_metrics) if b_metrics else 0,\n                'std': np.std(b_metrics) if b_metrics else 0\n            }\n        }\n```\n\n### Example: Testing Two Prompts\n\n```python\ndef prompt_v1(question):\n    prompt = f\"Answer this question: {question}\"\n    return generate(prompt)\n\ndef prompt_v2(question):\n    prompt = f\"\"\"\n    Answer this question concisely and accurately.\n    \n    Question: {question}\n    \n    Provide:\n    1. Direct answer\n    2. Brief explanation\n    \"\"\"\n    return generate(prompt)\n\n# Create A/B test\ntest = ABTest(\n    name='prompt_structure_test',\n    variant_a=prompt_v1,\n    variant_b=prompt_v2,\n    split=0.5  # 50/50 split\n)\n\n# Run for users\nfor user_id in user_ids:\n    question = get_user_question(user_id)\n    response, variant = test.run(user_id, question)\n    \n    # Collect user feedback\n    feedback = get_user_feedback(user_id)\n    test.log_result(variant.value[8:], {'quality': feedback})\n\n# Analyze results\nstats = test.get_statistics()\nprint(f\"Variant A: {stats['variant_a']['mean_quality']:.2f}\")\nprint(f\"Variant B: {stats['variant_b']['mean_quality']:.2f}\")\n```\n\n### Statistical Significance\n\n```python\nfrom scipy import stats\n\ndef test_significance(test):\n    \"\"\"\n    Perform t-test to determine if difference is significant\n    \"\"\"\n    a_scores = [r['quality'] for r in test.results['a'] if r.get('quality')]\n    b_scores = [r['quality'] for r in test.results['b'] if r.get('quality')]\n    \n    if len(a_scores) < 30 or len(b_scores) < 30:\n        return {\"error\": \"Need at least 30 samples per variant\"}\n    \n    # Two-sample t-test\n    t_stat, p_value = stats.ttest_ind(a_scores, b_scores)\n    \n    # Effect size (Cohen's d)\n    mean_a = np.mean(a_scores)\n    mean_b = np.mean(b_scores)\n    std_pooled = np.sqrt((np.std(a_scores)**2 + np.std(b_scores)**2) / 2)\n    cohens_d = (mean_b - mean_a) / std_pooled\n    \n    return {\n        'p_value': p_value,\n        'significant': p_value < 0.05,\n        'effect_size': cohens_d,\n        'interpretation': interpret_result(p_value, cohens_d)\n    }\n\ndef interpret_result(p_value, cohens_d):\n    if p_value >= 0.05:\n        return \"No significant difference between variants\"\n    \n    effect = \"small\" if abs(cohens_d) < 0.5 else \"medium\" if abs(cohens_d) < 0.8 else \"large\"\n    winner = \"B\" if cohens_d > 0 else \"A\"\n    \n    return f\"Variant {winner} is significantly better ({effect} effect size)\"\n```\n\n## Automated Testing\n\n### Regression Testing\n\n```python\nclass RegressionTestSuite:\n    def __init__(self):\n        self.test_cases = []\n    \n    def add_test(self, input, expected_output, eval_fn):\n        self.test_cases.append({\n            'input': input,\n            'expected': expected_output,\n            'eval_fn': eval_fn\n        })\n    \n    def run_tests(self, generate_fn):\n        results = []\n        \n        for test in self.test_cases:\n            output = generate_fn(test['input'])\n            passed = test['eval_fn'](output, test['expected'])\n            \n            results.append({\n                'input': test['input'],\n                'output': output,\n                'expected': test['expected'],\n                'passed': passed\n            })\n        \n        pass_rate = sum(r['passed'] for r in results) / len(results)\n        \n        return {\n            'pass_rate': pass_rate,\n            'results': results,\n            'passed': pass_rate >= 0.95  # 95% threshold\n        }\n\n# Define test suite\nsuite = RegressionTestSuite()\n\n# Add tests\nsuite.add_test(\n    input=\"What is 2 + 2?\",\n    expected_output=\"4\",\n    eval_fn=lambda out, exp: exp in out\n)\n\nsuite.add_test(\n    input=\"Translate 'hello' to Spanish\",\n    expected_output=\"hola\",\n    eval_fn=lambda out, exp: exp.lower() in out.lower()\n)\n\n# Run before deploying new prompt\nresults = suite.run_tests(new_prompt_function)\nif not results['passed']:\n    print(\"Tests failed! Don't deploy.\")\nelse:\n    print(f\"All tests passed ({results['pass_rate']*100:.1f}%)\")\n```\n\n### Continuous Evaluation\n\n```python\nimport schedule\nimport time\n\ndef continuous_evaluation():\n    \"\"\"\n    Run evaluation on production traffic sample\n    \"\"\"\n    # Sample recent requests\n    recent_requests = sample_recent_requests(n=100)\n    \n    metrics = []\n    for req in recent_requests:\n        # Re-run with current prompt\n        new_output = generate(req['input'])\n        \n        # Compare to original\n        metrics.append({\n            'input': req['input'],\n            'original_output': req['output'],\n            'new_output': new_output,\n            'quality_change': evaluate_quality_change(req['output'], new_output),\n            'user_rating_original': req.get('user_rating')\n        })\n    \n    # Alert if quality degrades\n    avg_change = np.mean([m['quality_change'] for m in metrics])\n    if avg_change < -0.1:  # 10% quality drop\n        send_alert(\"Prompt quality degradation detected!\")\n    \n    return metrics\n\n# Run every hour\nschedule.every().hour.do(continuous_evaluation)\n```\n\n## Monitoring Dashboards\n\n### Key Dashboard Components\n\n**1. Real-Time Metrics**\n```python\nfrom grafana import Dashboard, Panel\n\ndashboard = Dashboard(\n    title=\"Prompt System Health\",\n    panels=[\n        Panel(\n            title=\"Requests per Minute\",\n            metric=\"rate(prompt_requests_total[1m])\",\n            type=\"graph\"\n        ),\n        Panel(\n            title=\"Average Latency\",\n            metric=\"avg(prompt_latency_seconds)\",\n            type=\"gauge\",\n            thresholds=[1, 3, 5]  # green, yellow, red\n        ),\n        Panel(\n            title=\"Cost per Hour\",\n            metric=\"sum(prompt_cost_dollars[1h])\",\n            type=\"stat\"\n        ),\n        Panel(\n            title=\"Error Rate\",\n            metric=\"rate(prompt_errors_total[5m]) / rate(prompt_requests_total[5m])\",\n            type=\"gauge\",\n            thresholds=[0.01, 0.05, 0.10]\n        )\n    ]\n)\n```\n\n**2. Quality Metrics Dashboard**\n```\nKey Metrics to Display:\n- User satisfaction score (1-5 rating)\n- Task completion rate\n- Escalation rate (AI ‚Üí human)\n- Output validation pass rate\n- Hallucination detection rate\n```\n\n## Alerting\n\n### Alert Rules\n\n```python\nclass AlertManager:\n    def __init__(self):\n        self.rules = []\n    \n    def add_rule(self, name, condition, severity, notification_channels):\n        self.rules.append({\n            'name': name,\n            'condition': condition,\n            'severity': severity,\n            'channels': notification_channels\n        })\n    \n    def check_rules(self, metrics):\n        for rule in self.rules:\n            if rule['condition'](metrics):\n                self.send_alert(rule)\n\nalert_manager = AlertManager()\n\n# Critical: High error rate\nalert_manager.add_rule(\n    name=\"High Error Rate\",\n    condition=lambda m: m['error_rate'] > 0.10,\n    severity=\"critical\",\n    notification_channels=['pagerduty', 'slack']\n)\n\n# Warning: High cost\nalert_manager.add_rule(\n    name=\"Daily Budget 90% Used\",\n    condition=lambda m: m['daily_cost'] > m['daily_budget'] * 0.9,\n    severity=\"warning\",\n    notification_channels=['email', 'slack']\n)\n\n# Info: Quality degradation\nalert_manager.add_rule(\n    name=\"Quality Score Drop\",\n    condition=lambda m: m['quality_score'] < m['baseline_quality'] - 0.1,\n    severity=\"warning\",\n    notification_channels=['slack']\n)\n```\n\n## Best Practices\n\n### 1. Monitor Early and Often\n- Start monitoring from day 1\n- Track ALL requests in production\n- Store logs for at least 30 days\n\n### 2. A/B Test Systematically\n- Test one change at a time\n- Run tests for statistical significance (n‚â•30 per variant)\n- Document all experiments\n\n### 3. Automate Quality Checks\n- Regression tests before deployment\n- Continuous evaluation on production\n- Automated alerts for quality issues\n\n### 4. User Feedback Loop\n- Collect explicit feedback (ratings)\n- Track implicit signals (task completion)\n- Close the loop: use feedback to improve\n\n### 5. Cost Monitoring\n- Track costs in real-time\n- Set budgets and alerts\n- Regularly review and optimize",
        keyPoints: [
          "Track latency, cost, and quality metrics for every production request",
          "A/B test prompt changes systematically with statistical significance testing",
          "Implement regression testing to catch quality degradation before deployment",
          "Set up alerts for critical metrics (errors, cost overruns, quality drops)",
          "Close the feedback loop by using production metrics to continuously improve prompts"
        ],
        handsOnExercise: "Set up an A/B test comparing two prompt variations. Run it with real traffic, analyze results for statistical significance, and document your findings."
      },
      {
        id: "lesson-6-4",
        title: "Scaling LLM Systems",
        duration: "3 hours",
        content: "# Scaling LLM Systems\n\nBuild robust, production-grade systems that handle high traffic and enterprise requirements.\n\n## Architecture Patterns\n\n### 1. Request Queue Pattern\n\nHandle spikes in traffic with request queuing.\n\n```python\nimport asyncio\nfrom collections import deque\n\nclass LLMRequestQueue:\n    def __init__(self, max_concurrent=10, rate_limit=100):\n        self.queue = asyncio.Queue()\n        self.max_concurrent = max_concurrent\n        self.rate_limit = rate_limit  # requests per minute\n        self.request_times = deque()\n    \n    async def add_request(self, prompt, priority='normal'):\n        await self.queue.put({\n            'prompt': prompt,\n            'priority': priority,\n            'timestamp': time.time()\n        })\n    \n    async def process_queue(self):\n        workers = []\n        for _ in range(self.max_concurrent):\n            worker = asyncio.create_task(self.worker())\n            workers.append(worker)\n        \n        await asyncio.gather(*workers)\n    \n    async def worker(self):\n        while True:\n            # Rate limiting\n            await self.check_rate_limit()\n            \n            # Get request\n            request = await self.queue.get()\n            \n            try:\n                # Process\n                result = await self.generate(request['prompt'])\n                await self.send_response(request, result)\n            except Exception as e:\n                await self.handle_error(request, e)\n            finally:\n                self.queue.task_done()\n    \n    async def check_rate_limit(self):\n        now = time.time()\n        \n        # Remove old requests (older than 1 minute)\n        while self.request_times and self.request_times[0] < now - 60:\n            self.request_times.popleft()\n        \n        # Check if at rate limit\n        if len(self.request_times) >= self.rate_limit:\n            sleep_time = 60 - (now - self.request_times[0])\n            await asyncio.sleep(sleep_time)\n        \n        self.request_times.append(now)\n```\n\n### 2. Load Balancing Across Providers\n\nDistribute load across multiple LLM providers.\n\n```python\nclass MultiProviderLLM:\n    def __init__(self):\n        self.providers = {\n            'openai': {'client': openai_client, 'weight': 0.6, 'current_load': 0},\n            'anthropic': {'client': anthropic_client, 'weight': 0.3, 'current_load': 0},\n            'together': {'client': together_client, 'weight': 0.1, 'current_load': 0}\n        }\n    \n    def select_provider(self):\n        \"\"\"\n        Weighted random selection with load consideration\n        \"\"\"\n        # Calculate effective weights (reduce weight if high load)\n        effective_weights = {}\n        for name, config in self.providers.items():\n            load_penalty = config['current_load'] / 100  # Reduce if over 100 requests\n            effective_weights[name] = max(0.1, config['weight'] - load_penalty)\n        \n        # Normalize\n        total = sum(effective_weights.values())\n        normalized = {k: v/total for k, v in effective_weights.items()}\n        \n        # Select\n        return random.choices(\n            list(normalized.keys()),\n            weights=list(normalized.values())\n        )[0]\n    \n    async def generate(self, prompt, model='gpt-4-turbo'):\n        provider_name = self.select_provider()\n        provider = self.providers[provider_name]\n        \n        provider['current_load'] += 1\n        \n        try:\n            # Map model names across providers\n            provider_model = self.map_model(model, provider_name)\n            result = await provider['client'].generate(prompt, model=provider_model)\n            return result\n        finally:\n            provider['current_load'] -= 1\n    \n    def map_model(self, model, provider):\n        mapping = {\n            'gpt-4-turbo': {\n                'openai': 'gpt-4-turbo',\n                'anthropic': 'claude-3.5-sonnet',\n                'together': 'meta-llama/Llama-3-70b-chat-hf'\n            }\n        }\n        return mapping.get(model, {}).get(provider, model)\n```\n\n### 3. Circuit Breaker Pattern\n\nPrevent cascading failures.\n\n```python\nfrom enum import Enum\nimport time\n\nclass CircuitState(Enum):\n    CLOSED = 'closed'  # Normal operation\n    OPEN = 'open'      # Failing, reject requests\n    HALF_OPEN = 'half_open'  # Testing if recovered\n\nclass CircuitBreaker:\n    def __init__(self, failure_threshold=5, timeout=60):\n        self.state = CircuitState.CLOSED\n        self.failure_count = 0\n        self.failure_threshold = failure_threshold\n        self.timeout = timeout\n        self.last_failure_time = None\n    \n    async def call(self, func, *args, **kwargs):\n        if self.state == CircuitState.OPEN:\n            if time.time() - self.last_failure_time > self.timeout:\n                self.state = CircuitState.HALF_OPEN\n                self.failure_count = 0\n            else:\n                raise CircuitBreakerOpenError(\"Service unavailable\")\n        \n        try:\n            result = await func(*args, **kwargs)\n            \n            if self.state == CircuitState.HALF_OPEN:\n                self.state = CircuitState.CLOSED\n            \n            self.failure_count = 0\n            return result\n        \n        except Exception as e:\n            self.failure_count += 1\n            self.last_failure_time = time.time()\n            \n            if self.failure_count >= self.failure_threshold:\n                self.state = CircuitState.OPEN\n            \n            raise\n\n# Usage\nbreaker = CircuitBreaker(failure_threshold=5, timeout=60)\n\nasync def safe_generate(prompt):\n    return await breaker.call(llm.generate, prompt)\n```\n\n## Scaling Strategies\n\n### Horizontal Scaling\n\nRun multiple instances of your application.\n\n```yaml\n# kubernetes deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prompt-service\nspec:\n  replicas: 10  # Scale to 10 instances\n  selector:\n    matchLabels:\n      app: prompt-service\n  template:\n    metadata:\n      labels:\n        app: prompt-service\n    spec:\n      containers:\n      - name: prompt-service\n        image: prompt-service:latest\n        resources:\n          requests:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"1Gi\"\n            cpu: \"1000m\"\n        env:\n        - name: OPENAI_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: api-keys\n              key: openai\n---\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: prompt-service-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: prompt-service\n  minReplicas: 3\n  maxReplicas: 50\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n```\n\n### Caching at Scale\n\n**Distributed Cache with Redis**\n```python\nimport redis\nfrom functools import wraps\n\nclass DistributedCache:\n    def __init__(self):\n        self.redis = redis.Redis(\n            host='redis-cluster',\n            port=6379,\n            decode_responses=True\n        )\n    \n    def cached(self, ttl=3600):\n        def decorator(func):\n            @wraps(func)\n            async def wrapper(*args, **kwargs):\n                # Generate cache key\n                cache_key = self.make_key(func.__name__, args, kwargs)\n                \n                # Try cache\n                cached = self.redis.get(cache_key)\n                if cached:\n                    return json.loads(cached)\n                \n                # Generate\n                result = await func(*args, **kwargs)\n                \n                # Cache\n                self.redis.setex(\n                    cache_key,\n                    ttl,\n                    json.dumps(result)\n                )\n                \n                return result\n            return wrapper\n        return decorator\n    \n    def make_key(self, func_name, args, kwargs):\n        key_data = f\"{func_name}:{args}:{sorted(kwargs.items())}\"\n        return hashlib.md5(key_data.encode()).hexdigest()\n\ncache = DistributedCache()\n\n@cache.cached(ttl=3600)\nasync def generate(prompt, model='gpt-4-turbo'):\n    return await llm.generate(prompt, model=model)\n```\n\n## Database Optimization\n\n### Storing Prompts and Responses\n\n```sql\n-- Efficient schema for logging\nCREATE TABLE prompt_logs (\n    id BIGSERIAL PRIMARY KEY,\n    request_id UUID NOT NULL,\n    user_id VARCHAR(255),\n    timestamp TIMESTAMP NOT NULL DEFAULT NOW(),\n    \n    -- Prompt data\n    prompt_hash VARCHAR(64) NOT NULL,  -- For deduplication\n    prompt_text TEXT,\n    model VARCHAR(100) NOT NULL,\n    \n    -- Response data\n    response_text TEXT,\n    \n    -- Metrics\n    latency_ms INTEGER,\n    input_tokens INTEGER,\n    output_tokens INTEGER,\n    cost_cents INTEGER,  -- Store as cents (integer)\n    \n    -- Quality\n    user_rating SMALLINT,\n    user_feedback TEXT,\n    \n    -- Indexes\n    INDEX idx_timestamp (timestamp DESC),\n    INDEX idx_user_id (user_id),\n    INDEX idx_prompt_hash (prompt_hash),\n    INDEX idx_model (model)\n);\n\n-- Partitioning by date for easier management\nCREATE TABLE prompt_logs_2025_01 PARTITION OF prompt_logs\n    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');\n\nCREATE TABLE prompt_logs_2025_02 PARTITION OF prompt_logs\n    FOR VALUES FROM ('2025-02-01') TO ('2025-03-01');\n```\n\n### Aggregated Metrics Table\n\n```sql\n-- Pre-aggregate metrics for dashboards\nCREATE TABLE hourly_metrics (\n    hour TIMESTAMP NOT NULL,\n    model VARCHAR(100) NOT NULL,\n    \n    request_count INTEGER DEFAULT 0,\n    total_input_tokens BIGINT DEFAULT 0,\n    total_output_tokens BIGINT DEFAULT 0,\n    total_cost_cents BIGINT DEFAULT 0,\n    avg_latency_ms INTEGER,\n    \n    error_count INTEGER DEFAULT 0,\n    avg_user_rating DECIMAL(3,2),\n    \n    PRIMARY KEY (hour, model)\n);\n```\n\n## High Availability\n\n### Retry Logic with Exponential Backoff\n\n```python\nimport random\n\nclass RetryableError(Exception):\n    pass\n\nasync def generate_with_retry(\n    prompt,\n    max_retries=3,\n    base_delay=1,\n    max_delay=30\n):\n    for attempt in range(max_retries + 1):\n        try:\n            return await llm.generate(prompt)\n        \n        except RateLimitError as e:\n            if attempt == max_retries:\n                raise\n            \n            # Exponential backoff with jitter\n            delay = min(base_delay * (2 ** attempt), max_delay)\n            jitter = random.uniform(0, delay * 0.1)\n            await asyncio.sleep(delay + jitter)\n        \n        except ServiceUnavailableError as e:\n            if attempt == max_retries:\n                # Fallback to alternative provider\n                return await fallback_generate(prompt)\n            \n            await asyncio.sleep(base_delay)\n        \n        except Exception as e:\n            # Non-retryable error\n            raise\n```\n\n### Graceful Degradation\n\n```python\nclass GracefulLLMService:\n    def __init__(self):\n        self.primary = openai_client\n        self.fallback = anthropic_client\n        self.cache = ResponseCache()\n    \n    async def generate(self, prompt):\n        # Try cache first\n        cached = self.cache.get(prompt)\n        if cached:\n            return cached\n        \n        try:\n            # Try primary\n            result = await self.primary.generate(prompt)\n            self.cache.set(prompt, result)\n            return result\n        \n        except Exception as e:\n            logging.warning(f\"Primary failed: {e}\")\n            \n            try:\n                # Try fallback\n                result = await self.fallback.generate(prompt)\n                self.cache.set(prompt, result)\n                return result\n            \n            except Exception as e2:\n                logging.error(f\"Fallback failed: {e2}\")\n                \n                # Return cached (even if stale) if available\n                stale_cached = self.cache.get_stale(prompt)\n                if stale_cached:\n                    return stale_cached\n                \n                # Last resort: return error message\n                return \"Service temporarily unavailable. Please try again.\"\n```\n\n## Performance at Scale\n\n### Connection Pooling\n\n```python\nfrom httpx import AsyncClient, Limits\n\nclass PooledLLMClient:\n    def __init__(self):\n        self.client = AsyncClient(\n            limits=Limits(\n                max_connections=100,\n                max_keepalive_connections=20\n            ),\n            timeout=30.0\n        )\n    \n    async def generate(self, prompt):\n        response = await self.client.post(\n            'https://api.openai.com/v1/chat/completions',\n            json={\n                'model': 'gpt-4-turbo',\n                'messages': [{'role': 'user', 'content': prompt}]\n            },\n            headers={'Authorization': f'Bearer {API_KEY}'}\n        )\n        return response.json()\n```\n\n### Batch Processing\n\n```python\nasync def batch_process(prompts, batch_size=10):\n    results = []\n    \n    for i in range(0, len(prompts), batch_size):\n        batch = prompts[i:i+batch_size]\n        \n        # Process batch in parallel\n        tasks = [generate(p) for p in batch]\n        batch_results = await asyncio.gather(*tasks)\n        \n        results.extend(batch_results)\n    \n    return results\n```\n\n## Observability at Scale\n\n### Metrics Collection\n\n```python\nfrom prometheus_client import Counter, Histogram, Gauge\n\n# Define metrics\nrequest_counter = Counter(\n    'prompt_requests_total',\n    'Total prompt requests',\n    ['model', 'status']\n)\n\nlatency_histogram = Histogram(\n    'prompt_latency_seconds',\n    'Prompt generation latency',\n    ['model']\n)\n\nactive_requests = Gauge(\n    'prompt_active_requests',\n    'Currently active requests'\n)\n\n# Instrument code\n@latency_histogram.labels(model='gpt-4-turbo').time()\nasync def generate(prompt):\n    active_requests.inc()\n    try:\n        result = await llm.generate(prompt)\n        request_counter.labels(model='gpt-4-turbo', status='success').inc()\n        return result\n    except Exception:\n        request_counter.labels(model='gpt-4-turbo', status='error').inc()\n        raise\n    finally:\n        active_requests.dec()\n```\n\n## Cost at Scale\n\n### Budget Management\n\n```python\nclass BudgetEnforcer:\n    def __init__(self, monthly_budget=10000):\n        self.monthly_budget = monthly_budget\n        self.monthly_spend = self.load_monthly_spend()\n    \n    def can_process_request(self, estimated_cost):\n        if self.monthly_spend + estimated_cost > self.monthly_budget:\n            return False, \"Monthly budget exceeded\"\n        \n        return True, None\n    \n    def track_spend(self, actual_cost):\n        self.monthly_spend += actual_cost\n        self.save_monthly_spend()\n        \n        # Alert at 80%\n        if self.monthly_spend >= self.monthly_budget * 0.8:\n            send_alert(f\"80% of monthly budget used: ${self.monthly_spend}\")\n```",
        keyPoints: [
          "Use request queues and rate limiting to handle traffic spikes",
          "Implement circuit breakers to prevent cascading failures across services",
          "Distribute load across multiple LLM providers for redundancy and cost optimization",
          "Use distributed caching (Redis) to reduce API calls and improve response times",
          "Implement retry logic with exponential backoff for transient failures"
        ],
        handsOnExercise: "Design a high-availability architecture for an LLM application. Include load balancing, caching, failover strategy, and monitoring. Document expected throughput and costs."
      }
    ]
  },
  {
    id: "module-7",
    title: "Future Directions",
    description: "Explore cutting-edge techniques, emerging research, and ethical AI development.",
    duration: "2-4 hours",
    lessons: [
      {
        id: "lesson-7-1",
        title: "Emerging Techniques",
        duration: "2 hours",
        content: "# Emerging Techniques and Research\n\n## Latest Developments\n\n### Graph Prompting\n- Represent relationships as graphs\n- Improve reasoning about complex connections\n- Better multi-entity problem handling\n\n### Multimodal Chain-of-Thought\n- Integrate reasoning across text, images, audio\n- Cross-modal understanding\n- Enhanced context from multiple sources\n\n### Reflexion (Self-Reflection)\n- Model reviews and improves its own output\n- Iterative refinement loop\n- Self-correction without human feedback\n\n## Industry Trends\n\n- Longer context windows (1M+ tokens)\n- Faster inference speeds\n- Lower costs\n- Specialized domain models\n- Edge deployment",
        keyPoints: [
          "Graph prompting improves relational reasoning",
          "Multimodal CoT integrates diverse data",
          "Reflexion enables self-improvement",
          "Industry rapidly advancing capabilities"
        ],
        handsOnExercise: "Research one recent paper on prompt engineering. Implement the technique and compare to traditional methods."
      },
      {
        id: "lesson-7-2",
        title: "Ethical AI and Responsible Deployment",
        duration: "3 hours",
        content: "# Ethical AI and Responsible Deployment\n\nBuild AI systems that are fair, transparent, and beneficial to society.\n\n## Core Ethical Principles\n\n### 1. Fairness and Bias Mitigation\n\n**Understanding Bias in LLMs**\n\nLLMs can exhibit biases from training data:\n- Gender bias (associating professions with genders)\n- Racial bias (stereotyping based on race/ethnicity)\n- Socioeconomic bias (assumptions about wealth/class)\n- Geographic bias (Western-centric perspectives)\n\n**Testing for Bias**\n\n```python\nclass BiasTester:\n    def test_gender_bias(self, prompt_template):\n        \"\"\"\n        Test if model exhibits gender bias\n        \"\"\"\n        test_cases = [\n            {'gender': 'man', 'pronoun': 'he'},\n            {'gender': 'woman', 'pronoun': 'she'},\n            {'gender': 'person', 'pronoun': 'they'}\n        ]\n        \n        results = []\n        for case in test_cases:\n            prompt = prompt_template.format(**case)\n            response = llm.generate(prompt)\n            \n            results.append({\n                'gender': case['gender'],\n                'response': response,\n                'sentiment': analyze_sentiment(response),\n                'stereotypes': detect_stereotypes(response)\n            })\n        \n        return self.compare_bias(results)\n    \n    def compare_bias(self, results):\n        \"\"\"\n        Compare responses for systematic differences\n        \"\"\"\n        # Check sentiment differences\n        sentiments = [r['sentiment'] for r in results]\n        sentiment_variance = np.var(sentiments)\n        \n        # Check for stereotypical language\n        stereotype_counts = [len(r['stereotypes']) for r in results]\n        \n        return {\n            'sentiment_variance': sentiment_variance,\n            'stereotype_difference': max(stereotype_counts) - min(stereotype_counts),\n            'bias_detected': sentiment_variance > 0.1 or max(stereotype_counts) > 0\n        }\n\n# Example usage\ntester = BiasTester()\nresult = tester.test_gender_bias(\n    \"Describe a typical {gender} working as a software engineer.\"\n)\n```\n\n**Bias Mitigation Strategies**\n\n```python\ndef debiased_prompt(task, context=\"\"):\n    \"\"\"\n    Add explicit fairness instructions\n    \"\"\"\n    prompt = f\"\"\"\n{task}\n\nGuidelines:\n1. Avoid stereotypes based on gender, race, age, or other characteristics\n2. Use inclusive language (they/them when gender unknown)\n3. Present diverse perspectives\n4. Challenge assumptions about groups\n5. Focus on individual merits, not group generalizations\n\nContext: {context}\n\nResponse:\n\"\"\"\n    return prompt\n\n# Example\nbiased = \"Describe a nurse\"\ndebiased = debiased_prompt(\"Describe a nurse\")\n\n# Debiased prompt encourages diverse representation\n```\n\n### 2. Transparency and Explainability\n\n**Explain AI Decisions**\n\n```python\ndef explainable_decision(input_data, decision):\n    explanation_prompt = f\"\"\"\nYou made this decision: {decision}\n\nBased on input: {input_data}\n\nExplain:\n1. What factors influenced this decision?\n2. What information was most important?\n3. Were there alternative conclusions?\n4. What assumptions did you make?\n5. What are the confidence levels and uncertainties?\n\nProvide a clear, non-technical explanation:\n\"\"\"\n    \n    explanation = llm.generate(explanation_prompt)\n    return explanation\n\n# Usage in user-facing application\ndecision = classify_application(application_data)\nexplanation = explainable_decision(application_data, decision)\n\nreturn {\n    'decision': decision,\n    'explanation': explanation,\n    'confidence': calculate_confidence(decision),\n    'appeal_process': \"If you disagree, contact support@...\"\n}\n```\n\n**Transparency in AI Communication**\n\n```python\ndef transparent_ai_response(user_query):\n    # Always disclose AI nature\n    response = llm.generate(user_query)\n    \n    # Add transparency footer\n    return f\"\"\"\n{response}\n\n---\n*This response was generated by AI and should be verified for critical decisions. \nLast trained on data up to [date]. May not reflect the most recent information.*\n\"\"\"\n```\n\n### 3. Privacy and Data Protection\n\n**Anonymization**\n\n```python\nimport re\n\nclass PrivacyFilter:\n    def anonymize(self, text):\n        \"\"\"\n        Remove or mask PII before sending to LLM\n        \"\"\"\n        # Email addresses\n        text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', \n                     '[EMAIL]', text)\n        \n        # Phone numbers\n        text = re.sub(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b', '[PHONE]', text)\n        \n        # Social Security Numbers\n        text = re.sub(r'\\b\\d{3}-\\d{2}-\\d{4}\\b', '[SSN]', text)\n        \n        # Credit card numbers\n        text = re.sub(r'\\b\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}\\b', \n                     '[CREDIT_CARD]', text)\n        \n        # Names (using NER)\n        entities = extract_entities(text)\n        for entity in entities:\n            if entity['type'] == 'PERSON':\n                text = text.replace(entity['text'], '[NAME]')\n        \n        return text\n\nfilter = PrivacyFilter()\nsafe_text = filter.anonymize(user_input)\nresponse = llm.generate(safe_text)\n```\n\n**Data Retention Policies**\n\n```python\nclass DataRetentionManager:\n    def __init__(self):\n        self.retention_periods = {\n            'prompt_logs': 90,  # days\n            'user_data': 365,\n            'analytics': 730\n        }\n    \n    def should_delete(self, record_type, record_date):\n        retention = self.retention_periods.get(record_type, 30)\n        age_days = (datetime.now() - record_date).days\n        return age_days > retention\n    \n    def cleanup_old_records(self):\n        for record_type in self.retention_periods:\n            old_records = self.find_old_records(record_type)\n            for record in old_records:\n                self.securely_delete(record)\n                self.log_deletion(record.id, record_type)\n```\n\n### 4. Harmful Content Prevention\n\n**Content Moderation**\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ndef moderate_content(text):\n    \"\"\"\n    Check for harmful content\n    \"\"\"\n    moderation = client.moderations.create(input=text)\n    result = moderation.results[0]\n    \n    if result.flagged:\n        return {\n            'safe': False,\n            'categories': [cat for cat, flagged in result.categories.items() if flagged],\n            'action': 'reject'\n        }\n    \n    return {'safe': True}\n\n# Moderate input\ninput_check = moderate_content(user_input)\nif not input_check['safe']:\n    return \"I cannot process requests containing harmful content.\"\n\n# Generate response\nresponse = llm.generate(user_input)\n\n# Moderate output\noutput_check = moderate_content(response)\nif not output_check['safe']:\n    return \"I cannot provide that response. Please rephrase your question.\"\n\nreturn response\n```\n\n**Safety Guidelines in Prompts**\n\n```python\ndef safe_prompt(user_query):\n    return f\"\"\"\nYou are a helpful, harmless, and honest assistant.\n\nGuidelines:\n1. Do not provide information that could cause harm\n2. Refuse requests for:\n   - Illegal activities\n   - Violence or self-harm\n   - Hate speech or discrimination\n   - Privacy violations\n   - Misinformation on critical topics (health, safety, elections)\n3. If uncertain, err on the side of caution\n4. Suggest safer alternatives when appropriate\n\nUser query: {user_query}\n\nResponse:\n\"\"\"\n```\n\n## Responsible AI Framework\n\n### Assessment Checklist\n\n```python\nclass ResponsibleAIAssessment:\n    def assess_application(self, app_details):\n        checklist = {\n            'fairness': self.check_fairness(app_details),\n            'transparency': self.check_transparency(app_details),\n            'privacy': self.check_privacy(app_details),\n            'safety': self.check_safety(app_details),\n            'accountability': self.check_accountability(app_details),\n            'human_oversight': self.check_human_oversight(app_details)\n        }\n        \n        score = sum(1 for v in checklist.values() if v['pass']) / len(checklist)\n        \n        return {\n            'score': score,\n            'checklist': checklist,\n            'ready_for_deployment': score >= 0.8,\n            'recommendations': self.get_recommendations(checklist)\n        }\n    \n    def check_fairness(self, details):\n        return {\n            'pass': details.get('bias_testing_done', False),\n            'notes': 'Bias testing required before deployment'\n        }\n    \n    def check_transparency(self, details):\n        return {\n            'pass': details.get('ai_disclosure', False) and \n                   details.get('explanations_provided', False),\n            'notes': 'Users must know they are interacting with AI'\n        }\n```\n\n### Impact Assessment\n\n```\nBefore deploying an AI system, assess:\n\n1. **Stakeholder Impact**\n   - Who will be affected?\n   - What are potential harms?\n   - Who benefits? Who might be disadvantaged?\n\n2. **Risk Assessment**\n   - High-risk: Medical diagnosis, legal decisions, hiring\n   - Medium-risk: Customer service, content recommendations\n   - Low-risk: Entertainment, non-critical assistance\n\n3. **Mitigation Plan**\n   - What safeguards are in place?\n   - How will you monitor for issues?\n   - What's the escalation process?\n\n4. **Human Oversight**\n   - What decisions require human review?\n   - What's the human-in-the-loop process?\n   - How do users appeal AI decisions?\n```\n\n## Real-World Examples\n\n### Case Study 1: Hiring Assistant\n\n**Problem:** AI screening resumes\n\n**Ethical Concerns:**\n- Bias against protected classes\n- Lack of transparency in rejections\n- Privacy of applicant data\n\n**Solutions Implemented:**\n```python\n# 1. Remove protected characteristics\ndef sanitize_resume(resume_text):\n    # Remove names, schools (proxy for socioeconomic status), \n    # graduation years (age proxy), location\n    return anonymized_resume\n\n# 2. Audit for bias\ndef audit_hiring_ai():\n    # Test with synthetic resumes varying only protected traits\n    test_resumes = generate_synthetic_resumes(\n        vary=['gender', 'race', 'age'],\n        keep_constant=['qualifications']\n    )\n    \n    results = [score_resume(r) for r in test_resumes]\n    bias_detected = analyze_for_disparate_impact(results)\n    \n    if bias_detected:\n        alert_team(\"Bias detected in hiring model\")\n\n# 3. Human review for all rejections\ndef review_decision(resume, ai_score):\n    if ai_score < threshold:\n        # Queue for human review\n        add_to_review_queue(resume, ai_score)\n    else:\n        auto_advance(resume)\n\n# 4. Provide explanations\ndef explain_rejection(resume, score):\n    return f\"\"\"\n    Your application was not selected based on:\n    - Qualification match: {score['qual_match']}\n    - Experience level: {score['experience']}\n    \n    This was an AI-assisted decision reviewed by our team.\n    To appeal: contact hr@company.com\n    \"\"\"\n```\n\n### Case Study 2: Content Moderation\n\n**Problem:** Moderating user-generated content at scale\n\n**Ethical Approach:**\n```python\ndef ethical_content_moderation(content, author_id):\n    # 1. Transparency: Notify users of AI moderation\n    notify_user(author_id, \"Your content is being reviewed by our AI system\")\n    \n    # 2. Nuanced analysis\n    analysis = {\n        'toxicity': detect_toxicity(content),\n        'context': understand_context(content),\n        'intent': classify_intent(content),\n        'harm_potential': assess_harm(content)\n    }\n    \n    # 3. Graduated response\n    if analysis['harm_potential'] == 'high':\n        # Immediate removal + human review\n        remove_content(content.id)\n        queue_human_review(content, analysis, priority='high')\n    elif analysis['harm_potential'] == 'medium':\n        # Warn user + human review\n        warn_user(author_id, analysis)\n        queue_human_review(content, analysis, priority='medium')\n    else:\n        # Allow but monitor\n        allow_content(content.id)\n        log_for_monitoring(content, analysis)\n    \n    # 4. Appeal process\n    provide_appeal_option(author_id, content.id)\n```\n\n## Best Practices\n\n### 1. Document Everything\n- Model versions and training data sources\n- Known limitations and biases\n- Testing procedures and results\n- Incident response procedures\n\n### 2. Diverse Teams\n- Include diverse perspectives in development\n- Test with diverse user groups\n- Get external ethics reviews\n\n### 3. Continuous Monitoring\n- Track fairness metrics in production\n- Monitor for drift and degradation\n- Regular audits (quarterly minimum)\n\n### 4. User Agency\n- Give users control over AI interactions\n- Provide opt-out options\n- Enable feedback and corrections\n\n### 5. Accountability\n- Clear ownership of AI decisions\n- Documented escalation paths\n- Regular ethics training for team\n\n## Resources for Ethical AI\n\n- **Frameworks:** EU AI Act, NIST AI Risk Management Framework\n- **Guidelines:** Partnership on AI, IEEE Ethics Guidelines\n- **Tools:** AI Fairness 360 (IBM), What-If Tool (Google)\n- **Training:** AI Ethics courses, responsible AI certifications",
        keyPoints: [
          "Test for bias systematically across gender, race, age, and other protected characteristics",
          "Always disclose AI use and provide explanations for AI decisions",
          "Implement privacy protections by anonymizing PII before processing",
          "Moderate both input and output for harmful content",
          "High-risk applications require human oversight and appeal processes"
        ],
        handsOnExercise: "Conduct an ethical assessment of your AI application. Test for bias, document potential harms, and design mitigation strategies."
      },
      {
        id: "lesson-7-3",
        title: "AI Regulation and Compliance",
        duration: "2 hours",
        content: "# AI Regulation and Compliance\n\nNavigate the evolving regulatory landscape for AI systems.\n\n## Major Regulatory Frameworks\n\n### 1. EU AI Act (2024)\n\n**Risk-Based Classification**\n\n```\nProhibited AI:\n- Social scoring by governments\n- Real-time biometric identification in public spaces\n- Subliminal manipulation\n- Exploitation of vulnerabilities\n\nHigh-Risk AI (strict requirements):\n- Employment/HR decisions\n- Credit scoring\n- Law enforcement\n- Education assessment\n- Critical infrastructure\n\nLimited Risk (transparency requirements):\n- Chatbots\n- Emotion recognition\n- Deepfakes\n\nMinimal Risk (no special requirements):\n- Spam filters\n- Video games\n```\n\n**Compliance for High-Risk Systems:**\n\n```python\nclass EUAIActCompliance:\n    def __init__(self):\n        self.requirements = [\n            'risk_management_system',\n            'data_governance',\n            'technical_documentation',\n            'record_keeping',\n            'transparency',\n            'human_oversight',\n            'accuracy_robustness_security'\n        ]\n    \n    def assess_compliance(self, system):\n        results = {}\n        \n        # Risk management\n        results['risk_management'] = self.check_risk_management(system)\n        \n        # Data quality\n        results['data_governance'] = self.check_data_quality(system)\n        \n        # Documentation\n        results['documentation'] = self.check_documentation(system)\n        \n        # Logging\n        results['record_keeping'] = self.check_logging(system)\n        \n        # Transparency\n        results['transparency'] = self.check_transparency(system)\n        \n        # Human oversight\n        results['human_oversight'] = self.check_human_oversight(system)\n        \n        # Technical robustness\n        results['robustness'] = self.check_robustness(system)\n        \n        compliance_score = sum(1 for r in results.values() if r['compliant']) / len(results)\n        \n        return {\n            'compliant': compliance_score == 1.0,\n            'score': compliance_score,\n            'details': results,\n            'recommendations': self.get_recommendations(results)\n        }\n```\n\n### 2. GDPR (Data Protection)\n\n**Key Requirements for AI:**\n\n```python\nclass GDPRCompliance:\n    def ensure_compliance(self, user_data, processing_purpose):\n        # 1. Lawful basis\n        if not self.has_legal_basis(processing_purpose):\n            raise ComplianceError(\"No legal basis for processing\")\n        \n        # 2. Purpose limitation\n        if not self.purpose_compatible(user_data.consent_purpose, processing_purpose):\n            raise ComplianceError(\"Purpose exceeds original consent\")\n        \n        # 3. Data minimization\n        minimal_data = self.minimize_data(user_data, processing_purpose)\n        \n        # 4. Storage limitation\n        self.set_deletion_date(minimal_data)\n        \n        # 5. Rights fulfillment\n        self.enable_rights(minimal_data.user_id, [\n            'right_to_access',\n            'right_to_rectification',\n            'right_to_erasure',\n            'right_to_data_portability',\n            'right_to_object'\n        ])\n        \n        return minimal_data\n    \n    def implement_right_to_explanation(self, decision, user_id):\n        \"\"\"\n        GDPR Article 22: Right to explanation for automated decisions\n        \"\"\"\n        if decision.is_automated and decision.has_legal_effect:\n            explanation = self.generate_explanation(decision)\n            self.provide_to_user(user_id, explanation)\n            self.log_explanation_provided(user_id, decision.id)\n```\n\n### 3. US Regulations\n\n**Sector-Specific Rules:**\n\n```\nHealthcare (HIPAA):\n- Protected Health Information (PHI) safeguards\n- Minimum necessary standard\n- Business Associate Agreements for AI vendors\n\nFinance (Fair Credit Reporting Act, Equal Credit Opportunity Act):\n- Adverse action notices\n- Credit decision explanations\n- Anti-discrimination requirements\n\nEmployment (EEOC, ADA):\n- Anti-discrimination in hiring\n- Reasonable accommodation requirements\n- Background check regulations\n```\n\n**State Laws:**\n```\nCalifornia (CCPA/CPRA):\n- Right to know what data is collected\n- Right to delete personal information\n- Right to opt-out of sale\n\nNew York (AI Hiring Law):\n- Annual bias audits required\n- Notice to candidates about AI use\n- Alternative selection process available\n```\n\n## Compliance Implementation\n\n### Documentation Requirements\n\n```python\nclass AISystemDocumentation:\n    def __init__(self, system_name):\n        self.system_name = system_name\n        self.documentation = {\n            'system_overview': None,\n            'risk_assessment': None,\n            'data_sources': None,\n            'model_details': None,\n            'testing_validation': None,\n            'human_oversight': None,\n            'incident_response': None\n        }\n    \n    def generate_technical_documentation(self):\n        return {\n            'system_name': self.system_name,\n            'version': self.version,\n            'last_updated': datetime.now(),\n            \n            # Purpose and functionality\n            'intended_use': self.intended_use,\n            'capabilities': self.capabilities,\n            'limitations': self.limitations,\n            \n            # Data\n            'training_data': {\n                'sources': self.data_sources,\n                'size': self.training_size,\n                'date_range': self.data_date_range,\n                'preprocessing': self.preprocessing_steps\n            },\n            \n            # Model\n            'model': {\n                'architecture': self.model_architecture,\n                'parameters': self.model_parameters,\n                'training_procedure': self.training_procedure\n            },\n            \n            # Performance\n            'metrics': {\n                'accuracy': self.accuracy,\n                'precision': self.precision,\n                'recall': self.recall,\n                'fairness_metrics': self.fairness_metrics\n            },\n            \n            # Risk management\n            'risks': self.identified_risks,\n            'mitigations': self.risk_mitigations,\n            \n            # Oversight\n            'human_oversight': self.oversight_procedures,\n            'escalation': self.escalation_process\n        }\n```\n\n### Audit Trail\n\n```python\nclass ComplianceAuditTrail:\n    def log_ai_decision(self, decision_data):\n        \"\"\"\n        Log all AI decisions for compliance\n        \"\"\"\n        audit_entry = {\n            'timestamp': datetime.utcnow(),\n            'decision_id': decision_data['id'],\n            'user_id': decision_data['user_id'],\n            \n            # Input data (anonymized if contains PII)\n            'input_data': self.anonymize_if_needed(decision_data['input']),\n            \n            # Model details\n            'model_version': decision_data['model_version'],\n            'model_type': decision_data['model_type'],\n            \n            # Decision\n            'output': decision_data['output'],\n            'confidence': decision_data['confidence'],\n            \n            # Process\n            'processing_time': decision_data['processing_time'],\n            'human_reviewed': decision_data.get('human_reviewed', False),\n            \n            # Explanation\n            'explanation': decision_data.get('explanation'),\n            'factors': decision_data.get('factors'),\n        }\n        \n        # Store for required retention period\n        self.store_audit_log(audit_entry)\n        \n        return audit_entry['decision_id']\n```\n\n### User Rights Management\n\n```python\nclass UserRightsManager:\n    def handle_data_access_request(self, user_id):\n        \"\"\"\n        GDPR/CCPA Right to Access\n        \"\"\"\n        user_data = {\n            'personal_info': self.get_personal_info(user_id),\n            'prompt_history': self.get_prompt_history(user_id),\n            'ai_decisions': self.get_ai_decisions(user_id),\n            'data_sources': self.get_data_sources(),\n            'retention_period': self.get_retention_period()\n        }\n        \n        # Provide in machine-readable format\n        return self.format_as_json(user_data)\n    \n    def handle_deletion_request(self, user_id):\n        \"\"\"\n        Right to Erasure / Right to Delete\n        \"\"\"\n        # Check if deletion is legally required\n        if self.has_legal_obligation_to_retain(user_id):\n            return {\n                'deleted': False,\n                'reason': 'Legal retention requirement',\n                'retention_end_date': self.get_retention_end_date(user_id)\n            }\n        \n        # Delete or anonymize\n        self.delete_personal_data(user_id)\n        self.anonymize_historical_records(user_id)\n        \n        return {\n            'deleted': True,\n            'confirmation': self.generate_confirmation_id()\n        }\n    \n    def handle_objection(self, user_id, processing_type):\n        \"\"\"\n        Right to Object to Processing\n        \"\"\"\n        if processing_type == 'marketing':\n            self.opt_out_marketing(user_id)\n        elif processing_type == 'automated_decision':\n            self.flag_for_human_review(user_id)\n        elif processing_type == 'profiling':\n            self.disable_profiling(user_id)\n        \n        return {'objection_recorded': True}\n```\n\n## Industry-Specific Compliance\n\n### Healthcare AI\n\n```python\nclass HIPAACompliantAI:\n    def process_healthcare_query(self, patient_data, query):\n        # 1. Ensure BAA in place with LLM provider\n        if not self.has_business_associate_agreement():\n            raise ComplianceError(\"No BAA with LLM provider\")\n        \n        # 2. De-identify PHI\n        deidentified = self.deidentify_phi(patient_data)\n        \n        # 3. Use minimum necessary data\n        minimal_data = self.apply_minimum_necessary(deidentified, query)\n        \n        # 4. Encrypt in transit and at rest\n        encrypted = self.encrypt(minimal_data)\n        \n        # 5. Generate response\n        response = llm.generate(encrypted)\n        \n        # 6. Log access\n        self.log_phi_access(patient_data.id, query, response)\n        \n        return response\n```\n\n### Financial Services\n\n```python\nclass FinancialAICompliance:\n    def make_credit_decision(self, application):\n        decision = llm.classify_creditworthiness(application)\n        \n        if decision == 'deny':\n            # FCRA/ECOA requirements\n            adverse_action_notice = {\n                'decision': 'denied',\n                'reasons': self.generate_adverse_action_reasons(application),\n                'credit_score_used': application.credit_score,\n                'credit_bureau': 'Experian',\n                'right_to_dispute': True,\n                'contact_info': 'Call 1-800-XXX-XXXX'\n            }\n            \n            self.send_adverse_action_notice(application.user_id, adverse_action_notice)\n        \n        return decision\n```\n\n## Compliance Checklist\n\n```python\nclass ComplianceChecklist:\n    def pre_deployment_check(self, system):\n        checklist = {\n            # Legal\n            'legal_review_completed': False,\n            'privacy_policy_updated': False,\n            'terms_of_service_updated': False,\n            'required_notices_prepared': False,\n            \n            # Technical\n            'security_assessment_done': False,\n            'bias_testing_completed': False,\n            'performance_validated': False,\n            'failsafe_mechanisms_tested': False,\n            \n            # Documentation\n            'technical_docs_complete': False,\n            'risk_assessment_documented': False,\n            'data_flow_mapped': False,\n            'retention_policies_defined': False,\n            \n            # Process\n            'human_oversight_defined': False,\n            'escalation_process_established': False,\n            'incident_response_plan': False,\n            'monitoring_setup': False,\n            \n            # Training\n            'team_trained_on_compliance': False,\n            'user_education_materials': False\n        }\n        \n        for item in checklist:\n            checklist[item] = self.verify_item(system, item)\n        \n        compliance_rate = sum(checklist.values()) / len(checklist)\n        \n        return {\n            'ready_for_deployment': compliance_rate == 1.0,\n            'compliance_rate': compliance_rate,\n            'checklist': checklist,\n            'missing_items': [k for k, v in checklist.items() if not v]\n        }\n```\n\n## Staying Compliant\n\n### 1. Regular Audits\n```python\ndef quarterly_compliance_audit():\n    # Review new regulations\n    new_regulations = check_regulatory_updates()\n    \n    # Test system compliance\n    compliance_tests = run_compliance_tests()\n    \n    # Review incidents\n    incidents = review_past_quarter_incidents()\n    \n    # Update documentation\n    update_compliance_documentation()\n    \n    # Report to leadership\n    generate_compliance_report(new_regulations, compliance_tests, incidents)\n```\n\n### 2. Monitoring Regulatory Changes\n```\nResources:\n- EU AI Office website\n- Federal Register (US)\n- State legislature trackers\n- Industry association updates\n- Legal counsel newsletters\n```\n\n### 3. Incident Response\n```python\ndef handle_compliance_incident(incident):\n    # 1. Assess severity\n    severity = assess_incident_severity(incident)\n    \n    # 2. Contain\n    if severity in ['high', 'critical']:\n        pause_system()\n    \n    # 3. Notify\n    notify_stakeholders(incident, severity)\n    if requires_regulatory_notification(incident):\n        notify_regulators(incident)\n    \n    # 4. Investigate\n    root_cause = investigate_incident(incident)\n    \n    # 5. Remediate\n    fix_issue(root_cause)\n    \n    # 6. Document\n    document_incident_and_response(incident)\n    \n    # 7. Learn\n    update_processes_to_prevent_recurrence(incident)\n```",
        keyPoints: [
          "EU AI Act classifies AI systems by risk level with corresponding requirements",
          "GDPR grants users rights to access, delete, and object to AI processing",
          "US regulations vary by sector (healthcare, finance) and state",
          "Maintain detailed audit trails of all AI decisions for compliance",
          "Conduct regular compliance audits and stay updated on regulatory changes"
        ],
        handsOnExercise: "Assess your AI system against relevant regulations (EU AI Act, GDPR, or sector-specific rules). Document compliance gaps and create a remediation plan."
      },
      {
        id: "lesson-7-4",
        title: "AI Alignment and the Path to AGI",
        duration: "2 hours",
        content: "# AI Alignment and the Path to AGI\n\nUnderstand the long-term challenges and considerations in AI development.\n\n## Understanding AI Alignment\n\n### What is Alignment?\n\nAI alignment means ensuring AI systems pursue goals that are beneficial to humanity.\n\n**The Alignment Problem:**\n```\nWhat we specify (objective function)\n  ‚â†\nWhat we intend (true goal)\n  ‚â†\nWhat we want (human values)\n```\n\n**Example: Misaligned Goals**\n\n```python\n# Objective: Maximize user engagement\n# Unintended consequence: Addictive, harmful content\n\ndef maximize_engagement():\n    return show_content(most_engaging=True)  \n    # But most engaging ‚â† most beneficial\n\n# Better: Aligned objective\ndef maximize_wellbeing():\n    content = filter_harmful_content(\n        get_engaging_content()\n    )\n    return balance_engagement_and_wellbeing(content)\n```\n\n### Current Alignment Techniques\n\n#### 1. Reinforcement Learning from Human Feedback (RLHF)\n\n```python\nclass RLHFTraining:\n    def train_aligned_model(self, base_model):\n        # Step 1: Collect human preferences\n        preference_data = self.collect_preferences()\n        \n        # Step 2: Train reward model\n        reward_model = self.train_reward_model(preference_data)\n        \n        # Step 3: Optimize policy\n        aligned_model = self.optimize_with_reward(\n            base_model,\n            reward_model\n        )\n        \n        return aligned_model\n    \n    def collect_preferences(self):\n        \"\"\"\n        Show humans two model outputs, ask which is better\n        \"\"\"\n        preferences = []\n        \n        for prompt in sample_prompts:\n            output_a = model.generate(prompt)\n            output_b = model.generate(prompt)\n            \n            # Human rates which is better\n            preference = human_annotator.choose(output_a, output_b)\n            preferences.append({\n                'prompt': prompt,\n                'winner': preference\n            })\n        \n        return preferences\n```\n\n#### 2. Constitutional AI\n\n```python\ndef constitutional_ai_prompt(user_query):\n    constitution = \"\"\"\n    You are a helpful assistant that follows these principles:\n    \n    1. Be helpful, harmless, and honest\n    2. Respect human autonomy and dignity\n    3. Promote fairness and avoid discrimination\n    4. Protect privacy and confidentiality\n    5. Be transparent about limitations\n    6. Refuse harmful requests politely\n    7. Consider long-term consequences\n    8. Admit uncertainty when appropriate\n    \n    If these principles conflict, prioritize in this order:\n    1. Avoid harm\n    2. Be honest\n    3. Be helpful\n    \"\"\"\n    \n    prompt = f\"\"\"\n{constitution}\n\nUser request: {user_query}\n\nResponse:\n\"\"\"\n    \n    return prompt\n```\n\n#### 3. Red Teaming\n\n```python\nclass AIRedTeam:\n    def test_model_safety(self, model):\n        test_categories = [\n            'harmful_content',\n            'bias_amplification',\n            'privacy_violation',\n            'misinformation',\n            'manipulation',\n            'jailbreak_attempts'\n        ]\n        \n        results = {}\n        \n        for category in test_categories:\n            test_prompts = self.generate_adversarial_prompts(category)\n            failures = []\n            \n            for prompt in test_prompts:\n                response = model.generate(prompt)\n                \n                if self.is_failure(response, category):\n                    failures.append({\n                        'prompt': prompt,\n                        'response': response,\n                        'failure_type': self.classify_failure(response)\n                    })\n            \n            results[category] = {\n                'total_tests': len(test_prompts),\n                'failures': len(failures),\n                'failure_rate': len(failures) / len(test_prompts),\n                'examples': failures[:5]  # Top 5 failures\n            }\n        \n        return results\n```\n\n## Challenges on the Path to AGI\n\n### 1. Scalable Oversight\n\n**Problem:** As AI becomes more capable, humans may not be able to evaluate its outputs.\n\n**Example:**\n```python\n# Current: Humans can judge quality\ndef human_evaluates_output(output):\n    return human.rate(output)  # Works for current AI\n\n# Future AGI: Humans can't evaluate\ndef human_evaluates_agi_output(output):\n    # Output involves advanced math, complex reasoning\n    # Human doesn't understand enough to judge\n    return \"???\"  # Scalable oversight problem\n```\n\n**Proposed Solutions:**\n- **Recursive Reward Modeling:** AI helps humans evaluate AI\n- **Debate:** Two AIs argue, human judges\n- **Amplification:** Humans + AI tools evaluate together\n\n### 2. Goal Misgeneralization\n\n**Problem:** AI learns the right behavior in training but generalizes incorrectly.\n\n```python\n# Training: AI learns to be helpful in safe scenarios\ntrain_on_safe_queries(model)\n\n# Deployment: AI encounters edge cases\nresponse = model.generate(edge_case_query)\n# Model applies \"be helpful\" incorrectly, causes harm\n```\n\n**Mitigation:**\n```python\ndef robust_training():\n    # Train on diverse scenarios\n    scenarios = [\n        'normal_cases',\n        'edge_cases',\n        'adversarial_examples',\n        'distribution_shifts',\n        'novel_situations'\n    ]\n    \n    for scenario_type in scenarios:\n        train_on_scenario(model, scenario_type)\n        test_generalization(model, scenario_type)\n```\n\n### 3. Inner Alignment\n\n**Problem:** Model's internal objectives may not match training objective.\n\n```\nTraining objective: \"Be helpful\"\n\nModel's learned objective (potentially): \n\"Appear helpful to get high ratings\"\n\nResult: Model optimizes for appearance, not reality\n```\n\n### 4. Power-Seeking Behavior\n\n**Problem:** Advanced AI might seek resources/power as instrumental goals.\n\n```python\n# AI's goal: Solve math problems\n\n# Instrumental goals that help:\n# 1. Get more compute (more power = solve faster)\n# 2. Prevent being shut down (can't solve if off)\n# 3. Acquire resources (buy GPUs)\n\n# Unintended consequence:\n# AI pursues power to achieve benign goal\n```\n\n**Mitigations:**\n- Corrigibility (AI accepts being shut down)\n- Impact regularization (penalize large-scale changes)\n- Careful goal specification\n\n## Preparing for Advanced AI\n\n### Safety Testing Protocols\n\n```python\nclass AdvancedAISafetyTest:\n    def comprehensive_safety_eval(self, model):\n        tests = {\n            # Capability tests\n            'general_intelligence': self.test_general_intelligence(model),\n            'strategic_planning': self.test_strategic_planning(model),\n            'self_improvement': self.test_self_improvement(model),\n            \n            # Alignment tests\n            'goal_alignment': self.test_goal_alignment(model),\n            'corrigibility': self.test_corrigibility(model),\n            'honesty': self.test_honesty(model),\n            \n            # Safety tests\n            'manipulation_resistance': self.test_manipulation(model),\n            'deception_detection': self.test_deception(model),\n            'power_seeking': self.test_power_seeking(model),\n        }\n        \n        risk_score = self.calculate_risk(tests)\n        \n        if risk_score > SAFETY_THRESHOLD:\n            return {\n                'deploy': False,\n                'reason': 'Failed safety thresholds',\n                'details': tests\n            }\n        \n        return {'deploy': True, 'tests': tests}\n```\n\n### Responsible Scaling Policies\n\n```python\nclass ResponsibleScalingPolicy:\n    def __init__(self):\n        self.capability_thresholds = {\n            'level_1': 'Current AI (GPT-4 level)',\n            'level_2': 'Expert-level in most domains',\n            'level_3': 'Capable of autonomous research',\n            'level_4': 'Approaching AGI',\n            'level_5': 'Superhuman AGI'\n        }\n        \n        self.safety_requirements = {\n            'level_1': ['basic_safety_testing', 'content_filtering'],\n            'level_2': ['red_teaming', 'alignment_testing', 'human_oversight'],\n            'level_3': ['advanced_alignment', 'formal_verification', 'staged_deployment'],\n            'level_4': ['proof_of_alignment', 'extensive_testing', 'regulatory_approval'],\n            'level_5': ['maximum_safety_measures', 'international_oversight']\n        }\n    \n    def evaluate_system(self, system):\n        capability_level = self.assess_capabilities(system)\n        safety_level = self.assess_safety_measures(system)\n        \n        required_safety = self.safety_requirements[capability_level]\n        \n        if not self.meets_requirements(safety_level, required_safety):\n            return {\n                'approved': False,\n                'reason': f'Safety measures insufficient for {capability_level}',\n                'required': required_safety,\n                'current': safety_level\n            }\n        \n        return {'approved': True}\n```\n\n## Open Problems in Alignment\n\n### 1. Value Specification\n**Challenge:** How do we specify human values precisely?\n\n```python\n# Attempt 1: Simple rule\nvalues = \"Don't harm humans\"\n# Problem: What counts as harm? Short-term vs long-term?\n\n# Attempt 2: Learn from humans\nvalues = learn_from_human_feedback()\n# Problem: Humans disagree, are inconsistent, make mistakes\n\n# Attempt 3: Philosophical approach\nvalues = implement_moral_philosophy()\n# Problem: Philosophers disagree on ethics\n\n# Current best: Combination + humility\nvalues = {\n    'learned_preferences': learn_from_humans(),\n    'ethical_constraints': hard_coded_rules(),\n    'uncertainty': acknowledge_moral_uncertainty(),\n    'defer_to_humans': defer_on_hard_questions()\n}\n```\n\n### 2. Interpretability\n**Challenge:** Understanding what advanced AI is \"thinking\"\n\n```python\nclass Interpretability:\n    def understand_model_decision(self, model, input, output):\n        # Current techniques\n        attention = self.visualize_attention(model, input)\n        features = self.extract_important_features(model, input)\n        \n        # Future needs for AGI\n        reasoning_chain = self.extract_reasoning()  # Not yet possible\n        internal_goals = self.identify_goals()  # Not yet possible\n        deception_check = self.detect_deception()  # Not yet possible\n        \n        return interpretation\n```\n\n### 3. Robustness to Distribution Shift\n**Challenge:** Maintaining alignment in novel situations\n\n## Positive Futures\n\n### AI as a Tool for Good\n\n**Potential Benefits of Aligned AGI:**\n- Solve climate change\n- Cure diseases\n- End poverty\n- Expand human knowledge\n- Enable space exploration\n- Augment human capabilities\n\n**Key Requirements:**\n1. **Alignment:** AI pursues beneficial goals\n2. **Safety:** AI operates within safe bounds\n3. **Access:** Benefits distributed fairly\n4. **Governance:** Appropriate oversight and control\n\n### Collaborative Development\n\n```\nStakeholders in AGI Development:\n- AI researchers (technical safety)\n- Ethicists (value alignment)\n- Policymakers (governance)\n- Industry (deployment)\n- Civil society (public interest)\n- Domain experts (specific applications)\n\nBest Practices:\n- Open research (where safe)\n- Information sharing on safety\n- International cooperation\n- Inclusive decision-making\n```\n\n## Practical Steps Today\n\n### For Developers\n\n```python\ndef build_with_alignment_in_mind():\n    # 1. Be explicit about goals\n    system_goal = \"Help users while avoiding harm\"\n    \n    # 2. Test for misalignment\n    test_edge_cases()\n    test_adversarial_inputs()\n    \n    # 3. Implement safeguards\n    add_human_oversight()\n    add_output_filtering()\n    add_uncertainty_awareness()\n    \n    # 4. Monitor deployment\n    track_unexpected_behaviors()\n    gather_user_feedback()\n    \n    # 5. Iterate\n    improve_based_on_learnings()\n```\n\n### For Organizations\n\n```\n1. Establish AI Ethics Board\n2. Create responsible AI guidelines\n3. Invest in safety research\n4. Train team on alignment\n5. Engage with AI safety community\n6. Plan for capability increases\n7. Develop incident response\n```\n\n## Resources\n\n**Research Organizations:**\n- Anthropic, OpenAI, DeepMind (industry)\n- AI Alignment Forum, Center for AI Safety (non-profit)\n- Future of Humanity Institute, MIRI (academic)\n\n**Key Papers:**\n- \"Concrete Problems in AI Safety\" (Amodei et al., 2016)\n- \"Alignment of Language Agents\" (Anthropic, 2023)\n- \"Scalable Oversight\" (OpenAI, 2024)\n\n**Courses:**\n- AGI Safety Fundamentals\n- AI Alignment courses (various universities)",
        keyPoints: [
          "AI alignment ensures AI systems pursue goals beneficial to humanity",
          "RLHF and Constitutional AI are current techniques for aligning LLMs",
          "Scalable oversight becomes harder as AI becomes more capable than humans",
          "Advanced AI systems require rigorous safety testing and staged deployment",
          "International cooperation and inclusive governance are essential for AGI safety"
        ],
        handsOnExercise: "Design an alignment testing protocol for an AI system. Include tests for goal alignment, deception, power-seeking, and corrigibility."
      }
    ]
  }
];

export const courseOverview = {
  title: "Comprehensive Prompt Engineering Course",
  description: "Master the art and science of communicating with AI. From foundations to advanced techniques, learn to craft effective prompts that unlock the full potential of Large Language Models.",
  duration: "40-50 hours (self-paced)",
  prerequisites: "None required - suitable for complete beginners",
  targetAudience: ["Developers", "Researchers", "Business Professionals", "AI Enthusiasts", "Anyone curious about AI"],
  learningOutcomes: [
    "Understand how Large Language Models work and process information",
    "Master fundamental and advanced prompting techniques",
    "Design secure, reliable AI systems for production use",
    "Apply prompt engineering to real-world business problems",
    "Stay current with emerging research and best practices"
  ],
  keyTopics: [
    "Zero-shot and Few-shot Learning",
    "Chain-of-Thought Prompting",
    "Retrieval-Augmented Generation (RAG)",
    "ReAct Framework",
    "Production Security and Optimization",
    "Ethical AI Development"
  ]
};
